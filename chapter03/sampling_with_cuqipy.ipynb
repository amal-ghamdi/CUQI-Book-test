{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a7f74",
   "metadata": {},
   "source": [
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\dd{\\mathrm{d}}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\abs#1{\\left\\vert#1\\right\\vert}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\ve#1{\\bm{#1}}$\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\mat#1{\\mathbf{#1}}$\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Sampling with CUQIpy\n",
    "\n",
    "In this notebook, we explore uncertainty quantification for inverse problems through Bayesian inference using CUQIpy. We focus on exploring the sampling capabilities of CUQIpy, for two target distribution: \n",
    "- a bi-variate \"donut\" distribution,\n",
    "- and a posterior distribution for a 1D Poisson-based BIP.\n",
    "\n",
    "The former is used for illustrative purposes and is not associated with an inverse problem, while the latter is a more realistic example of a BIP.\n",
    "\n",
    "## <font color=#CD853F> Contents of this notebook: </font>\n",
    "\n",
    "## <font color=#CD853F> Learning objectives: </font> <a name=\"r-learning-objectives\"></a>\n",
    "\n",
    "\n",
    "The section marked with ★ is optional and can be skipped if you are short on time.\n",
    "\n",
    "## References\n",
    "[1] Gelman, Andrew, et al. \"Bayesian workflow.\" arXiv preprint arXiv:2011.01808 (2020) https://arxiv.org/abs/2011.01808.\n",
    "\n",
    "[2] Riis, Nicolai AB, et al. \"CUQIpy--Part I: computational uncertainty quantification for inverse problems in Python.\" arXiv preprint arXiv:2305.16949 (2023) https://arxiv.org/abs/2305.16949.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169478f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery, Gaussian, JointDistribution\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.problem import BayesianProblem\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH\n",
    "import time\n",
    "import scipy.stats as sps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d37a81",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot2d(val, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    # plot\n",
    "    pixelwidth_x = (x1_max-x1_min)/(N2-1)\n",
    "    pixelwidth_y = (x2_max-x2_min)/(N2-1)\n",
    "\n",
    "    hp_x = 0.5*pixelwidth_x\n",
    "    hp_y = 0.5*pixelwidth_y\n",
    "\n",
    "    extent = (x1_min-hp_x, x1_max+hp_x, x2_min-hp_y, x2_max+hp_y)\n",
    "\n",
    "    plt.imshow(val, origin='lower', extent=extent)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def plot_pdf_2D(distb, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    N2 = 201\n",
    "    ls1 = np.linspace(x1_min, x1_max, N2)\n",
    "    ls2 = np.linspace(x2_min, x2_max, N2)\n",
    "    grid1, grid2 = np.meshgrid(ls1, ls2)\n",
    "    distb_pdf = np.zeros((N2,N2))\n",
    "    for ii in range(N2):\n",
    "        for jj in range(N2):\n",
    "            distb_pdf[ii,jj] = np.exp(distb.logd(np.array([grid1[ii,jj], grid2[ii,jj]]))) \n",
    "    plot2d(distb_pdf, x1_min, x1_max, x2_min, x2_max, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed3a59",
   "metadata": {},
   "source": [
    "## <font color=#CD853F> The \"dount\" distribution </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "In CUQIpy, we provide a set of bi-variate distributions for illustrative purposes. One of these is the \"donut\" distribution, which is a bi-variate distribution of a donut-shaped. The distribution is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "log (p(\\mathbf{x})) \\propto - \\frac{1}{\\sigma_\\text{donut}^2} \\left( \\left\\| \\mathbf{x} \\right\\| - r_\\text{donut} \\right)^2\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x} = (x_1, x_2)$ is a 2D vector, $\\left\\| \\mathbf{x} \\right\\|$ is the Euclidean norm of $\\mathbf{x}$, $r_\\text{donut}$ is the radius of the donut, and $\\sigma_\\text{donut}$ is a scalar value that controls the width of the \"donut\".\n",
    "\n",
    "To load the \"donut\" distribution, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_donut = DistributionGallery(\"donut\")\n",
    "\n",
    "print(target_donut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649d363",
   "metadata": {},
   "source": [
    "We can plot the distribution probability density function (pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce235016",
   "metadata": {},
   "source": [
    "\n",
    "## <font color=#CD853F> A 1D Poisson-based BIP </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "##### <font color=#8B4513> The forward model </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "Consider a heat conductive rod of length $L = \\pi$ with a varying conductivity (the conductivity of the rod changes from point to point). We fix the temperature at the end-points of the rod and apply a heat source distributed along the length of the rod. We wait until the rod reaches an equilibrium temperature distribution. The equilibrium temperature of the rod is modelled using the second order steady-state PDE as\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& \\dfrac{\\dd}{\\dd \\xi}\\left(u(\\xi) \\dfrac{\\dd y(\\xi)}{\\dd \\xi}\\right) = -f(\\xi), \\quad & \\xi\\in (0,L) \\\\\n",
    "& y(0) = y(L) = 0.\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "Here, $y$ represents the temperature distribution along the rod, $u(\\xi) $ is the unknown conductivity of the rod and $f(\\xi)$ is a deterministic heat source given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tf(\\xi) = 10\\exp( -\\frac{ (\\xi - L/2)^2} {0.02} ).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To ensure that the conductivity of the rod is non-negative, we parameterize $u$ by a random variable $x$ as follows:\n",
    " \n",
    "$$\n",
    " \\begin{aligned}\n",
    " u( \\cdot  ) = \\exp( x( \\cdot  ) )\n",
    " \\end{aligned}\n",
    "$$\n",
    "where $x$ is not necessarily positive.\n",
    "\n",
    "Let us load the forward model that maps the random variable $x$ to the temperature distribution $y$ in CUQIpy. We will use the following parameters:\n",
    "* `dim` : number of discretization points for the rod\n",
    "* `L` : length of the rod\n",
    "* `f` : a function that represents the heat source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 201\n",
    "L = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60eb4a9",
   "metadata": {},
   "source": [
    "The source term represents spikes at four locations `xs` with weight `ws`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef704b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([0.2, 0.4, 0.6, 0.8])*L\n",
    "ws = 0.8\n",
    "sigma_s = 0.05\n",
    "def f(t):\n",
    "    s = np.zeros(dim-1)\n",
    "    for i in range(4):\n",
    "        s += ws * sps.norm.pdf(t, loc=xs[i], scale=sigma_s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af772e",
   "metadata": {},
   "source": [
    "Let us plot the source term for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grid = np.linspace(0, L, dim-1)\n",
    "plt.plot(temp_grid, f(temp_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6a223",
   "metadata": {},
   "source": [
    "Then we can load the 1D Poisson forward model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, _, _ = Poisson1D(dim=dim, \n",
    "                    endpoint=L,\n",
    "                    field_type='KL',\n",
    "                    field_params={'num_modes': 10} ,\n",
    "                    map=lambda x: np.exp(x), \n",
    "                    source=f).get_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074001c",
   "metadata": {},
   "source": [
    "We print the forward model to see its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765684",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3308c",
   "metadata": {},
   "source": [
    "Let us look at the `pde` property of the forward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.pde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152f551",
   "metadata": {},
   "source": [
    "We can look at the domain and range geometries of the forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_geometry)\n",
    "print(A.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99369a8",
   "metadata": {},
   "source": [
    "And inspect the domain geometry further. Let us look at the mapping in the `MappedGeometry` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(A.domain_geometry.map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f9a8c",
   "metadata": {},
   "source": [
    "We can extract the underlying geometry of the `MappedGeometry` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_geometry = A.domain_geometry.geometry\n",
    "print(underlying_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f923c2",
   "metadata": {},
   "source": [
    "The underlying geometry represents a Karhunen–Loève (KL) expansion of a random field. Let us look at some of the properties of this `underlying_geometry` such as the number of modes in the KL expansion and the grid on which the KL expansion basis functions are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666dc5",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "print(A.domain_geometry.geometry.num_modes)\n",
    "print(A.domain_geometry.geometry.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04b0dd",
   "metadata": {},
   "source": [
    "The range geometry is of type `Continuous1D` which represents a 1D continuous signal/field defined on a grid. We can view the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d6938",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "print(A.domain_geometry.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76189a89",
   "metadata": {},
   "source": [
    "Additionally, the properties `domain_dim` and `range_dim` of the forward model represent the dimension of the input and output of the forward model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_dim)\n",
    "print(A.range_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a229d",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the prior </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "We now build a posterior distribution based on this forward model. The unknown  $\\mathbf{x}$ represents the coefficients in the KL expansion. We assume that the prior distribution of $\\mathbf{x}$ is an i.i.d Gaussian distribution with mean $0$ and variance $\\sigma_x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x = 30\n",
    "x = Gaussian(0, sigma_x**2, geometry=A.domain_geometry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3eb9fc",
   "metadata": {},
   "source": [
    "Let us assume that the true solution we want to infer is a sample from `x`. Note: we fix the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "x_true = x.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1b87c",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "- Visualize `x_true` in the KL coefficient space. Hint: try `x_true.plot(plot_par=True)`\n",
    "- Visualize `x_true` in the corresponding function space (after applying the linear combination of KL basis weighted by KL vectors and then applying the exponantial mapping). Hint: all this can be achieved in one line by `x_true.plot(plot_par=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11499ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a1e04",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the likelihood </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "We assume the data we obtain is a noisy measurement of the temperature $y$ over the interval $[0, L]$ in all grid points. The measurements form a vector $\\mathbf{y}$. The noise is assumed to be additive Gaussian noise with mean $0$ and variance $\\sigma_y^2$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A}(\\mathbf{x}) + \\epsilon  \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2).\n",
    "\n",
    "$$\n",
    "\n",
    "We define the data distribution $p(\\mathbf{y} | \\mathbf{x})$ in `CUQIpy` in this case as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = np.sqrt(0.001)\n",
    "y = Gaussian(A(x), sigma_y**2, geometry=A.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19409",
   "metadata": {},
   "source": [
    "We create a synthetic data to use it to test solving our BIP.  We denote this data as $\\mathbf{y}_{\\text{obs}}$ which is a particular observed data realization from a setup where the KL coefficients are `x_true`. To create this data in `CUQIpy`, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = y(x=x_true).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e2df5",
   "metadata": {},
   "source": [
    "Let us plot the true conductivity field, corresponding to `x_true`, the data `y_true` without noise i.e. `A(x_true)`, and the noisy data `y_obs` in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs.plot(label='y_obs')\n",
    "A(x_true).plot(label='y_true')\n",
    "x_true.plot(label='x_true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33525819",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the posterior distribution  (the high level approach: using the BayesianProblem class)</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846ffa3",
   "metadata": {},
   "source": [
    "The posterior distribution of the Bayesian inverse problem in this case is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs}) \\propto L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs}) := p(\\mathbf{y}=\\mathbf{y}_\\mathrm{obs} \\mid \\mathbf{x})$ for the likelihood function to emphasize that, in the context of the posterior where $\\mathbf{y}$ is fixed to $\\mathbf{y}_\\mathrm{obs}$, it is a function of $\\mathbf{x}$ and not on $\\mathbf{y}$. In CUQIpy we sometimes use the short-hand printing style `L(x|y)` for brevity.\n",
    "\n",
    "\n",
    "\n",
    "The simplest way to sample a Bayesian inverse problem in CUQIpy is to use the [BayesianProblem class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "\n",
    "Using the BayesianProblem class, one can easily define and sample from the posterior distribution of a Bayesian inverse problem by providing the distributions for the parameters and data and subsequently setting the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_poisson = BayesianProblem(x, y)      # Create Bayesian problem\n",
    "BP_poisson.set_data(y=y_obs)           # Provide observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c578f",
   "metadata": {},
   "source": [
    "In the above example, we provided our assumptions about the data generating process by defining the distributions for the parameters and data and provided the observed data for the problem. `CUQIpy` internally created the posterior distribution using the provided distributions and data. \n",
    "\n",
    "We can use this object to sample from the posterior distribution using the `UQ` method, which we will experiment with in this exercise:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfc7de",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "Use the `UQ` method of the `BP_poisson` object to sample the posterior distribution. The `UQ` returns a `Samples` object, store the result in a variable called `BP_poisson_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2cd3e",
   "metadata": {},
   "source": [
    "In the previous exercise we saw that `CUQIpy` automatically decided on using a sampler, preconditioned Crank Nicolson `pCN` in this case, and sampled the posterior distribution. Additionally, the  credibility interval for the parameter $\\mathbf{x}$ as well as the mean of the posterior was plotted and compared to the ground truth (`x_true`).\n",
    "\n",
    "**Note about visualizing the credible interval**:\n",
    "Using the `UQ` method, the credibility interval is computed for the KL coefficients. Then mapped to the function space and plotted. We can also compute the credibility interval directly on the function values. We will revisit this at a later stage.\n",
    "\n",
    "\n",
    "In the next section, we show how to define the posterior distribution more explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad108f0",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the posterior distribution  (the low level approach: using the JointDistribution)</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5404b4",
   "metadata": {},
   "source": [
    "To define the posterior distribution explicitly in CUQIpy, we first define the joint distribution $p(\\mathbf{y},\\mathbf{x})$, then we supply the observed data to create the conditional distribution $p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs})$.\n",
    "\n",
    "\n",
    "Let us first define the joint distribution $p(\\mathbf{y},\\mathbf{x})$ in CUQIpy. We use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a45e5",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description of the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4732984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cf449",
   "metadata": {},
   "source": [
    "CUQIpy can automatically derive the posterior distribution for any joint distribution when we pass the observed data as an argument to the \"call\" (condition) method of the joint distribution.  This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a495cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=y_obs) # Condition p(x,y) on y=y_data. Applies Bayes' rule automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b83148",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it. Notice that the posterior equation matches the mathematical expression we showed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ea355",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "- The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "- Try evaluating the posterior log probability density function (logpdf) and pdf at some points say `x_true` and `x_true*1.1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76151632",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ffe538e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "234f5525",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0570cbf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0faa8e53",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining and sampling a Bayesian Inverse Problem (high-level)<a class=\"anchor\" id=\"BIP\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the posterior distribution.\n",
    "\n",
    "The posterior describes the probability distribution of the parameters we are interested in. This is achieved by combining prior knowledge of the parameters and observed data. In its most general form, the posterior is given by Bayes' theorem:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})} \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\boldsymbol{\\theta}$ is the parameter vector of *all* the parameters we are interested in inferring and $\\mathbf{y}$ is the observable data. In the simplest case one could have a single parameter vector of interest, say $\\boldsymbol{\\theta}=[\\mathbf{x}]$.\n",
    "\n",
    "The probability density function $p(\\boldsymbol{\\theta})$ is the prior distribution of the parameters.\n",
    "\n",
    "Given fixed observed data $\\mathbf{y}_\\mathrm{data}$, the term $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ considered as a function of $\\boldsymbol{\\theta}$ is known as the\n",
    "likelihood function or just *likelihood*, also denoted $L(\\boldsymbol{\\theta} \\mid \\mathbf{y} = \\mathbf{y}_\\mathrm{data})$, which we note is not a probability density but a density function.\n",
    "\n",
    "When $\\mathbf{y}$ is not fixed, $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ is a probability density function of the data $\\mathbf{y}$ given the value of the parameters $\\boldsymbol{\\theta}$. In CUQIpy we refer to this distribution as the *data distribution*.\n",
    "\n",
    "The denominator $p(\\mathbf{y})$ is the *evidence* and is a normalization constant (that we typically ignore because it does not affect the MCMC sampling) that ensures that the posterior integrates to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30f3764",
   "metadata": {},
   "source": [
    "### Note on Bayesian inverse problems with CUQIpy\n",
    "\n",
    "CUQIpy uses a general approach to Bayesian modeling that not only aims to define the posterior distribution, but instead to define the joint distribution of all the parameters. This more general approach is useful because it allows one to carry out more tasks related to uncertainty quantification of inverse problems such as prior-predictive checks, model checking, posterior-predictive checks and more. For more details on some of these topics see the overview in [1].\n",
    "\n",
    "In this notebook, we initially focus on how to define and sample a Bayesian inverse problem using the high-level interface in CUQIpy. We then later show a more low-level approach to defining the posterior distribution, which is useful for users who want to have more control the type of sampler used for sampling the posterior distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic forward model and observed data\n",
    "Consider a Bayesian inverse problem\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{e},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the (deterministic) forward model of the inverse problem and $\\mathbf{y}$ and $\\mathbf{x}$ are random variables representing the observed data and parameter of interest respectively. Here $\\mathbf{e}$ is a random variable representing additive noise in the data.\n",
    "\n",
    "For this example let us consider the `Deconvolution1D` testproblem and extract a CUQIpy forward model and some synthetic data denoted $\\mathbf{y}_\\mathrm{data}$ (a realization of $\\mathbf{y}$). \n",
    "\n",
    "Note that this is a linear inverse problem, but the same approach can be used for nonlinear inverse problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# For convenience, we define the dimension of the domain of A\n",
    "n = A.domain_dim\n",
    "\n",
    "# For convenience, we extract the exact solution as x_exact\n",
    "x_exact = probInfo.exactSolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem.\n",
    "\n",
    "Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the `probInfo` variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); x_exact.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc2f4a9",
   "metadata": {},
   "source": [
    "### Note on notation\n",
    "\n",
    "It is common (for convenience in terms of notation) not to explicitly write the dependance of each random variable when specifying a complete Bayesian problem. For example, for the case above one would often write\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d)\\\\\n",
    "\\mathbf{y} &\\sim \\mathrm{Gaussian}(\\mathbf{A}\\mathbf{x}, s_\\mathbf{y}^2 I),\n",
    "\\end{align*}\n",
    "\n",
    "where the dependance of $\\mathbf{y}$ on $\\mathbf{x}$ is implicit.\n",
    "\n",
    "This compact notation completely specifies the Bayesian problem for the so-called *data generating process*, making clear all the assumptions about the data and parameters.\n",
    "\n",
    "In CUQIpy - when all deterministic parameters and forward models are defined - the Bayesian problem is written in code using almost exactly the same syntax as the mathematical notation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian problem (repeated in case previous cells were modified)\n",
    "x = GMRF(np.zeros(n), 50)\n",
    "y = Gaussian(A@x, s_y**2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "# 2.3 Sampling the posterior\n",
    "\n",
    "Now that we have defined the posterior distribution for our parameter of interest $\\mathbf{x}$ given $\\mathbf{y}_\\mathrm{data}$, we can characterize the parameter and its uncertainty by samples from the posterior distribution. However, in general the posterior is not a simple distribution that we can easily sample from. Instead, we need to rely on Markov Chain Monte Carlo (MCMC) methods to sample from the posterior.\n",
    "\n",
    "In CUQIpy, a number of MCMC samplers are provided in the sampler module that can be used to sample probability distributions. All samplers have the same signature, namely `Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above, which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize [(LinearRTO)](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.LinearRTO.html#cuqi.sampler.LinearRTO) sampler is a good choice to efficiently generate samples. This is also the sampler chosen by the BayesianProblem class for this problem.\n",
    "\n",
    " Like any of the other samplers, we set up the sampler by simply providing the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = LinearRTO(posterior)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "After the sampler is defined we can compute samples via the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, the returned object is a `cuqi.samples.Samples` object.\n",
    "\n",
    "This object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the chain has converged and there is little need for removing samples that are part of a \"burn-in\" period. In practice, the samples should be inspected with more rigor to ensure that the MCMC chain has converged, but this is outside the scope of this notebook.\n",
    "\n",
    "The good sampling is in large part due to the LinearRTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see [samples.burnthin](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.samples/cuqi.samples.Samples.burnthin.html#cuqi.samples.Samples.burnthin)) and store the \"burnthinned\" samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from `probInfo`).\n",
    "\n",
    "This is what the `UQ` method of the BayesianProblem class did under the hood for us earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "779d95f1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The LinearRTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the logpdf. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers (click the links to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from LinearRTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as LinearRTO or NUTS. For some samplers, the method [sample_adapt](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.sample_adapt.html#cuqi.sampler.NUTS.sample_adapt) will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate and a burn-in should be added to specify how many samples to use for the adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "# 3. Exploring different prior choices<a class=\"anchor\" id=\"ModifyPriors\"></a>\n",
    "\n",
    "In the above example, we used a GMRF prior for the parameter $\\mathbf{x}$. However, it is not always obvious what prior to use for a given problem. In such cases, it is often useful to try out different priors to see how they affect the posterior distribution.\n",
    "\n",
    "In CUQIpy, it is easy to modify the prior and re-sample the posterior distribution. This is most easily done by using the BayesianProblem class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "####  Try yourself (optional):  \n",
    "\n",
    "Please carry out the following exercise to see how the prior affects the posterior distribution. \n",
    "\n",
    "Note that: here we use the [sample_posterior](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.sample_posterior.html#cuqi.problem.BayesianProblem.sample_posterior) method of the BayesianProblem class to sample the posterior distribution and store the samples without plotting. We then manually plot the samples using the `plot_ci` method of the `Samples` object.\n",
    "\n",
    "- Try another prior such as [LMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.LMRF.html#cuqi.distribution.LMRF) or [CMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.CMRF.html#cuqi.distribution.CMRF) for the 1D case (look up appropriate arguments in the documentation) using [`BayesianProblem`](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "- Try switching the testproblem from Deconvolution1D to [Deconvolution2D](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.testproblem/cuqi.testproblem.Deconvolution2D.html#cuqi.testproblem.Deconvolution2D) (look up appropriate arguments in the documentation).\n",
    "- You may also try defining your own Bayesian inverse problem using this interface. ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can modify this code or write your own from scratch\n",
    "\n",
    "# 1. Forward model and data\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# 2. Distributions\n",
    "x = GMRF(np.zeros(A.domain_dim), 50) # Try e.g. LMRF or CMRF (also update scale parameters!)\n",
    "y = Gaussian(A@x, 0.01**2)\n",
    "\n",
    "# 3. Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "\n",
    "# 4. Sample posterior\n",
    "samples = BP.sample_posterior(500)\n",
    "\n",
    "# 5. Analyze posterior\n",
    "samples.plot_ci(exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0583cafe",
   "metadata": {},
   "source": [
    "You may have noticed that finding suitable parameters for the prior could be a challenge. To see how to automatically find suitable parameters for the prior, see the [Gibbs notebook](Exercise06_Gibbs.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097194b0",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine in case something was changed\n",
    "\n",
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = GMRF(np.zeros(A.domain_dim), 50)\n",
    "y = Gaussian(mean=A@x, cov=0.01**2)\n",
    "\n",
    "# Define Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP() # Maximum a posteriori estimate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2931bb8f",
   "metadata": {},
   "source": [
    "The automatic solver selection is also still work-in-progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24cab183",
   "metadata": {},
   "source": [
    "After we have computed the MAP, we can then estimate to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a931f3a-9534-48b1-a273-5bc8616954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching to the Deconvolution2D testproblem. You may have to play with the prior standard deviation to get a good MAP estimate.\n",
    "- Try switching the prior to a CMRF distribution for the 1D case. Does the MAP estimate change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5adf1d",
   "metadata": {},
   "source": [
    "## Story 1: MH adaptive scaling, donuts (it pays off to adapt)\n",
    "\n",
    "## Sample a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0715bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH\n",
    "import time\n",
    "\n",
    "target = DistributionGallery(\"donut\")\n",
    "\n",
    "\n",
    "print(target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MH\n",
    "# try scale = 0.1, 1, 10\n",
    "Ns = 30000\n",
    "Nb = 5000\n",
    "scale = 0.05\n",
    "MH_sampler = MH(target, scale=scale, x0=np.array([0,0]))\n",
    "ti = time.time()\n",
    "MH_fixed_samples = MH_sampler.sample(Ns, Nb)\n",
    "print('Elapsed time MH:', time.time() - ti, '\\n')\n",
    "MH_fixed_samples.plot_pair(ax=plt.gca())\n",
    "\n",
    "\n",
    "# try sample adapt\n",
    "MH_adapted_samples = MH_sampler.sample_adapt(Ns, Nb)\n",
    "print('Elapsed time MH:', time.time() - ti, '\\n')\n",
    "\n",
    "#set defult color matplotlib\n",
    "MH_adapted_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'r'})\n",
    "\n",
    "\n",
    "#CWMH_sampler = CWMH(target, scale=scale, x0=np.array([0,0]))\n",
    "#CWMH_fixed_samples = CWMH_sampler.sample(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_fixed_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'g'})\n",
    "#\n",
    "#\n",
    "#CWMH_adapted_samples = CWMH_sampler.sample_adapt(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_adapted_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'y'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8539c",
   "metadata": {},
   "source": [
    "what do you notice about acceptance rate?\n",
    "scale?\n",
    "plot diagnostics, ESS\n",
    "\n",
    "try 60000 / 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe36d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MH_fixed_samples.compute_ess())\n",
    "print(MH_adapted_samples.compute_ess())\n",
    "MH_fixed_samples.plot_trace()\n",
    "MH_adapted_samples.plot_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0d888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4415e99f",
   "metadata": {},
   "source": [
    "## story 2: Poisson, CW vs MH  (not all unkowns are created equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aff035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson inverse problem\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.distribution import GMRF, Gaussian, CMRF\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.sampler import MH, CWMH, pCN\n",
    "\n",
    "# Load forward model, data and problem information\n",
    "n = 20\n",
    "Ns = 3000\n",
    "Nb = 2000\n",
    "cov = 4.62905925e-05\n",
    "A, y_data, probInfo = Poisson1D(dim=n).get_components()\n",
    "\n",
    "x = GMRF(np.zeros(A.domain_dim), 8, geometry=A.domain_geometry,  order=1)\n",
    "y = Gaussian(A(x), cov)\n",
    "\n",
    "BP = BayesianProblem(x, y).set_data(y=y_data)\n",
    "posterior = BP.posterior()\n",
    "\n",
    "scale = 0.06\n",
    "MH_sampler = MH(posterior, scale = scale, x0=np.ones(n))\n",
    "MH_samples = MH_sampler.sample_adapt(Ns, Nb)\n",
    "\n",
    "\n",
    "CWMH_sampler = CWMH(posterior, scale = scale, x0=np.ones(n))\n",
    "CWMH_samples = CWMH_sampler.sample_adapt(Ns, Nb)\n",
    "int(0.1*Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_samples.burnthin(int(0.1*(Ns))).plot_ci(95, exact=probInfo.exactSolution)\n",
    "plt.figure()\n",
    "CWMH_samples.burnthin(int(0.1*Ns)).plot_ci(95, exact=probInfo.exactSolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWMH_sampler.scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22010fa2",
   "metadata": {},
   "source": [
    "time both\n",
    "insptect scale for both\n",
    "use KL expansion and compare both samplers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d58c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MH_samples.compute_ess())\n",
    "print(CWMH_samples.compute_ess())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Poisson1D(dim=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.likelihood.distribution.cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3abda6",
   "metadata": {},
   "source": [
    "## Story 3: Gradient info, ULA (univariate, bias) (Wandering randomly or be Guided by the force / let the force guide you)\n",
    "\n",
    "## Story 4: Gradient info, MALA (is it ever possible to fix the bias)\n",
    "\n",
    "## Story 5: NUTS (it is time to go nuts with nuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5628eb2",
   "metadata": {},
   "source": [
    "inspect sampler / its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e027a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "- Compute the credibility interval in the function space and plot it. Hint: use the property `funvals` of the samples: `BP_poisson_samples.funvals` which returns another samp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
