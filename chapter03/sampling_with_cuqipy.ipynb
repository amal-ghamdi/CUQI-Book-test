{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a7f74",
   "metadata": {},
   "source": [
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\dd{\\mathrm{d}}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\abs#1{\\left\\vert#1\\right\\vert}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\ve#1{\\bm{#1}}$\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\mat#1{\\mathbf{#1}}$\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Sampling with CUQIpy\n",
    "\n",
    "In this notebook, we explore uncertainty quantification for inverse problems through Bayesian inference using CUQIpy. We focus on exploring the sampling capabilities of CUQIpy, for two target distribution: \n",
    "- a bi-variate \"donut\" distribution,\n",
    "- and a posterior distribution for a 1D Poisson-based BIP.\n",
    "\n",
    "The former is used for illustrative purposes and is not associated with an inverse problem, while the latter is a more realistic example of a BIP.\n",
    "\n",
    "## <font color=#CD853F> Contents of this notebook: </font>\n",
    "\n",
    "## <font color=#CD853F> Learning objectives: </font> <a name=\"r-learning-objectives\"></a>\n",
    "\n",
    "\n",
    "The section marked with ★ is optional and can be skipped if you are short on time.\n",
    "\n",
    "## References\n",
    "[1] Gelman, Andrew, et al. \"Bayesian workflow.\" arXiv preprint arXiv:2011.01808 (2020) https://arxiv.org/abs/2011.01808.\n",
    "\n",
    "[2] Riis, Nicolai AB, et al. \"CUQIpy--Part I: computational uncertainty quantification for inverse problems in Python.\" arXiv preprint arXiv:2305.16949 (2023) https://arxiv.org/abs/2305.16949.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH\n",
    "import time\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d37a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot2d(val, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    # plot\n",
    "    pixelwidth_x = (x1_max-x1_min)/(N2-1)\n",
    "    pixelwidth_y = (x2_max-x2_min)/(N2-1)\n",
    "\n",
    "    hp_x = 0.5*pixelwidth_x\n",
    "    hp_y = 0.5*pixelwidth_y\n",
    "\n",
    "    extent = (x1_min-hp_x, x1_max+hp_x, x2_min-hp_y, x2_max+hp_y)\n",
    "\n",
    "    plt.imshow(val, origin='lower', extent=extent)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def plot_pdf_2D(distb, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    N2 = 201\n",
    "    ls1 = np.linspace(x1_min, x1_max, N2)\n",
    "    ls2 = np.linspace(x2_min, x2_max, N2)\n",
    "    grid1, grid2 = np.meshgrid(ls1, ls2)\n",
    "    distb_pdf = np.zeros((N2,N2))\n",
    "    for ii in range(N2):\n",
    "        for jj in range(N2):\n",
    "            distb_pdf[ii,jj] = np.exp(distb.logd(np.array([grid1[ii,jj], grid2[ii,jj]]))) \n",
    "    plot2d(distb_pdf, x1_min, x1_max, x2_min, x2_max, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed3a59",
   "metadata": {},
   "source": [
    "## <font color=#CD853F> The \"dount\" distribution </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "In CUQIpy, we provide a set of bi-variate distributions for illustrative purposes. One of these is the \"donut\" distribution, which is a bi-variate distribution of a donut-shaped. The distribution is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "log (p(\\mathbf{x})) \\propto - \\frac{1}{\\sigma_\\text{donut}^2} \\left( \\left\\| \\mathbf{x} \\right\\| - r_\\text{donut} \\right)^2\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x} = (x_1, x_2)$ is a 2D vector, $\\left\\| \\mathbf{x} \\right\\|$ is the Euclidean norm of $\\mathbf{x}$, $r_\\text{donut}$ is the radius of the donut, and $\\sigma_\\text{donut}$ is a scalar value that controls the width of the \"donut\".\n",
    "\n",
    "To load the \"donut\" distribution, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_donut = DistributionGallery(\"donut\")\n",
    "\n",
    "print(target_donut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649d363",
   "metadata": {},
   "source": [
    "We can plot the distribution probability density function (pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce235016",
   "metadata": {},
   "source": [
    "\n",
    "## <font color=#CD853F> A 1D Poisson-based BIP </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "##### <font color=#8B4513> The forward model </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "Consider a heat conductive rod of length $L = \\pi$ with a varying conductivity (the conductivity of the rod changes from point to point). We fix the temperature at the end-points of the rod and apply a heat source distributed along the length of the rod. We wait until the rod reaches an equilibrium temperature distribution. The equilibrium temperature of the rod is modelled using the second order steady-state PDE as\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& \\dfrac{\\dd}{\\dd \\xi}\\left(u(\\xi) \\dfrac{\\dd y(\\xi)}{\\dd \\xi}\\right) = -f(\\xi), \\quad & \\xi\\in (0,L) \\\\\n",
    "& y(0) = y(L) = 0.\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "Here, $y$ represents the temperature distribution along the rod, $u(\\xi) $ is the unknown conductivity of the rod and $f(\\xi)$ is a deterministic heat source given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tf(\\xi) = 10\\exp( -\\frac{ (\\xi - L/2)^2} {0.02} ).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To ensure that the conductivity of the rod is non-negative, we parameterize $u$ by a random variable $x$ as follows:\n",
    " \n",
    "$$\n",
    " \\begin{aligned}\n",
    " u( \\cdot  ) = \\exp( x( \\cdot  ) )\n",
    " \\end{aligned}\n",
    "$$\n",
    "where $x$ is not necessarily positive.\n",
    "\n",
    "Let us load the forward model that maps the random variable $x$ to the temperature distribution $y$ in CUQIpy. We will use the following parameters:\n",
    "* `dim` : number of discretization points for the rod\n",
    "* `L` : length of the rod\n",
    "* `f` : a function that represents the heat source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 201\n",
    "L = np.pi\n",
    "\n",
    "xs = np.array([0.2, 0.4, 0.6, 0.8])*L\n",
    "ws = 0.8\n",
    "sigma_s = 0.05\n",
    "def source(t):\n",
    "    s = np.zeros(N-1)\n",
    "    for i in range(4):\n",
    "        s += ws * sps.norm.pdf(t, loc=xs[i], scale=sigma_s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6a223",
   "metadata": {},
   "source": [
    "Then we can load the 1D Poisson forward model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, _, _ = cuqi.testproblem.Poisson1D(dim=dim, \n",
    "                                    endpoint=L,\n",
    "                                    field_type='KL',\n",
    "                                    field_params={'num_modes': 10} ,\n",
    "                                    map=lambda x: np.exp(x), \n",
    "                                    source=source).get_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074001c",
   "metadata": {},
   "source": [
    "We print the forward model to see its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765684",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152f551",
   "metadata": {},
   "source": [
    "We can look at the domain and range geometries of the forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_geometry)\n",
    "print(A.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04b0dd",
   "metadata": {},
   "source": [
    "These geometries are of type `Continuous1D` which represents a 1D continuous signal/field defined on a grid. We can view the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_geometry.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76189a89",
   "metadata": {},
   "source": [
    "Additionally, the properties `domain_dim` and `range_dim` of the forward model represent the dimension of the input and output of the forward model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_dim)\n",
    "print(A.range_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdec9b2",
   "metadata": {},
   "source": [
    "Let us create an array representing a constant conductivity over the grid nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_x_array = 20*np.ones(A.domain_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb466e",
   "metadata": {},
   "source": [
    "We can wrap the array in a `CUQIarray` object which is the main data structure in CUQIpy for variables (e.g. arrays and fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_x = CUQIarray(some_x_array, geometry=A.domain_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430cc57",
   "metadata": {},
   "source": [
    "Note that we pass `geometry=A.domain_geometry` to equip the `CUQIarray` object with the same geometry as the domain of the forward model, which is has information about what the values of the array represent (e.g. function values on a grid for this case).\n",
    "\n",
    "We can plot the conductivity array using the `plot` method of the `CUQIarray` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f935e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_x.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfba4d6",
   "metadata": {},
   "source": [
    "We can also evaluate the forward model at the conductivity array and plot the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_y = A(some_x)\n",
    "some_y.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e3a284",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise: </font>\n",
    "  * Is this true for you: **\"I can, mathematically, write a definition of a 1D Poisson forward model that maps the conductivity field to the temperature distribution field\"**? If not, what questions do you have?\n",
    "  * Trying a different conductivity profile: \n",
    "      * Create another constant conductivity profile `another_x` of value 20 as a `CUQIarray` object.\n",
    "      * Evaluate the forward model at the new conductivity profile and store the result in a variable `another_y`. Plot the solution.\n",
    "      * In one plot, compare the solutions `some_y` and `another_y` using the `plot` method of the `CUQIarray` object. What do you observe? and why?\n",
    "  * Experimenting with setting up the `map` in `Poisson1D`:\n",
    "      * Execute `help(Poisson1D)`\n",
    "      * Note the `map` parameter of `Poisson1D`. We can use this parameter to transform the conductivity field before solving the PDE. Create the forward model again, name it `A_map`, with setting up `map` to be `lambda x: np.exp(x)` to ensure that the conductivity is always positive.\n",
    "      * Inspect the domain and range geometries of the new forward model `A_map` what is different this time, and why?\n",
    "      * Create a CUQIarray object `x_with_map` with a constant conductivity profile of value 0 and evaluate the forward model at `x_with_map` and store it in variable `y_with_map`. Make sure to use the right geometry for `x_with_map`.\n",
    "      * Plot `x_with_map` using `x_with_map.plot()`, What do you observe?\n",
    "      * Plot the solution `y_with_map` and `some_y` in the same plot. What do you observe? and why?\n",
    "  * Experimenting with setting up the `field_type` in `Poisson1D`:\n",
    "      * Note the `field_type` parameter of `Poisson1D`. We can use this parameter to change the parameterization of the conductivity field. Create the forward model again, name it `A_step`, with setting both the `map` as in the previous exercise and the `field_type` to be `\"Step\"` to generate a step parameterization of the conductivity field.\n",
    "      * What is the domain and range geometries of the new forward model `A_step`?\n",
    "      * Create `x_step` to be `x_step = CUQIarray(np.arange(A_step.domain_dim)*0.1, geometry=A_step.domain_geometry)`.\n",
    "      * Plot `x_step` using `x_step.plot()`.\n",
    "      * What does the dimension `A_step.domain_dim` represent? and is it equal to the size of the domain geometry grid?\n",
    "      * Evaluate the forward model at `x_step` and store it in variable `y_step`. Plot the solution.\n",
    "  * Is this true for you: **\"I can load the pre-existing Poisson 1D forward model in CUQIpy and evaluate it on some input and visualize the input and output\"**? If not, what questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20720f95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e436eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5be5565",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016eddd3",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "We start of by importing the Python packages we need (including CUQIpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cuqi\n",
    "from cuqi.testproblem import Deconvolution1D, Deconvolution2D\n",
    "from cuqi.distribution import JointDistribution, Gaussian, CMRF, LMRF, GMRF\n",
    "from cuqi.sampler import LinearRTO, pCN, CWMH, ULA, MALA, NUTS\n",
    "from cuqi.problem import BayesianProblem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list of samplers!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining and sampling a Bayesian Inverse Problem (high-level)<a class=\"anchor\" id=\"BIP\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the posterior distribution.\n",
    "\n",
    "The posterior describes the probability distribution of the parameters we are interested in. This is achieved by combining prior knowledge of the parameters and observed data. In its most general form, the posterior is given by Bayes' theorem:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})} \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\boldsymbol{\\theta}$ is the parameter vector of *all* the parameters we are interested in inferring and $\\mathbf{y}$ is the observable data. In the simplest case one could have a single parameter vector of interest, say $\\boldsymbol{\\theta}=[\\mathbf{x}]$.\n",
    "\n",
    "The probability density function $p(\\boldsymbol{\\theta})$ is the prior distribution of the parameters.\n",
    "\n",
    "Given fixed observed data $\\mathbf{y}_\\mathrm{data}$, the term $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ considered as a function of $\\boldsymbol{\\theta}$ is known as the\n",
    "likelihood function or just *likelihood*, also denoted $L(\\boldsymbol{\\theta} \\mid \\mathbf{y} = \\mathbf{y}_\\mathrm{data})$, which we note is not a probability density but a density function.\n",
    "\n",
    "When $\\mathbf{y}$ is not fixed, $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ is a probability density function of the data $\\mathbf{y}$ given the value of the parameters $\\boldsymbol{\\theta}$. In CUQIpy we refer to this distribution as the *data distribution*.\n",
    "\n",
    "The denominator $p(\\mathbf{y})$ is the *evidence* and is a normalization constant (that we typically ignore because it does not affect the MCMC sampling) that ensures that the posterior integrates to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30f3764",
   "metadata": {},
   "source": [
    "### Note on Bayesian inverse problems with CUQIpy\n",
    "\n",
    "CUQIpy uses a general approach to Bayesian modeling that not only aims to define the posterior distribution, but instead to define the joint distribution of all the parameters. This more general approach is useful because it allows one to carry out more tasks related to uncertainty quantification of inverse problems such as prior-predictive checks, model checking, posterior-predictive checks and more. For more details on some of these topics see the overview in [1].\n",
    "\n",
    "In this notebook, we initially focus on how to define and sample a Bayesian inverse problem using the high-level interface in CUQIpy. We then later show a more low-level approach to defining the posterior distribution, which is useful for users who want to have more control the type of sampler used for sampling the posterior distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic forward model and observed data\n",
    "Consider a Bayesian inverse problem\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{e},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the (deterministic) forward model of the inverse problem and $\\mathbf{y}$ and $\\mathbf{x}$ are random variables representing the observed data and parameter of interest respectively. Here $\\mathbf{e}$ is a random variable representing additive noise in the data.\n",
    "\n",
    "For this example let us consider the `Deconvolution1D` testproblem and extract a CUQIpy forward model and some synthetic data denoted $\\mathbf{y}_\\mathrm{data}$ (a realization of $\\mathbf{y}$). \n",
    "\n",
    "Note that this is a linear inverse problem, but the same approach can be used for nonlinear inverse problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# For convenience, we define the dimension of the domain of A\n",
    "n = A.domain_dim\n",
    "\n",
    "# For convenience, we extract the exact solution as x_exact\n",
    "x_exact = probInfo.exactSolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem.\n",
    "\n",
    "Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the `probInfo` variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); x_exact.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "984a1531",
   "metadata": {},
   "source": [
    "## 1.2 Distributions for parameters and data\n",
    "\n",
    "The goal now is to define distributions for each the random variables that represent the parameters and data in the above-mentioned Bayesian inverse problem. One way to think about this, is that we are trying to define a statistical model for the data generating process (see reference [1]), which we call a *Bayesian Problem* in CUQIpy.\n",
    "\n",
    "For the unknown $\\mathbf{x}$, we use a-priori knowledge to define its distribution. In this case, the sinc phantom in the Deconvolution test problem can be represented fairly well by a distribution that generates smooth, but spatially correlated realizations.\n",
    "\n",
    "One such distribution is the Gaussian Markov Random Field (GMRF) distribution. This distribution assumes a Gaussian distribution on the differences between neighboring elements of $\\mathbf{x}$, i.e. in 1D:\n",
    "\n",
    "\\begin{align*}\n",
    "x_i - x_{i-1} \\sim \\mathrm{Gaussian}(0, d^{-1}), \\quad i=1, \\ldots, n,\n",
    "\\end{align*}\n",
    "\n",
    "where we purposely leave out the details on the boundary conditions for this notebook.\n",
    "\n",
    "To simplify the notation, we denote by *GMRF* the distribution that induces this property on a vector $\\mathbf{x}$ defined by its mean and precision $d$. That is, the above can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d),\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "with some choice of the precision say $d=50$. For more details on GMRF see the first CUQIpy paper [2].\n",
    "\n",
    "The GMRF distribution is implemented in CUQIpy as [GMRF class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.GMRF.html#cuqi.distribution.GMRF) and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior precision\n",
    "d = 50\n",
    "\n",
    "# Define GMRF prior (zero boundary conditions are the default)\n",
    "x = GMRF(np.zeros(n), d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e5008c",
   "metadata": {},
   "source": [
    "From the problem info string earlier, we saw that the noise is Gaussian with standard deviation $s_\\mathbf{e}=0.01$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{e} &\\sim \\mathrm{Gaussian}(\\mathbf{0}, s_\\mathbf{e}^2 \\mathbf{I}).\n",
    "\\end{align*}\n",
    "\n",
    "Instead of working directly with the noise variable $\\mathbf{e}$, we can include the stochastic element of the noise in the random variable $\\mathbf{y}$ representing the data.\n",
    "\n",
    "Because the data depend on $\\mathbf{x}$ we are really interested in defining the *data distribution* for $\\mathbf{y} \\mid \\mathbf{x}$ and since the noise is the only stochastic element of $\\mathbf{y}$ when $\\mathbf{x}$ is fixed we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} \\mid \\mathbf{x} &\\sim \\mathrm{Gaussian}(\\mathbf{A}\\mathbf{x}, s_\\mathbf{y}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "where $s_\\mathbf{e}=s_\\mathbf{y} = 0.01$.\n",
    "\n",
    "Notice that this definition depends both on the forward model $\\mathbf{A}$ and the random variable $\\mathbf{x}$.\n",
    "\n",
    "In CUQIpy, we can define the distribution for $\\mathbf{y} \\mid \\mathbf{x}$ matching the mathematical expression using our previously defined variables `A` and `x` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise standard deviation\n",
    "s_y = 0.01\n",
    "\n",
    "# Define data distribution\n",
    "y = Gaussian(A@x, s_y**2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Have a look at the distributions for $\\mathbf{x}$ and $\\mathbf{y}$ by calling `print` on them in the code-cell below. \n",
    "- How are the distributions different?\n",
    "- Is it clear that the distribution for $\\mathbf{y}$ is a conditional distribution?\n",
    "- Can you generate and plot a realization (sample) of $\\mathbf{x}$? Does the realization show spatial correlation?\n",
    "- How about generating a sample from $\\mathbf{y}\\mid\\mathbf{x}$ given some fixed value of $\\mathbf{x}$? What does a sample represent in this case?\n",
    "\n",
    "**Hint:** to condition `y` on a value e.g. `x_exact` the syntax is `y(x=x_exact)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc2f4a9",
   "metadata": {},
   "source": [
    "### Note on notation\n",
    "\n",
    "It is common (for convenience in terms of notation) not to explicitly write the dependance of each random variable when specifying a complete Bayesian problem. For example, for the case above one would often write\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d)\\\\\n",
    "\\mathbf{y} &\\sim \\mathrm{Gaussian}(\\mathbf{A}\\mathbf{x}, s_\\mathbf{y}^2 I),\n",
    "\\end{align*}\n",
    "\n",
    "where the dependance of $\\mathbf{y}$ on $\\mathbf{x}$ is implicit.\n",
    "\n",
    "This compact notation completely specifies the Bayesian problem for the so-called *data generating process*, making clear all the assumptions about the data and parameters.\n",
    "\n",
    "In CUQIpy - when all deterministic parameters and forward models are defined - the Bayesian problem is written in code using almost exactly the same syntax as the mathematical notation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian problem (repeated in case previous cells were modified)\n",
    "x = GMRF(np.zeros(n), 50)\n",
    "y = Gaussian(A@x, s_y**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Bayesian inverse problem\n",
    "The simplest way to sample a Bayesian inverse problem in CUQIpy is to use the [BayesianProblem class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "\n",
    "Using the BayesianProblem class, one can easily define and sample from the posterior distribution of a Bayesian inverse problem by providing the distributions for the parameters and data and subsequently setting the observed data.\n",
    "\n",
    "Calling the `UQ` method on the BayesianProblem object will automatically sample the posterior distribution using a suitable choice of sampling algorithm and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest way to do uncertainty quantification in CUQIpy:\n",
    "\n",
    "BP = BayesianProblem(x, y)      # Create Bayesian problem\n",
    "BP.set_data(y=y_data)           # Provide observed data\n",
    "samples = BP.UQ(exact=x_exact)  # Run UQ analysis (comparing with exact solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we provided our assumptions about the data generating process by defining the distributions for the parameters and data and provided the observed data for the problem. CUQIpy then automatically sampled the posterior distribution and a credibility interval for the parameter $\\mathbf{x}$ as well as the mean of the posterior was plotted and compared to the ground truth (x_exact).\n",
    "\n",
    "We see that compared to the ground truth, the mean of the posterior is a good estimate and the credibility interval contains the ground truth.\n",
    "\n",
    "In the next section, we show how to define the posterior distribution more explicitly and how to sample it with a specific choice of sampler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb3bf801",
   "metadata": {},
   "source": [
    "# 2 Defining and sampling a Bayesian Inverse Problem (low-level)<a class=\"anchor\" id=\"Joint\"></a>\n",
    "\n",
    "Instead of relying on the BayesianProblem class to automatically sample the posterior distribution (which does not always work well), we can also define the posterior distribution explicitly and sample it using a specific sampler in CUQIpy. This requires a bit more work, but it gives us more control over the sampling process.\n",
    "\n",
    "## 2.1 Defining the joint distribution\n",
    "In the Bayesian framework, we are interested in the joint distribution over $\\mathbf{x}$ and $\\mathbf{y}$. The joint distribution is defined as the product of the individual probability density functions. In our simple case, the joint distribution can be described in density form as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y},\\mathbf{x}) = p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In CUQIpy, if we want more control, we can as an alternative to using BayesianProblem, instead define the joint distribution ourselves by passing each of the distributions as arguments to the [JointDistribution class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.JointDistribution.html#cuqi.distribution.JointDistribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035c5126",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09194832",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we are not going to dive further into the details of the joint distribution, and simply use it to define the posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d9ca744",
   "metadata": {},
   "source": [
    "## 2.2 Defining the posterior distribution\n",
    "\n",
    "To define the posterior distribution, we have to condition the joint distribution $p(\\mathbf{y},\\mathbf{x})$ on the observed data, say $\\mathbf{y}_\\mathrm{data}$. This is done by using Bayes' theorem, which in our simple case can easily be derived as\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data}) \\propto L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data}) := p(\\mathbf{y}=\\mathbf{y}_\\mathrm{data} \\mid \\mathbf{x})$ for the likelihood function to emphasize that, in the context of the posterior where $\\mathbf{y}$ is fixed to $\\mathbf{y}_\\mathrm{data}$, it is a function of $\\mathbf{x}$ and not on $\\mathbf{y}$. In CUQIpy we sometimes use the short-hand printing style `L(x|y)` for brevity.\n",
    "\n",
    "CUQIpy can automatically derive the posterior distribution for any joint distribution by passing the observed data as an argument to the \"call\" (condition) method of the joint distribution.  This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=y_data) # Condition p(x,y) on y=y_data. Applies Bayes' rule automatically\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ab207d",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it. Notice that the posterior equation matches the mathematical expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9def31",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "Try evaluating the posterior log probability density function (logpdf) at some point say $\\mathbf{1}$ and $2\\cdot\\mathbf{1}$, where $\\mathbf{1}$ is a ones-vector.\n",
    "\n",
    "Try evaluating the posterior pdf at the same points. Can you explain why the pdf gives the same result for both points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "# 2.3 Sampling the posterior\n",
    "\n",
    "Now that we have defined the posterior distribution for our parameter of interest $\\mathbf{x}$ given $\\mathbf{y}_\\mathrm{data}$, we can characterize the parameter and its uncertainty by samples from the posterior distribution. However, in general the posterior is not a simple distribution that we can easily sample from. Instead, we need to rely on Markov Chain Monte Carlo (MCMC) methods to sample from the posterior.\n",
    "\n",
    "In CUQIpy, a number of MCMC samplers are provided in the sampler module that can be used to sample probability distributions. All samplers have the same signature, namely `Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above, which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize [(LinearRTO)](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.LinearRTO.html#cuqi.sampler.LinearRTO) sampler is a good choice to efficiently generate samples. This is also the sampler chosen by the BayesianProblem class for this problem.\n",
    "\n",
    " Like any of the other samplers, we set up the sampler by simply providing the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = LinearRTO(posterior)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "After the sampler is defined we can compute samples via the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, the returned object is a `cuqi.samples.Samples` object.\n",
    "\n",
    "This object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the chain has converged and there is little need for removing samples that are part of a \"burn-in\" period. In practice, the samples should be inspected with more rigor to ensure that the MCMC chain has converged, but this is outside the scope of this notebook.\n",
    "\n",
    "The good sampling is in large part due to the LinearRTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see [samples.burnthin](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.samples/cuqi.samples.Samples.burnthin.html#cuqi.samples.Samples.burnthin)) and store the \"burnthinned\" samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from `probInfo`).\n",
    "\n",
    "This is what the `UQ` method of the BayesianProblem class did under the hood for us earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "779d95f1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The LinearRTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the logpdf. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers (click the links to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from LinearRTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as LinearRTO or NUTS. For some samplers, the method [sample_adapt](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.sample_adapt.html#cuqi.sampler.NUTS.sample_adapt) will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate and a burn-in should be added to specify how many samples to use for the adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "# 3. Exploring different prior choices<a class=\"anchor\" id=\"ModifyPriors\"></a>\n",
    "\n",
    "In the above example, we used a GMRF prior for the parameter $\\mathbf{x}$. However, it is not always obvious what prior to use for a given problem. In such cases, it is often useful to try out different priors to see how they affect the posterior distribution.\n",
    "\n",
    "In CUQIpy, it is easy to modify the prior and re-sample the posterior distribution. This is most easily done by using the BayesianProblem class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "####  Try yourself (optional):  \n",
    "\n",
    "Please carry out the following exercise to see how the prior affects the posterior distribution. \n",
    "\n",
    "Note that: here we use the [sample_posterior](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.sample_posterior.html#cuqi.problem.BayesianProblem.sample_posterior) method of the BayesianProblem class to sample the posterior distribution and store the samples without plotting. We then manually plot the samples using the `plot_ci` method of the `Samples` object.\n",
    "\n",
    "- Try another prior such as [LMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.LMRF.html#cuqi.distribution.LMRF) or [CMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.CMRF.html#cuqi.distribution.CMRF) for the 1D case (look up appropriate arguments in the documentation) using [`BayesianProblem`](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "- Try switching the testproblem from Deconvolution1D to [Deconvolution2D](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.testproblem/cuqi.testproblem.Deconvolution2D.html#cuqi.testproblem.Deconvolution2D) (look up appropriate arguments in the documentation).\n",
    "- You may also try defining your own Bayesian inverse problem using this interface. ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can modify this code or write your own from scratch\n",
    "\n",
    "# 1. Forward model and data\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# 2. Distributions\n",
    "x = GMRF(np.zeros(A.domain_dim), 50) # Try e.g. LMRF or CMRF (also update scale parameters!)\n",
    "y = Gaussian(A@x, 0.01**2)\n",
    "\n",
    "# 3. Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "\n",
    "# 4. Sample posterior\n",
    "samples = BP.sample_posterior(500)\n",
    "\n",
    "# 5. Analyze posterior\n",
    "samples.plot_ci(exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0583cafe",
   "metadata": {},
   "source": [
    "You may have noticed that finding suitable parameters for the prior could be a challenge. To see how to automatically find suitable parameters for the prior, see the [Gibbs notebook](Exercise06_Gibbs.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097194b0",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine in case something was changed\n",
    "\n",
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = GMRF(np.zeros(A.domain_dim), 50)\n",
    "y = Gaussian(mean=A@x, cov=0.01**2)\n",
    "\n",
    "# Define Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP() # Maximum a posteriori estimate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2931bb8f",
   "metadata": {},
   "source": [
    "The automatic solver selection is also still work-in-progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24cab183",
   "metadata": {},
   "source": [
    "After we have computed the MAP, we can then estimate to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a931f3a-9534-48b1-a273-5bc8616954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching to the Deconvolution2D testproblem. You may have to play with the prior standard deviation to get a good MAP estimate.\n",
    "- Try switching the prior to a CMRF distribution for the 1D case. Does the MAP estimate change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5adf1d",
   "metadata": {},
   "source": [
    "## Story 1: MH adaptive scaling, donuts (it pays off to adapt)\n",
    "\n",
    "## Sample a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0715bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH\n",
    "import time\n",
    "\n",
    "target = DistributionGallery(\"donut\")\n",
    "\n",
    "\n",
    "print(target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MH\n",
    "# try scale = 0.1, 1, 10\n",
    "Ns = 30000\n",
    "Nb = 5000\n",
    "scale = 0.05\n",
    "MH_sampler = MH(target, scale=scale, x0=np.array([0,0]))\n",
    "ti = time.time()\n",
    "MH_fixed_samples = MH_sampler.sample(Ns, Nb)\n",
    "print('Elapsed time MH:', time.time() - ti, '\\n')\n",
    "MH_fixed_samples.plot_pair(ax=plt.gca())\n",
    "\n",
    "\n",
    "# try sample adapt\n",
    "MH_adapted_samples = MH_sampler.sample_adapt(Ns, Nb)\n",
    "print('Elapsed time MH:', time.time() - ti, '\\n')\n",
    "\n",
    "#set defult color matplotlib\n",
    "MH_adapted_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'r'})\n",
    "\n",
    "\n",
    "#CWMH_sampler = CWMH(target, scale=scale, x0=np.array([0,0]))\n",
    "#CWMH_fixed_samples = CWMH_sampler.sample(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_fixed_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'g'})\n",
    "#\n",
    "#\n",
    "#CWMH_adapted_samples = CWMH_sampler.sample_adapt(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_adapted_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'y'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8539c",
   "metadata": {},
   "source": [
    "what do you notice about acceptance rate?\n",
    "scale?\n",
    "plot diagnostics, ESS\n",
    "\n",
    "try 60000 / 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe36d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MH_fixed_samples.compute_ess())\n",
    "print(MH_adapted_samples.compute_ess())\n",
    "MH_fixed_samples.plot_trace()\n",
    "MH_adapted_samples.plot_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0d888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4415e99f",
   "metadata": {},
   "source": [
    "## story 2: Poisson, CW vs MH  (not all unkowns are created equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aff035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson inverse problem\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.distribution import GMRF, Gaussian, CMRF\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.sampler import MH, CWMH, pCN\n",
    "\n",
    "# Load forward model, data and problem information\n",
    "n = 20\n",
    "Ns = 3000\n",
    "Nb = 2000\n",
    "cov = 4.62905925e-05\n",
    "A, y_data, probInfo = Poisson1D(dim=n).get_components()\n",
    "\n",
    "x = GMRF(np.zeros(A.domain_dim), 8, geometry=A.domain_geometry,  order=1)\n",
    "y = Gaussian(A(x), cov)\n",
    "\n",
    "BP = BayesianProblem(x, y).set_data(y=y_data)\n",
    "posterior = BP.posterior()\n",
    "\n",
    "scale = 0.06\n",
    "MH_sampler = MH(posterior, scale = scale, x0=np.ones(n))\n",
    "MH_samples = MH_sampler.sample_adapt(Ns, Nb)\n",
    "\n",
    "\n",
    "CWMH_sampler = CWMH(posterior, scale = scale, x0=np.ones(n))\n",
    "CWMH_samples = CWMH_sampler.sample_adapt(Ns, Nb)\n",
    "int(0.1*Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_samples.burnthin(int(0.1*(Ns))).plot_ci(95, exact=probInfo.exactSolution)\n",
    "plt.figure()\n",
    "CWMH_samples.burnthin(int(0.1*Ns)).plot_ci(95, exact=probInfo.exactSolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWMH_sampler.scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22010fa2",
   "metadata": {},
   "source": [
    "time both\n",
    "insptect scale for both\n",
    "use KL expansion and compare both samplers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d58c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MH_samples.compute_ess())\n",
    "print(CWMH_samples.compute_ess())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Poisson1D(dim=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.likelihood.distribution.cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3abda6",
   "metadata": {},
   "source": [
    "## Story 3: Gradient info, ULA (univariate, bias) (Wandering randomly or be Guided by the force / let the force guide you)\n",
    "\n",
    "## Story 4: Gradient info, MALA (is it ever possible to fix the bias)\n",
    "\n",
    "## Story 5: NUTS (it is time to go nuts with nuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5628eb2",
   "metadata": {},
   "source": [
    "inspect sampler / its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e027a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
