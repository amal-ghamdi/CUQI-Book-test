{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a7f74",
   "metadata": {},
   "source": [
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\dd{\\mathrm{d}}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\abs#1{\\left\\vert#1\\right\\vert}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\ve#1{\\bm{#1}}$\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\mat#1{\\mathbf{#1}}$\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Two target distributions\n",
    "\n",
    "In this notebook, we define two target distributions:\n",
    "- a bi-variate \"donut\" distribution,\n",
    "- and a posterior distribution for a 1D Poisson-based BIP.\n",
    "that we will use in this chapter to illustrate sampling with CUQIpy. The former target is used for illustrative purposes and is not associated with an inverse problem, while the latter is a more realistic example of a BIP.\n",
    "\n",
    "We also show high level usage of the `BayesianProblem` class to explore the posterior distribution as well as defining the target distribution in a more low-level approach through the `JointDistribution` class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169478f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery, Gaussian, JointDistribution\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.problem import BayesianProblem\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH\n",
    "import time\n",
    "import scipy.stats as sps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d37a81",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot2d(val, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    # plot\n",
    "    pixelwidth_x = (x1_max-x1_min)/(N2-1)\n",
    "    pixelwidth_y = (x2_max-x2_min)/(N2-1)\n",
    "\n",
    "    hp_x = 0.5*pixelwidth_x\n",
    "    hp_y = 0.5*pixelwidth_y\n",
    "\n",
    "    extent = (x1_min-hp_x, x1_max+hp_x, x2_min-hp_y, x2_max+hp_y)\n",
    "\n",
    "    plt.imshow(val, origin='lower', extent=extent)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def plot_pdf_2D(distb, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    N2 = 201\n",
    "    ls1 = np.linspace(x1_min, x1_max, N2)\n",
    "    ls2 = np.linspace(x2_min, x2_max, N2)\n",
    "    grid1, grid2 = np.meshgrid(ls1, ls2)\n",
    "    distb_pdf = np.zeros((N2,N2))\n",
    "    for ii in range(N2):\n",
    "        for jj in range(N2):\n",
    "            distb_pdf[ii,jj] = np.exp(distb.logd(np.array([grid1[ii,jj], grid2[ii,jj]]))) \n",
    "    plot2d(distb_pdf, x1_min, x1_max, x2_min, x2_max, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed3a59",
   "metadata": {},
   "source": [
    "## <font color=#CD853F> The \"donut\" distribution </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "In CUQIpy, we provide a set of bi-variate distributions for illustrative purposes. One of these is the \"donut\" distribution, which is a bi-variate distribution of a donut-shaped. The distribution is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "log (p(\\mathbf{x})) \\propto - \\frac{1}{\\sigma_\\text{donut}^2} \\left( \\left\\| \\mathbf{x} \\right\\| - r_\\text{donut} \\right)^2\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x} = (x_1, x_2)$ is a 2D vector, $\\left\\| \\mathbf{x} \\right\\|$ is the Euclidean norm of $\\mathbf{x}$, $r_\\text{donut}$ is the radius of the donut, and $\\sigma_\\text{donut}$ is a scalar value that controls the width of the \"donut\".\n",
    "\n",
    "To load the \"donut\" distribution, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_donut = DistributionGallery(\"donut\")\n",
    "\n",
    "print(target_donut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649d363",
   "metadata": {},
   "source": [
    "We can plot the distribution probability density function (pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce235016",
   "metadata": {},
   "source": [
    "\n",
    "## <font color=#CD853F> A 1D Poisson-based BIP </font> <a name=\"r-donut\"></a>\n",
    "\n",
    "##### <font color=#8B4513> The forward model </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "Consider a heat conductive rod of length $L = \\pi$ with a varying conductivity (the conductivity of the rod changes from point to point). We fix the temperature at the end-points of the rod and apply a heat source distributed along the length of the rod. We wait until the rod reaches an equilibrium temperature distribution. The equilibrium temperature of the rod is modelled using the second order steady-state PDE as\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& \\dfrac{\\dd}{\\dd \\xi}\\left(u(\\xi) \\dfrac{\\dd y(\\xi)}{\\dd \\xi}\\right) = -f(\\xi), \\quad & \\xi\\in (0,L) \\\\\n",
    "& y(0) = y(L) = 0.\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "Here, $y$ represents the temperature distribution along the rod, $u(\\xi) $ is the unknown conductivity of the rod and $f(\\xi)$ is a deterministic heat source given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tf(\\xi) = 10\\exp( -\\frac{ (\\xi - L/2)^2} {0.02} ).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To ensure that the conductivity of the rod is non-negative, we parameterize $u$ by a random variable $x$ as follows:\n",
    " \n",
    "$$\n",
    " \\begin{aligned}\n",
    " u( \\cdot  ) = \\exp( x( \\cdot  ) )\n",
    " \\end{aligned}\n",
    "$$\n",
    "where $x$ is not necessarily positive.\n",
    "\n",
    "Let us load the forward model that maps the random variable $x$ to the temperature distribution $y$ in CUQIpy. We will use the following parameters:\n",
    "* `dim` : number of discretization points for the rod\n",
    "* `L` : length of the rod\n",
    "* `f` : a function that represents the heat source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 201\n",
    "L = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60eb4a9",
   "metadata": {},
   "source": [
    "The source term represents spikes at four locations `xs` with weight `ws`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef704b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([0.2, 0.4, 0.6, 0.8])*L\n",
    "ws = 0.8\n",
    "sigma_s = 0.05\n",
    "def f(t):\n",
    "    s = np.zeros(dim-1)\n",
    "    for i in range(4):\n",
    "        s += ws * sps.norm.pdf(t, loc=xs[i], scale=sigma_s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af772e",
   "metadata": {},
   "source": [
    "Let us plot the source term for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grid = np.linspace(0, L, dim-1)\n",
    "plt.plot(temp_grid, f(temp_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6a223",
   "metadata": {},
   "source": [
    "Then we can load the 1D Poisson forward model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, _, _ = Poisson1D(dim=dim, \n",
    "                    endpoint=L,\n",
    "                    field_type='KL',\n",
    "                    field_params={'num_modes': 10} ,\n",
    "                    map=lambda x: np.exp(x), \n",
    "                    source=f).get_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074001c",
   "metadata": {},
   "source": [
    "We print the forward model to see its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765684",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3308c",
   "metadata": {},
   "source": [
    "Let us look at the `pde` property of the forward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.pde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152f551",
   "metadata": {},
   "source": [
    "We can look at the domain and range geometries of the forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_geometry)\n",
    "print(A.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99369a8",
   "metadata": {},
   "source": [
    "And inspect the domain geometry further. Let us look at the mapping in the `MappedGeometry` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(A.domain_geometry.map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f9a8c",
   "metadata": {},
   "source": [
    "We can extract the underlying geometry of the `MappedGeometry` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_geometry = A.domain_geometry.geometry\n",
    "print(underlying_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f923c2",
   "metadata": {},
   "source": [
    "The underlying geometry represents a Karhunen–Loève (KL) expansion of a random field. Let us look at some of the properties of this `underlying_geometry` such as the number of modes in the KL expansion and the grid on which the KL expansion basis functions are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666dc5",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "print(A.domain_geometry.geometry.num_modes)\n",
    "print(A.domain_geometry.geometry.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04b0dd",
   "metadata": {},
   "source": [
    "The range geometry is of type `Continuous1D` which represents a 1D continuous signal/field defined on a grid. We can view the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d6938",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "print(A.domain_geometry.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76189a89",
   "metadata": {},
   "source": [
    "Additionally, the properties `domain_dim` and `range_dim` of the forward model represent the dimension of the input and output of the forward model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.domain_dim)\n",
    "print(A.range_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a229d",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the prior </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "We now build a posterior distribution based on this forward model. The unknown  $\\mathbf{x}$ represents the coefficients in the KL expansion. We assume that the prior distribution of $\\mathbf{x}$ is an i.i.d Gaussian distribution with mean $0$ and variance $\\sigma_x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x = 30\n",
    "x = Gaussian(0, sigma_x**2, geometry=A.domain_geometry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3eb9fc",
   "metadata": {},
   "source": [
    "Let us assume that the true solution we want to infer is a sample from `x`. Note: we fix the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "x_true = x.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1b87c",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "- Visualize `x_true` in the KL coefficient space. Hint: try `x_true.plot(plot_par=True)`\n",
    "- Visualize `x_true` in the corresponding function space (after applying the linear combination of KL basis weighted by KL vectors and then applying the exponantial mapping). Hint: all this can be achieved in one line by `x_true.plot(plot_par=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11499ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a1e04",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the likelihood </font> <a name=\"r-forward-model\"></a>\n",
    "\n",
    "We assume the data we obtain is a noisy measurement of the temperature $y$ over the interval $[0, L]$ in all grid points. The measurements form a vector $\\mathbf{y}$. The noise is assumed to be additive Gaussian noise with mean $0$ and variance $\\sigma_y^2$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A}(\\mathbf{x}) + \\epsilon  \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2).\n",
    "\n",
    "$$\n",
    "\n",
    "We define the data distribution $p(\\mathbf{y} | \\mathbf{x})$ in `CUQIpy` in this case as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = np.sqrt(0.001)\n",
    "y = Gaussian(A(x), sigma_y**2, geometry=A.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19409",
   "metadata": {},
   "source": [
    "We create a synthetic data to use it to test solving our BIP.  We denote this data as $\\mathbf{y}_{\\text{obs}}$ which is a particular observed data realization from a setup where the KL coefficients are `x_true`. To create this data in `CUQIpy`, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = y(x=x_true).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e2df5",
   "metadata": {},
   "source": [
    "Let us plot the true conductivity field, corresponding to `x_true`, the data `y_true` without noise i.e. `A(x_true)`, and the noisy data `y_obs` in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs.plot(label='y_obs')\n",
    "A(x_true).plot(label='y_true')\n",
    "x_true.plot(label='x_true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33525819",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the posterior distribution  (the high level approach: using the BayesianProblem class)</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846ffa3",
   "metadata": {},
   "source": [
    "The posterior distribution of the Bayesian inverse problem in this case is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs}) \\propto L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs}) := p(\\mathbf{y}=\\mathbf{y}_\\mathrm{obs} \\mid \\mathbf{x})$ for the likelihood function to emphasize that, in the context of the posterior where $\\mathbf{y}$ is fixed to $\\mathbf{y}_\\mathrm{obs}$, it is a function of $\\mathbf{x}$ and not on $\\mathbf{y}$. In CUQIpy we sometimes use the short-hand printing style `L(x|y)` for brevity.\n",
    "\n",
    "\n",
    "\n",
    "The simplest way to sample a Bayesian inverse problem in CUQIpy is to use the [BayesianProblem class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "\n",
    "Using the BayesianProblem class, one can easily define and sample from the posterior distribution of a Bayesian inverse problem by providing the distributions for the parameters and data and subsequently setting the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_poisson = BayesianProblem(x, y)      # Create Bayesian problem\n",
    "BP_poisson.set_data(y=y_obs)           # Provide observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c578f",
   "metadata": {},
   "source": [
    "In the above example, we provided our assumptions about the data generating process by defining the distributions for the parameters and data and provided the observed data for the problem. `CUQIpy` internally created the posterior distribution using the provided distributions and data. \n",
    "\n",
    "We can use this object to sample from the posterior distribution using the `UQ` method, which we will experiment with in this exercise:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfc7de",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "Use the `UQ` method of the `BP_poisson` object to sample the posterior distribution. The `UQ` returns a `Samples` object, store the result in a variable called `BP_poisson_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2cd3e",
   "metadata": {},
   "source": [
    "In the previous exercise we saw that `CUQIpy` automatically decided on using a sampler, preconditioned Crank Nicolson `pCN` in this case, and sampled the posterior distribution. Additionally, the  credibility interval for the parameter $\\mathbf{x}$ as well as the mean of the posterior was plotted and compared to the ground truth (`x_true`).\n",
    "\n",
    "**Note about visualizing the credible interval**:\n",
    "Using the `UQ` method, the credibility interval is computed for the KL coefficients. Then mapped to the function space and plotted. We can also compute the credibility interval directly on the function values. We will revisit this at a later stage.\n",
    "\n",
    "\n",
    "In the next section, we show how to define the posterior distribution more explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b118e",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: computing the MAP point with the BayesianProblem class</font>\n",
    "\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{obs}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The easiest way to compute the MAP estimate in CUQIpy is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_map = BP_poisson.MAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cc0e2",
   "metadata": {},
   "source": [
    "We can plot the MAP estimate and compare it to the true solution `x_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb70b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_map.plot(label='MAP estimate')\n",
    "x_true.plot(label='True solution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d0254",
   "metadata": {},
   "source": [
    "We can also look at the MAP estimate in the KL coefficient space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7123dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_map.plot(label='MAP estimate' ,plot_par=True)\n",
    "x_true.plot(label='True solution' ,plot_par=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0f8aa",
   "metadata": {},
   "source": [
    "We notice that in general the larger the mode number, the harder it is to be inferred (can you think why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad108f0",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> The BIP: the posterior distribution  (the low level approach: using the JointDistribution)</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5404b4",
   "metadata": {},
   "source": [
    "To define the posterior distribution explicitly in CUQIpy, we first define the joint distribution $p(\\mathbf{y},\\mathbf{x})$, then we supply the observed data to create the conditional distribution $p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs})$.\n",
    "\n",
    "\n",
    "Let us first define the joint distribution $p(\\mathbf{y},\\mathbf{x})$ in CUQIpy. We use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a45e5",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description of the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4732984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cf449",
   "metadata": {},
   "source": [
    "CUQIpy can automatically derive the posterior distribution for any joint distribution when we pass the observed data as an argument to the \"call\" (condition) method of the joint distribution.  This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a495cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_poisson = joint(y=y_obs) # Condition p(x,y) on y=y_data. Applies Bayes' rule automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b83148",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it. Notice that the posterior equation matches the mathematical expression we showed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_poisson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ea355",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise </font> <a name=\"r-forward-model\"></a>\n",
    "- The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "- Try evaluating the posterior log probability density function (logpdf) and pdf at some points say `x_true` and `x_true*1.1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
