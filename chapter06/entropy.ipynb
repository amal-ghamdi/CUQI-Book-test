{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the entropy of a distribution, numerically\n",
    "The informativeness of a prior distribution is linked to its entropy. The larger the entropy, the less informative the prior distribution is. The entropy of a univariate continuous distribution with probability density function $p(x)$ is defined as:\n",
    "\n",
    "$$\n",
    "H(p) = -\\int_{-\\infty}^{\\infty} p(x) \\log(p(x)) dx\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Here, we will compute the entropy of continuous univariate distributions numerically to compare their informativeness relative to each other. Let us define a Gaussian distribution `x`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import Gaussian\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Gaussian(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a `lambda` function for the entropy integrand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_integrand = lambda dist, val: dist.pdf(val)*dist.logd(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the entropy, we can use scipy's `quad` function to integrate the entropy integrand over the support of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as sp_integrate\n",
    "x_entropy = -1*sp_integrate.quad(lambda val: entropy_integrand(x, val),\n",
    "                                     -np.inf, np.inf)[0]\n",
    "print(\"Entropy of x: \")\n",
    "print(x_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513>Exercise:</font>\n",
    "1. Compute the entropy of the distribution `y = Gaussian(1, 0.1)` using the same method as above, what is the effect of changing the variance of the distribution on the entropy?\n",
    "2. Similarly, compute the entropy of the distribution `z = Uniform(-3, 3)` and compare with the entropy of the two Gaussian distributions. Hint: use -3 and 3 as the limits of the integration.\n",
    "3. Which of all the distributions has the highest entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
