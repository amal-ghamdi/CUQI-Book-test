{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the entropy of a distribution, numerically\n",
    "\n",
    "You have studied the maximum entropy concept of choosing non-informative priors during the [lectures](https://learn.inside.dtu.dk/d2l/le/lessons/210042/topics/808891) this week. Here, we will compute the entropy of a distribution numerically. Let us define a Gaussian distribution `x_ent`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import Gaussian\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ent = Gaussian(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a `lambda` function for the entropy integrand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_integrand = lambda dist, val: dist.pdf(val)*dist.logd(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the entropy, we can use scipy's `quad` function to integrate the entropy integrand over the support of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as sp_integrate\n",
    "x_ent_entropy = -1*sp_integrate.quad(lambda val: entropy_integrand(x_ent, val),\n",
    "                                     -np.inf, np.inf)[0]\n",
    "print(x_ent_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513>Exercise:</font>\n",
    "1. Compute the entropy of the distribution `y_ent = Gaussian(1, 0.1)` using the same method as above, what is the effect of changing the variance of the distribution on the entropy?\n",
    "2. Similarly, compute the entropy of the distribution `z_ent = Uniform(-3, 3)` and compare with the entropy of the two Gaussian distributions. Hint: use -3 and 3 as the limits of the integration.\n",
    "3. Which of all the distributions has the highest entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
