{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Exercise B03: Bayesian Inverse Problems\n",
    "\n",
    "In this notebook, we explore uncertainty quantification for inverse problems through Bayesian inference using CUQIpy.\n",
    "\n",
    "Essentially, inverse problems aim to infer a set of unknown parameters of interest given a forward model and observed data. Within the realm of uncertainty quantification, the focus is on the uncertainty associated with these inferred parameters.\n",
    "\n",
    "In the Bayesian framework, uncertainty is quantified by a probability distribution over the parameter space. To estimate this uncertainty, Bayesian inference utilizes Bayes' theorem to define the *posterior distribution* of the parameters by combining prior knowledge and observed data.\n",
    "\n",
    "This notebook aims to demonstrate the use of CUQIpy to combine the all the components required for Bayesian inference in a way that closely matches the mathematical formalism.\n",
    "\n",
    "The online documentation for CUQIpy can be found at [https://cuqi-dtu.github.io/CUQIpy/](https://cuqi-dtu.github.io/CUQIpy/). Throughout the notebook, we will link to the relevant sections of the documentation for further details.\n",
    "\n",
    "## Learning objectives\n",
    "Going through the notebook, you will see how to quantify the uncertainty for an example inverse problem using the CUQIpy library by learning how to:\n",
    "\n",
    "* Define distributions for each of the relevant parameters in an inverse problem.\n",
    "* Define and sample a Bayesian inverse problem.\n",
    "* Construct explicitly a posterior distribution by conditioning the joint distribution on observed data.\n",
    "* Sample a posterior distribution with specific choice of sampler.\n",
    "* Analyze the samples from the posterior distribution.\n",
    "* Compute point estimates of posterior, e.g., MAP or ML.\n",
    "* Describe how the high-level \"BayesianProblem\" combines the above steps into a convenient non-expert interface.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Defining and sampling a Bayesian Inverse Problem (high-level)](#BIP)\n",
    "2. [Defining and sampling a Bayesian Inverse Problem (low-level)](#Joint)\n",
    "3. [Modifying priors](#ModifyPriors)\n",
    "4. [Computing point estimates of the posterior](#pointestimates) ★\n",
    "\n",
    "The section marked with ★ is optional and can be skipped if you are short on time.\n",
    "\n",
    "## References\n",
    "[1] Gelman, Andrew, et al. \"Bayesian workflow.\" arXiv preprint arXiv:2011.01808 (2020) https://arxiv.org/abs/2011.01808.\n",
    "\n",
    "[2] Riis, Nicolai AB, et al. \"CUQIpy--Part I: computational uncertainty quantification for inverse problems in Python.\" arXiv preprint arXiv:2305.16949 (2023) https://arxiv.org/abs/2305.16949.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016eddd3",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "We start of by importing the Python packages we need (including CUQIpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cuqi\n",
    "from cuqi.testproblem import Deconvolution1D, Deconvolution2D\n",
    "from cuqi.distribution import JointDistribution, Gaussian, CMRF, LMRF, GMRF\n",
    "from cuqi.sampler import LinearRTO, pCN, CWMH, ULA, MALA, NUTS\n",
    "from cuqi.problem import BayesianProblem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining and sampling a Bayesian Inverse Problem (high-level)<a class=\"anchor\" id=\"BIP\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the posterior distribution.\n",
    "\n",
    "The posterior describes the probability distribution of the parameters we are interested in. This is achieved by combining prior knowledge of the parameters and observed data. In its most general form, the posterior is given by Bayes' theorem:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})} \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\boldsymbol{\\theta}$ is the parameter vector of *all* the parameters we are interested in inferring and $\\mathbf{y}$ is the observable data. \n",
    "\n",
    "The probability density function $p(\\boldsymbol{\\theta})$ is the prior distribution of the parameters.\n",
    "\n",
    "Given fixed observed data $\\mathbf{y}_\\mathrm{data}$, the term $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ considered as a function of $\\boldsymbol{\\theta}$ is known as the\n",
    "likelihood function or just *likelihood*, also denoted $L(\\boldsymbol{\\theta} \\mid \\mathbf{y} = \\mathbf{y}_\\mathrm{data})$, which we note is not a probability density but a density function.\n",
    "\n",
    "When $\\mathbf{y}$ is not fixed, $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ is a probability density function of the data $\\mathbf{y}$ given the value of the parameters $\\boldsymbol{\\theta}$. In CUQIpy we refer to this distribution as the *data distribution*.\n",
    "\n",
    "The denominator $p(\\mathbf{y})$ is the *evidence* and is a normalization constant (that we typically ignore because it does not affect the MCMC sampling) that ensures that the posterior integrates to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30f3764",
   "metadata": {},
   "source": [
    "### Note on Bayesian inverse problems with CUQIpy\n",
    "\n",
    "CUQIpy uses a general approach to Bayesian modeling that not only aims to define the posterior distribution, but instead to define the joint distribution of all the parameters. This more general approach is useful because it allows one to carry out more tasks related to uncertainty quantification of inverse problems such as prior-predictive checks, model checking, posterior-predictive checks and more. For more details on some of these topics see the overview in [1].\n",
    "\n",
    "In this notebook, we initially focus on how to define and sample a Bayesian inverse problem using the high-level interface in CUQIpy. We then later show a more low-level approach to defining the posterior distribution, which is useful for users who want to have more control the type of sampler used for sampling the posterior distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic forward model and observed data\n",
    "Consider a Bayesian inverse problem\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{e},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the (deterministic) forward model of the inverse problem and $\\mathbf{y}$ and $\\mathbf{x}$ are random variables representing the observed data and parameter of interest respectively. Here $\\mathbf{e}$ is a random variable representing additive noise in the data.\n",
    "\n",
    "For this example let us consider the `Deconvolution1D` testproblem and extract a CUQIpy forward model and some synthetic data denoted $\\mathbf{y}_\\mathrm{data}$ (a realization of $\\mathbf{y}$). \n",
    "\n",
    "Note that this is a linear inverse problem, but the same approach can be used for nonlinear inverse problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# For convenience, we define the dimension of the domain of A\n",
    "n = A.domain_dim\n",
    "\n",
    "# For convenience, we extract the exact solution as x_exact\n",
    "x_exact = probInfo.exactSolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem.\n",
    "\n",
    "Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the `probInfo` variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); x_exact.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "984a1531",
   "metadata": {},
   "source": [
    "## 1.2 Distributions for parameters and data\n",
    "\n",
    "The goal now is to define distributions for each the random variables that represent the parameters and data in the above-mentioned Bayesian inverse problem. One way to think about this, is that we are trying to define a statistical model for the data generating process (see reference [1]), which we call a *Bayesian Problem* in CUQIpy.\n",
    "\n",
    "For the unknown $\\mathbf{x}$, we use a-priori knowledge to define its distribution. In this case, the sinc phantom in the Deconvolution test problem can be represented fairly well by a distribution that generates smooth, but spatially correlated realizations.\n",
    "\n",
    "One such distribution is the Gaussian Markov Random Field (GMRF) distribution. This distribution assumes a Gaussian distribution on the differences between neighboring elements of $\\mathbf{x}$, i.e. in 1D:\n",
    "\n",
    "\\begin{align*}\n",
    "x_i - x_{i-1} \\sim \\mathcal{N}(0, d^{-1}), \\quad i=1, \\ldots, n,\n",
    "\\end{align*}\n",
    "\n",
    "where we purposely leave out the details on the boundary conditions for this notebook.\n",
    "\n",
    "To simplify the notation, we denote by *GMRF* the distribution that induces this property on a vector $\\mathbf{x}$ defined by its mean and precision $d$. That is, the above can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d),\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "with some choice of the precision say $d=50$. For more details on GMRF see the first CUQIpy paper [2].\n",
    "\n",
    "The GMRF distribution is implemented in CUQIpy as [GMRF class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.GMRF.html#cuqi.distribution.GMRF) and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior precision\n",
    "d = 50\n",
    "\n",
    "# Define GMRF prior (zero boundary conditions are the default)\n",
    "x = GMRF(np.zeros(n), d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e5008c",
   "metadata": {},
   "source": [
    "From the problem info string earlier, we saw that the noise is Gaussian with standard deviation $s_\\mathbf{e}=0.01$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{e} &\\sim \\mathcal{N}(\\mathbf{0}, s_\\mathbf{e}^2 \\mathbf{I}).\n",
    "\\end{align*}\n",
    "\n",
    "Instead of working directly with the noise variable $\\mathbf{e}$, we can include the stochastic element of the noise in the random variable $\\mathbf{y}$ representing the data.\n",
    "\n",
    "Because the data depend on $\\mathbf{x}$ we are really interested in defining the *data distribution* for $\\mathbf{y} \\mid \\mathbf{x}$ and since the noise is the only stochastic element of $\\mathbf{y}$ when $\\mathbf{x}$ is fixed we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} \\mid \\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, s_\\mathbf{y}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "where $s_\\mathbf{e}=s_\\mathbf{y} = 0.01$.\n",
    "\n",
    "Notice that this definition depends both on the forward model $\\mathbf{A}$ and the random variable $\\mathbf{x}$.\n",
    "\n",
    "In CUQIpy, we can define the distribution for $\\mathbf{y} \\mid \\mathbf{x}$ matching the mathematical expression using our previously defined variables `A` and `x` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise standard deviation\n",
    "s_y = 0.01\n",
    "\n",
    "# Define data distribution\n",
    "y = Gaussian(A@x, s_y**2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Have a look at the distributions for $\\mathbf{x}$ and $\\mathbf{y}$ by calling `print` on them in the code-cell below. \n",
    "- How are the distributions different?\n",
    "- Is it clear that the distribution for $\\mathbf{y}$ is a conditional distribution?\n",
    "- Can you generate and plot a realization (sample) of $\\mathbf{x}$? Does the realization show spatial correlation?\n",
    "- How about generating a sample from $\\mathbf{y}\\mid\\mathbf{x}$ given some fixed value of $\\mathbf{x}$? What does a sample represent in this case?\n",
    "\n",
    "**Hint:** to condition `y` on a value e.g. `x_exact` the syntax is `y(x=x_exact)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc2f4a9",
   "metadata": {},
   "source": [
    "### Note on notation\n",
    "\n",
    "It is common (for convenience in terms of notation) not to explicitly write the dependance of each random variable when specifying a complete Bayesian problem. For example, for the case above one would often write\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d)\\\\\n",
    "\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, s_\\mathbf{y}^2 I),\n",
    "\\end{align*}\n",
    "\n",
    "where the dependance of $\\mathbf{y}$ on $\\mathbf{x}$ is implicit.\n",
    "\n",
    "This compact notation completely specifies the Bayesian problem for the so-called *data generating process*, making clear all the assumptions about the data and parameters.\n",
    "\n",
    "In CUQIpy - when all deterministic parameters and forward models are defined - the Bayesian problem is written in code using almost exactly the same syntax as the mathematical notation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian problem (repeated in case previous cells were modified)\n",
    "x = GMRF(np.zeros(n), 50)\n",
    "y = Gaussian(A@x, s_y**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Bayesian inverse problem\n",
    "The simplest way to sample a Bayesian inverse problem in CUQIpy is to use the [BayesianProblem class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.bayesian_problem/cuqi.bayesian_problem.BayesianProblem.html#cuqi.bayesian_problem.BayesianProblem).\n",
    "\n",
    "Using the BayesianProblem class, one can easily define and sample from the posterior distribution of a Bayesian inverse problem by providing the distributions for the parameters and data and subsequently setting the observed data.\n",
    "\n",
    "Calling the `UQ` method on the BayesianProblem object will automatically sample the posterior distribution using a suitable choice of sampling algorithm and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest way to do uncertainty quantification in CUQIpy:\n",
    "\n",
    "BP = BayesianProblem(x, y)      # Create Bayesian problem\n",
    "BP.set_data(y=y_data)           # Provide observed data\n",
    "samples = BP.UQ(exact=x_exact)  # Run UQ analysis (comparing with exact solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we provided our assumptions about the data generating process by defining the distributions for the parameters and data and provided the observed data for the problem. CUQIpy then automatically sampled the posterior distribution and a credibility interval for the parameter $\\mathbf{x}$ as well as the mean of the posterior was plotted and compared to the ground truth (x_exact).\n",
    "\n",
    "We see that compared to the ground truth, the mean of the posterior is a good estimate and the credibility interval contains the ground truth.\n",
    "\n",
    "In the next section, we show how to define the posterior distribution more explicitly and how to sample it with a specific choice of sampler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb3bf801",
   "metadata": {},
   "source": [
    "# 2 Defining and sampling a Bayesian Inverse Problem (low-level)<a class=\"anchor\" id=\"Joint\"></a>\n",
    "\n",
    "Instead of relying on the BayesianProblem class to automatically sample the posterior distribution (which way not always work well), we can also define the posterior distribution explicitly and sample it using a specific sampler in CUQIpy. This requires a bit more work, but it gives us more control over the sampling process.\n",
    "\n",
    "## 2.1 Defining the joint distribution\n",
    "In the Bayesian framework, we are interested in the joint distribution over $\\mathbf{x}$ and $\\mathbf{y}$. The joint distribution is defined as the product of the individual probability density functions. In our simple case, the joint distribution can be described in density form as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y},\\mathbf{x}) = p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In CUQIpy, if we want more control, we can as an alternative to using BayesianProblem, instead define the joint distribution ourselves by passing each of the distributions as arguments to the [JointDistribution class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.JointDistribution.html#cuqi.distribution.JointDistribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035c5126",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09194832",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we are not going to dive further into the details of the joint distribution, and simply use it to define the posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d9ca744",
   "metadata": {},
   "source": [
    "## 2.2 Defining the posterior distribution\n",
    "\n",
    "To define the posterior distribution, we have to condition the joint distribution $p(\\mathbf{y},\\mathbf{x})$ on the observed data, say $\\mathbf{y}_\\mathrm{data}$. This is done by using Bayes' theorem, which in our simple case can easily be derived as\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data}) \\propto L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data}) := p(\\mathbf{y}=\\mathbf{y}_\\mathrm{data} \\mid \\mathbf{x})$ for the likelihood function to emphasize that, in the context of the posterior where $\\mathbf{y}$ is fixed to $\\mathbf{y}_\\mathrm{data}$, it is a function of $\\mathbf{x}$ and not on $\\mathbf{y}$. In CUQIpy we sometimes use the short-hand printing style `L(x|y)` for brevity.\n",
    "\n",
    "CUQIpy can automatically derive the posterior distribution for any joint distribution by passing the observed data as an argument to the \"call\" (condition) method of the joint distribution.  This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=y_data) # Condition p(x,y) on y=y_data. Applies Bayes' rule automatically\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ab207d",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it. Notice that the posterior equation matches the mathematical expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9def31",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "Try evaluating the posterior log probability density function (logpdf) at some point say $\\mathbf{1}$ and $2\\cdot\\mathbf{1}$, where $\\mathbf{1}$ is a ones-vector.\n",
    "\n",
    "Try evaluating the posterior pdf at the same points. Can you explain why the pdf gives the same result for both points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "# 2.3 Sampling the posterior\n",
    "\n",
    "Now that we have defined the posterior distribution for our parameter of interest $\\mathbf{x}$ given $\\mathbf{y}_\\mathrm{data}$, we can characterize the parameter and its uncertainty by samples from the posterior distribution. However, in general the posterior is not a simple distribution that we can easily sample from. Instead, we need to rely on Markov Chain Monte Carlo (MCMC) methods to sample from the posterior.\n",
    "\n",
    "In CUQIpy, a number of MCMC samplers are provided in the sampler module that can be used to sample probability distributions. All samplers have the same signature, namely `Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above, which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize [(LinearRTO)](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.LinearRTO.html#cuqi.sampler.LinearRTO) sampler is a good choice to efficiently generate samples. This is also the sampler chosen by the BayesianProblem class for this problem.\n",
    "\n",
    " Like any of the other samplers, we set up the sampler by simply providing the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = LinearRTO(posterior)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "After the sampler is defined we can compute samples via the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, the returned object is a `cuqi.samples.Samples` object.\n",
    "\n",
    "This object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the chain has converged and there is little need for removing samples that are part of a \"burn-in\" period. In practice, the samples should be inspected with more rigor to ensure that the MCMC chain has converged, but this is outside the scope of this notebook.\n",
    "\n",
    "The good sampling is in large part due to the LinearRTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see [samples.burnthin](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.samples/cuqi.samples.Samples.burnthin.html#cuqi.samples.Samples.burnthin)) and store the \"burnthinned\" samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from `probInfo`).\n",
    "\n",
    "This is what the `UQ` method of the BayesianProblem class did under the hood for us earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "779d95f1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The LinearRTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the logpdf. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers (click the links to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from LinearRTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as LinearRTO or NUTS. For some samplers, the method [sample_adapt](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.sample_adapt.html#cuqi.sampler.NUTS.sample_adapt) will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate and a burn-in should be added to specify how many samples to use for the adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "# 3. Modifying priors<a class=\"anchor\" id=\"ModifyPriors\"></a>\n",
    "\n",
    "In the above example, we used a GMRF prior for the parameter $\\mathbf{x}$. However, it is not always obvious what prior to use for a given problem. In such cases, it is often useful to try out different priors to see how they affect the posterior distribution.\n",
    "\n",
    "In CUQIpy, it is easy to modify the prior and re-sample the posterior distribution. This is most easily done by using the BayesianProblem class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "####  Try yourself (optional):  \n",
    "\n",
    "Please carry out the following exercise to see how the prior affects the posterior distribution. \n",
    "\n",
    "Note that: here we use the [sample_posterior](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.sample_posterior.html#cuqi.problem.BayesianProblem.sample_posterior) method of the BayesianProblem class to sample the posterior distribution and store the samples without plotting. We then manually plot the samples using the `plot_ci` method of the `Samples` object.\n",
    "\n",
    "- Try another prior such as [LMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.LMRF.html#cuqi.distribution.LMRF) or [CMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.CMRF.html#cuqi.distribution.CMRF) for the 1D case (look up appropriate arguments in the documentation) using [`BayesianProblem`](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.problem/cuqi.problem.BayesianProblem.html#cuqi.problem.BayesianProblem).\n",
    "- Try switching the testproblem from Deconvolution1D to [Deconvolution2D](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.testproblem/cuqi.testproblem.Deconvolution2D.html#cuqi.testproblem.Deconvolution2D) (look up appropriate arguments in the documentation).\n",
    "- You may also try defining your own Bayesian inverse problem using this interface. ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can modify this code or write your own from scratch\n",
    "\n",
    "# 1. Forward model and data\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# 2. Distributions\n",
    "x = GMRF(np.zeros(A.domain_dim), 50) # Try e.g. LMRF or CMRF (also update scale parameters!)\n",
    "y = Gaussian(A@x, 0.01**2)\n",
    "\n",
    "# 3. Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "\n",
    "# 4. Sample posterior\n",
    "samples = BP.sample_posterior(500)\n",
    "\n",
    "# 5. Analyze posterior\n",
    "samples.plot_ci(exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0583cafe",
   "metadata": {},
   "source": [
    "You may have noticed that finding suitable parameters for the prior could be a challenge. To see how to automatically find suitable parameters for the prior, see the [Gibbs notebook](Exercise06_Gibbs.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097194b0",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine in case something was changed\n",
    "\n",
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D(phantom=\"sinc\").get_components()\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = GMRF(np.zeros(A.domain_dim), 50)\n",
    "y = Gaussian(mean=A@x, cov=0.01**2)\n",
    "\n",
    "# Define Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP() # Maximum a posteriori estimate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2931bb8f",
   "metadata": {},
   "source": [
    "The automatic solver selection is also still work-in-progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24cab183",
   "metadata": {},
   "source": [
    "After we have computed the MAP, we can then estimate to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a931f3a-9534-48b1-a273-5bc8616954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching to the Deconvolution2D testproblem. You may have to play with the prior standard deviation to get a good MAP estimate.\n",
    "- Try switching the prior to a CMRF distribution for the 1D case. Does the MAP estimate change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
