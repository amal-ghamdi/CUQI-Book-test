{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise XX: Gibbs sampling\n",
    "\n",
    "In this notebook we show how to use CUQIpy to sample hierarchical Bayesian models using Gibbs sampling.\n",
    "\n",
    "Gibbs sampling is a Markov chain Monte Carlo (MCMC) method for sampling a joint probability distribution of multiple random variables.\n",
    "Compared to sampling all variables simultaneously, Gibbs sampling samples the variables sequentially. The sampling of each variable is achieved by sampling from the conditional distribution of that variable given (fixed, previously sampled) values of the other variables.\n",
    "\n",
    "Gibbs sampling is often an efficient way of sampling from a joint distribution if the conditional distributions are easy to sample\n",
    "from. On the other hand, if the conditional distributions are highly correlated and/or are difficult to sample from, then Gibbs sampling can be very inefficient. For these reasons, Gibbs sampling is often a double-edged sword, that needs to be used in the right context.\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Going through this notebook you will learn to:\n",
    "\n",
    "- Describe the basic idea of Gibbs sampling.\n",
    "- Define a hierarchical Bayesian model using CUQIpy.\n",
    "- Define a Gibbs sampling scheme in CUQIpy.\n",
    "- Run the Gibbs sampler and analyze the results.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by importing the necessary modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../../CUQIpy/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.testproblem import Deconvolution1D\n",
    "from cuqi.distribution import GaussianCov, Gamma, JointDistribution, GMRF, Laplace_diff\n",
    "from cuqi.sampler import Gibbs, Linear_RTO, Conjugate, UnadjustedLaplaceApproximation, ConjugateApprox, MetropolisHastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The basics of Gibbs sampling (WIP)\n",
    "\n",
    "To begin let us consider a very simple Bayesian model with two random variables, $d$ and $x$.\n",
    "\n",
    "\\begin{align}\n",
    "d &\\sim \\mathrm{Gamma}(1, 10^{-4})\\\\\n",
    "x &\\sim \\mathcal{N}(0, d^{-1}).\n",
    "\\end{align}\n",
    "\n",
    "As we have already seen in the previous notebooks, we can define models like this in CUQIpy as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Gamma(1, 1)\n",
    "x = GaussianCov(0, lambda d: 1/d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to sample from the joint distribution $p(d, x)$. We can think of this as our target distribution. \n",
    "\n",
    "We define the joint distribution in CUQIpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = JointDistribution(x, d)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One idea for sampling $p(d,x)$ is to define the parameter $\\theta = [d; x]$ and sample from distribution $p(\\theta)$ at the same time. However, this is only possible for simple models, where the joint distribution is easy to sample from.\n",
    "\n",
    "Gibbs sampling is an alternative approach, where we alternately sample from the distribution of each variable conditioned on the others. That is we would sample from $p(d|x)$ and $p(x|d)$. We can summarize this procedure as repeating the following steps:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{1. draw } d^{(i+1)} &\\sim p(d|x^{(i)})\\\\\n",
    "\\text{2. draw } x^{(i+1)} &\\sim p(x|d^{(i+1)})\n",
    "\\end{align}\n",
    "\n",
    "where $x^{(i)}$ and $d^{(i)}$ are the values of $x$ and $d$ at iteration $i$. It can be shown that samples drawn from the above scheme will be distributed according to $p(d, x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hack to direct sample a distribution (Should maybe be added to cuqi)\n",
    "class Direct:\n",
    "    def __init__(self, target):\n",
    "        self.target = target\n",
    "    \n",
    "    def step(self, x=None):\n",
    "        return self.target.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a sampling strategy\n",
    "\n",
    "When the conditional distributions are easy to sample from, Gibbs sampling can be very efficient. In the example above, we can sample from the conditional distributions as follows:\n",
    "\n",
    "1. Sample $x^{(i+1)}$ from $p(x|d^{(i)})$,\n",
    "2. Sample $d^{(i+1)}$ from $p(d|x^{(i+1)})\\propto L(d|x^{(i+1)})p(d)$,\n",
    "\n",
    "where $L(d|x):=p(x|d)$ is the likelihood function with respect to $d$.\n",
    "\n",
    "**For step 1:** Note that we already know that $p(x|d)=\\mathcal{N}(0, d^{-1})$. Hence, we can directly sample from $x | d$ by sampling from $\\mathcal{N}(0, d^{-1})$.\n",
    "\n",
    "**For step 2:** For $p(d|x)$ we could use any MCMC sampler to sample with. However, note that the Gaussian and Gamma distributions are conjugate, so we can recognize $p(d|x)$ as a Gamma distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "p(d|x) &\\propto L(d|x)p(d)\\\\\n",
    "&= \\exp\\left(-\\frac{d}{2}x^2\\right)\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}d^{\\alpha-1}\\exp(-\\beta d)\\\\\n",
    "&= \\mathrm{Gamma}(\\alpha+1/2, \\beta+\\frac{1}{2}x^2)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha=1$ and $\\beta=10^{-4}$ are the parameters of the Gamma distribution.\n",
    "\n",
    "Instead of explicitly defining the Gamma distribution, CUQIpy provides a `Conjugate` sampler, which can be used to automatically sample from conjugate distributions. Similarly we can use the `Direct` sampler to sample from the Gaussian distribution.\n",
    "\n",
    "We can now define a Gibbs sampling scheme in CUQIpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling strategy\n",
    "sampling_strategy = {\n",
    "    'x' : Direct,\n",
    "    'd' : Conjugate, # Can't do MetropolisHastings because we need new to keep track of proposal scale #34\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the Gibbs sampler and sample from the joint distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampler\n",
    "sampler = Gibbs(target, sampling_strategy)\n",
    "\n",
    "# Sample\n",
    "samples = sampler.sample(1000, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the trace plots as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"x\"].plot_trace()\n",
    "samples[\"d\"].plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gibbs for inverse problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Deterministic forward model and data\n",
    "Consider an inverse problem\n",
    "$$y=Ax,$$\n",
    "where $A: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the forward model of the inverse problem, $y\\in\\mathbb{R}^m$ is the data and $x\\in \\mathbb{R}^n$ is the parameter of interest.\n",
    "\n",
    "In this case, we assume that the forward model is a convolution operator. We can load this example from the testproblem library of CUQIpy and visualize the true solution (sharp signal) and data (convolved signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Model and data\n",
    "A, y_data, probinfo = Deconvolution1D.get_components(phantom='square')\n",
    "\n",
    "# Get dimension of signal\n",
    "n = A.domain_dim\n",
    "\n",
    "# Plot exact solution and observed data\n",
    "plt.subplot(121)\n",
    "probinfo.exactSolution.plot()\n",
    "plt.title('exact solution')\n",
    "\n",
    "plt.subplot(122)\n",
    "y_data.plot()\n",
    "plt.title(\"Observed data\")\n",
    "\n",
    "# Print problem information\n",
    "print(probinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hierarchical Bayesian model\n",
    "\n",
    "We are now going to define a hierarchical Bayesian model for the inverse problem.\n",
    "\n",
    "First, we are going to model the prior distribution of the parameter $x$ as a Gaussian Markov random field (GMRF), i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} \\mid d &\\sim \\mathrm{GMRF}(\\mathbf{0}, d)\n",
    "\\end{align*}\n",
    "\n",
    "This is a built-in distribution in CUQIpy, which models the difference between neighboring elements as a Gaussian, see [GMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.GMRF.html#cuqi.distribution.GMRF) for more details.\n",
    "\n",
    "We do not know a good choice of precision parameter $d$ for this prior, so we make use of `lambda` (anonymous) functions to make $d$ a conditioning variable of the distribution. The distribution is then defined in CUQIpy as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GMRF(np.zeros(n), lambda d: d)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data is corrupted by additive Gaussian noise, so we can assume a Gaussian distribution for $y$ and write\n",
    "\n",
    "$$\\mathbf{y} \\mid \\mathbf{x}, s \\sim \\mathcal{N}(\\mathbf{A} \\mathbf{x}, s^{-1}\\mathbf{I}_m),$$\n",
    "\n",
    "where for this example, we pretend also not to know the noise standard deviation $s$.\n",
    "\n",
    "Similar to the prior distribution, we can define this distribution as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = GaussianCov(A@x, lambda s: 1/s)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the two *hyperparameters* $d$ and $s$, we can use a weakly informative `Gamma` distribution, i.e. a distribution with a wide range of possible values.\n",
    "\n",
    "\\begin{align*}\n",
    "        d &\\sim \\mathrm{Gamma}(1, 10^{-4}) \\\\\n",
    "        s &\\sim \\mathrm{Gamma}(1, 10^{-4})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Gamma(1, 1e-4)\n",
    "s = Gamma(1, 1e-4)\n",
    "\n",
    "print(d)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, we can summarize the hierarchical Bayesian model as follows (avoiding explicitly writing the conditioning variables):\n",
    "\n",
    "\\begin{align*}\n",
    "        d &\\sim \\mathrm{Gamma}(1, 10^{-4}) \\\\\n",
    "        s &\\sim \\mathrm{Gamma}(1, 10^{-4}) \\\\\n",
    "        \\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d) \\\\\n",
    "        \\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A} \\mathbf{x}, s^{-1} \\mathbf{I}_m)\n",
    "\\end{align*}\n",
    "\n",
    "which in CUQIpy matches line-by-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Gamma(1, 1e-4)\n",
    "s = Gamma(1, 1e-4)\n",
    "x = GMRF(np.zeros(n), lambda d: d)\n",
    "y = GaussianCov(A@x, lambda s: 1/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior distribution\n",
    "\n",
    "Similarly as in the previous notebook, we now define the posterior distribution by first creating the joint distribution $p(\\mathbf{y}, \\mathbf{x}, d, s)$ and then conditioning on the data $\\mathbf{y}^\\mathrm{data}$ to obtain the posterior defined in density form as\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x}, d, s \\mid \\mathbf{y}^\\mathrm{data}) = L(\\mathbf{x}, s \\mid \\mathbf{y}^\\mathrm{data}) p(\\mathbf{x} \\mid d) p(d) p(s).\n",
    "\\end{align*}\n",
    "\n",
    "This in done in CUQIpy as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create joint distribution\n",
    "joint = JointDistribution(y, x, d, s)\n",
    "\n",
    "# Define posterior by conditioning on the data\n",
    "posterior = joint(y=y_data)\n",
    "\n",
    "# View the posterior\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after conditioning on the data, the distribution associated with\n",
    "$\\mathbf{y}$ became a likelihood function and that the posterior is now\n",
    "a joint distribution of the variables $d$, $l$, $\\mathbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gibbs Sampler\n",
    "\n",
    "The hierarchical model above has some important properties that we\n",
    "can exploit to make the sampling more efficient. First, note that\n",
    "the Gamma distribution are conjugate priors for the precision of\n",
    "the Gaussian distributions. This means that we can efficiently sample\n",
    "from $d$ and $l$ conditional on the other variables.\n",
    "\n",
    "Second, note that the prior distribution of $\\mathbf{x}$ is\n",
    "a Gaussian Markov random field (GMRF) and that the distribution for\n",
    "$\\mathbf{y}$ is also Gaussian with a Linear operator acting\n",
    "on $\\mathbf{x}$ as the mean variable. This means that we can\n",
    "efficiently sample from $\\mathbf{x}$ conditional on the other\n",
    "variables using the ``Linear_RTO`` sampler.\n",
    "\n",
    "Taking these two facts into account, we can define a Gibbs sampler\n",
    "that uses the ``Conjugate`` sampler for $d$ and $l$ and\n",
    "the ``Linear_RTO`` sampler for $\\mathbf{x}$.\n",
    "\n",
    "This is done in CUQIpy as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define sampling strategy\n",
    "sampling_strategy = {\n",
    "    'x': Linear_RTO,\n",
    "    'd': Conjugate,\n",
    "    's': Conjugate\n",
    "}\n",
    "\n",
    "# Define Gibbs sampler\n",
    "sampler = Gibbs(posterior, sampling_strategy)\n",
    "\n",
    "# Run sampler\n",
    "samples = sampler.sample(Ns=1000, Nb=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "After sampling we can inspect the results. The samples are stored\n",
    "as a dictionary with the variable names as keys. Samples for each \n",
    "variable is stored as a CUQIpy Samples object which contains the\n",
    "many convenience methods for diagnostics and plotting of MCMC samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot credible intervals for the signal\n",
    "samples['x'].plot_ci(exact=probinfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace plot for d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "samples['d'].plot_trace(figsize=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace plot for s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "samples['s'].plot_trace(figsize=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the true noise standard deviation was $0.05$ which translates to a precision of $s=400$. The trace plot for $s$ shows that the sampler has converged to the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Exploring other priors\n",
    "\n",
    "Notice that while the sampling went well in the previous example,\n",
    "the posterior distribution did not match the characteristics of\n",
    "the exact solution. We can improve this result by switching to a\n",
    "prior that better matches the exact solution $\\mathbf{x}$.\n",
    "\n",
    "One choice is the Laplace difference prior, which assumes a\n",
    "Laplace distribution for the differences between neighboring\n",
    "elements of $\\mathbf{x}$. That is,\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{x} \\sim \\text{Laplace_diff}(\\mathbf{0}, d^{-1}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which translates to the assumption that $x_i-x_{i-1} \\sim \\mathrm{Laplace}(0, d^{-1})$ for $i=1, \\ldots, n$.\n",
    "\n",
    "This prior is implemented in CUQIpy as the ``Laplace_diff`` distribution.\n",
    "To update our model we simply need to replace the ``GMRF`` distribution\n",
    "with the ``Laplace_diff`` distribution. Note that the Laplace distribution\n",
    "is defined via a scale parameter, so we invert the parameter $d$.\n",
    "\n",
    "This laplace distribution and new posterior can be defined as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define new distribution for x\n",
    "x = Laplace_diff(np.zeros(n), lambda d: 1/d)\n",
    "\n",
    "# Define new joint distribution with piecewise constant prior\n",
    "joint_Ld = JointDistribution(d, s, x, y)\n",
    "\n",
    "# Define new posterior by conditioning on the data\n",
    "posterior_Ld = joint_Ld(y=y_data)\n",
    "\n",
    "print(posterior_Ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Gibbs Sampler (with Laplace prior)\n",
    "\n",
    "Using the same approach as ealier we can define a Gibbs sampler\n",
    "for this new hierarchical model. The only difference is that we\n",
    "now need to use a different sampler for $\\mathbf{x}$ because\n",
    "the ``Linear_RTO`` sampler only works for Gaussian distributions.\n",
    "\n",
    "In this case we use the UnadjustedLaplaceApproximation sampler\n",
    "for $\\mathbf{x}$. We also use an approximate Conjugate\n",
    "sampler for $d$ which approximately samples from the\n",
    "posterior distribution of $d$ conditional on the other\n",
    "variables in an efficient manner. For more details see e.g.\n",
    "[Uribe et al. (2021)](https://arxiv.org/abs/2104.06919).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define sampling strategy\n",
    "sampling_strategy = {\n",
    "    'x': UnadjustedLaplaceApproximation,\n",
    "    'd': ConjugateApprox,\n",
    "    's': Conjugate\n",
    "}\n",
    "\n",
    "# Define Gibbs sampler\n",
    "sampler_Ld = Gibbs(posterior_Ld, sampling_strategy)\n",
    "\n",
    "# Run sampler\n",
    "samples_Ld = sampler_Ld.sample(Ns=1000, Nb=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Analyze results\n",
    "\n",
    "Again we can inspect the results.\n",
    "Here we notice the posterior distribution matches the exact solution much better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot credible intervals for the signal\n",
    "samples_Ld['x'].plot_ci(exact=probinfo.exactSolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "samples_Ld['d'].plot_trace(figsize=(8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "samples_Ld['s'].plot_trace(figsize=(8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
