{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo with CUQIpy-PyTorch\n",
    "\n",
    "In this notebook, we use [CUQIpy-PyTorch](https://github.com/CUQI-DTU/CUQIpy-PyTorch) to extend CUQIpy by adding the ability to use PyTorch as a backend for array operations. PyTorch enables two main things: 1) GPU acceleration and 2) automatic differentiation. GPU acceleration is self-explanatory, but automatic differentiation deserves some explanation.\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) enables computing the gradient of a function with respect to its input variables automatically using repeated application of the chain rule. This is useful for many machine learning algorithms, but also in the context of Bayesian inference. In particular, it means that we can automatically compute the gradient of a log-posterior, which could be arbitrarily complex! This provides a huge advantage because we can then sample from the posterior distribution using Hamiltonian Monte Carlo (HMC) and other gradient-based methods.\n",
    "\n",
    "Hamiltonian Monte Carlo and in particular the [No-U-Turn Sampler](https://arxiv.org/abs/1111.4246) (NUTS) variant is a general, but still very efficient sampler for sampling high-dimensional distributions that only requires gradient information. This is useful when it is not possible to exploit the structure of the posterior distribution using e.g. conjugacy relations, linearity of forward models or other tricks, which in large part is what the main CUQIpy package is all about.\n",
    "\n",
    "In this way, CUQIpy-PyTorch compliments the main CUQIpy package by adding the option for an efficient sampling technique that works for arbitrary posterior distributions by using automatic differentiation to compute the gradient of the log-posterior.\n",
    "\n",
    "**Make sure you have installed the CUQIpy-PyTorch plugin (link in first paragraph) before starting this exercise.**\n",
    "\n",
    "## Learning objectives of this notebook:\n",
    "\n",
    "Going through this notebook, you will learn:\n",
    "\n",
    "- Why Hamiltonian Monte Carlo is useful for sampling distributions\n",
    "- The basics of PyTorch tensors\n",
    "- The basics of CUQIpy-PyTorch distributions\n",
    "- How to use Hamiltonian Monte Carlo to sample from distributions\n",
    "- How to use Hamiltonian Monte Carlo to sample Bayesian inference problems\n",
    "\n",
    "## Table of contents: \n",
    "* [1. Why Hamiltonian Monte Carlo?](#why-hmc?)\n",
    "* [2. PyTorch basics and CUQIpy-PyTorch](#pytorch-basics)\n",
    "* [3. Hamiltonian Monte Carlo in CUQIpy-PyTorch](#hmc-cuqipy-pytorch)\n",
    "* [4. Bayesian inverse problems with CUQIpy-PyTorch](#bayesian-inverse-problems)\n",
    "* [5. Open-ended exploration](#open-ended-exploration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages. Notice we use the PyTorch package `torch` (imported as `xp`) instead of NumPy for arrays and import both `cuqi` and `cuqipy_pytorch` from CUQIpy and CUQIpy-PyTorch, respectively. We also import `matplotlib` for plotting and some timing utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as xp\n",
    "import numpy as np\n",
    "import cuqi\n",
    "import cuqipy_pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Why Hamiltonian Monte Carlo? <a class=\"anchor\" id=\"why-hmc\"></a>\n",
    "\n",
    "As mentioned in the introduction, Hamiltonian Monte Carlo (HMC) is a general, but still very efficient sampler for sampling high-dimensional distributions. It is beyond the scope of this exercise to go into the details of HMC, but we instead give a short example showing how it compares to using the classical Metropolis-Hastings algorithm. For more details in the theory of HMC, we refer to the [original paper](https://arxiv.org/abs/1206.1901) by Neal and the [No-U-Turn Sampler](https://arxiv.org/abs/1111.4246) (NUTS) variant by Hoffman and Gelman.\n",
    "\n",
    "Suppose we were aiming to sample from a 2-dimensional probability density function shaped like a donut. We could also have selected a higher dimensional example making Metropolis-Hastings potentially look even worse, but it makes visualization more difficult.\n",
    "\n",
    "This example can be loaded from the `DistributionGallery` class in CUQIpy as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut = cuqi.distribution.DistributionGallery('donut')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The donut distribution has a manually derived gradient for the logpdf. For illustration let us plot the pdf and arrows showing the gradient of the pdf at a few points.\n",
    "\n",
    "Note: Currently there is no `plot_pdf` method for CUQIpy distributions, so we must write our own plotting code. Also at this point we still use NumPy arrays instead of PyTorch tensors and the main CUQIpy package, but we will switch to PyTorch tensors and CUQIpy-PyTorch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(dist, plot_grad=False):\n",
    "    \"\"\" Plot the pdf of a 2-dimensional distribution and optionally its gradient as a vector field. \"\"\"\n",
    "    # Ranges for the plot\n",
    "    m, n, nl, ng = 300, 300, 30, 30\n",
    "    xmin, xmax, ymin, ymax = -4, 4, -4, 4\n",
    "\n",
    "    # evaluate PDF\n",
    "    X, Y = np.meshgrid(np.linspace(xmin, xmax, m), np.linspace(ymin, ymax, n))\n",
    "    Xf, Yf = X.flatten(), Y.flatten()\n",
    "    pts = np.vstack([Xf, Yf]).T   # pts is (m*n, d)\n",
    "    Z = donut.pdf(pts).reshape((m, n))\n",
    "\n",
    "    if plot_grad:\n",
    "        # evaluate gradient\n",
    "        Xg, Yg = np.meshgrid(np.linspace(xmin, xmax, ng), np.linspace(ymin, ymax, ng))\n",
    "        Xfg, Yfg = Xg.flatten(), Yg.flatten()\n",
    "        posg = np.vstack([Xfg, Yfg]).T  \n",
    "        grad = donut.gradient(posg)\n",
    "        norm = np.linalg.norm(grad, axis=0)\n",
    "        u, v = grad[0, :]/norm, grad[1, :]/norm\n",
    "\n",
    "    # plot PDF and gradient\n",
    "    plt.contourf(X, Y, Z, nl)\n",
    "    plt.contour(X, Y, Z, nl, linewidths=0.5, colors='k') \n",
    "    if plot_grad:\n",
    "        plt.quiver(posg[:, 0], posg[:, 1], u, v, units='xy', scale=4, color='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the pdf and gradient arrows using the `plot_pdf` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plot_pdf(donut, plot_grad=True)\n",
    "plt.title('Donut PDF and Gradient');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample from the distribution using both the Metropolis-Hastings algorithm and HMC (NUTS). We also store the time it takes to sample from the distribution using each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings\n",
    "t = time.time()\n",
    "samples_MH = cuqi.sampler.MH(donut).sample_adapt(N=1000, Nb=1000)\n",
    "t_MH = time.time() - t\n",
    "\n",
    "# Hamiltonian Monte Carlo (NUTS)\n",
    "t = time.time()\n",
    "samples_NUTS = cuqi.sampler.NUTS(donut).sample_adapt(N=1000, Nb=1000)\n",
    "t_NUTS = time.time() - t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare the samples obtained from the two methods by plotting the samples in a \"pair plot\" (a scatter plot for each pair of variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_MH.plot_pair()\n",
    "plt.title('Metropolis-Hastings')\n",
    "plt.axis('equal')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "\n",
    "\n",
    "samples_NUTS.plot_pair()\n",
    "plt.title('Hamiltonian Monte Carlo (NUTS)')\n",
    "plt.axis('equal')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the samples obtained using HMC are much more evenly distributed than the samples obtained using the Metropolis-Hastings algorithm (which may not even have explored the entire distribution yet).\n",
    "\n",
    "We can also see that the chains obtained using HMC are much less correlated than the chains obtained using the Metropolis-Hastings algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(121, title='Metropolis-Hastings')\n",
    "samples_MH.plot_chain()\n",
    "\n",
    "plt.subplot(122, title='Hamiltonian Monte Carlo (NUTS)')\n",
    "samples_NUTS.plot_chain();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the effective samples size (ESS) of the two chains. The ESS is a measure of the number of independent samples in a chain. We divide by the total time it took to sample from the distribution to get the effective samples per second (ESS/s). We can see that the ESS/s is much higher for the HMC chain than the Metropolis-Hastings chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MH: Effective samples / second {samples_MH.compute_ess()/t_MH} \")\n",
    "print(f\"NUTS: Effective samples / second {samples_NUTS.compute_ess()/t_NUTS} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more to say about HMC, but we will leave that for another time. For now, we will move on to using HMC in CUQIpy-PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PyTorch basics and CUQIpy-PyTorch <a class=\"anchor\" id=\"pytorch-basics\"></a>\n",
    "\n",
    "**The examples in here are for illustration purposes only. You do not need to understand the all details of automatic differentiation to use CUQIpy-PyTorch.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we used the NUTS implementation from the main CUQIpy package, which requires the gradient of the log-posterior to be manually defined in the `Distribution` class.\n",
    "\n",
    "In this section we see how to use PyTorch tensors and automatic differentiation to compute the gradient of any distribution. The obvious benefit here is that we do not need to manually define the gradient of the logpdf, which makes the modelling much more flexible.\n",
    "\n",
    "## PyTorch primer\n",
    "\n",
    "Before using CUQIpy-PyTorch, we need to learn a bit about PyTorch.\n",
    "\n",
    "First, we can create a PyTorch tensor in a similar way to NumPy arrays, except we replace `np.array` with `torch.tensor` (or `xp.tensor` if we have imported PyTorch as `xp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xp.tensor([1, 2, 3], dtype=xp.float32)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch mirrors the NumPy API for array operations, so we can use the same operations on PyTorch tensors as we would on NumPy arrays. This includes broadcasting, slicing, indexing as well as most mathematical operations from e.g. the linear algebra sub-package `linalg`.\n",
    "\n",
    "For example, we can create a matrix and compute the matrix-vector product and then take the 2-norm of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "A = xp.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=xp.float32)\n",
    "\n",
    "# Multiply A with x\n",
    "y = A@x\n",
    "print(y)\n",
    "\n",
    "# Compute norm of A@x\n",
    "z = xp.linalg.norm(y)\n",
    "print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient computation\n",
    "\n",
    "Any operation involving PyTorch tensors can be differentiated! To use this feature, we need to define each tensor with the `requires_grad` flag set to `True`. This tells PyTorch to keep track of the operations performed on the tensor and to compute the gradient of the tensor with respect to the operations performed on it. For example, from the example above, we can automatically evaluate the gradient of the expression\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{A} \\mathbf{x}\\|_2^2\n",
    "$$\n",
    "\n",
    "with respect to $\\mathbf{x}$, for the specific value of $\\mathbf{x}=[1,2,3]^T$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xp.tensor([1, 2, 3], dtype=xp.float32, requires_grad=True) # Say we want to compute gradient wrt x\n",
    "\n",
    "z = xp.linalg.norm(A@x)**2 # Compute z = ||A@x||^2\n",
    "\n",
    "# Now evaluate gradient of z wrt x\n",
    "z.backward() # This will compute dz/dx and store it in x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we know the gradient is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\|\\mathbf{A} \\mathbf{x}\\|_2^2 = 2 \\mathbf{A}^T \\mathbf{A} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "which we can verify by comparing to the gradient computed by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*A.T@A@x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUQIpy-PyTorch distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `cuqi` distributions are written using NumPy and SciPy, we instead have to use the distributions defined in `cuqipy_pytorch`.\n",
    "\n",
    "*We are thinking of making the main CUQIpy package agnostic to the backend such that it can use either NumPy or PyTorch (or Jax, or TensorFlow etc.), but this is not yet implemented.*\n",
    "\n",
    "Instead, we use thin wrappers around PyTorch distributions, that acts as a drop-in replacement for the `cuqi` distributions. They are imported as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqipy_pytorch.distribution import Gaussian, HalfGaussian, LogGaussian, Uniform, Gamma, StackedJointDistribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUQIpy-PyTorch distributions work in a similar way to the `cuqi` distributions, but instead of using NumPy arrays, they use PyTorch tensors. For example, we can create a 2D i.i.d. Gaussian distribution and look at some of its properties and methods as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Gaussian(xp.zeros(2), xp.eye(2))\n",
    "\n",
    "print(f\" Name: {x.name}\")\n",
    "print(f\" Mean: {x.mean}\")\n",
    "print(f\" Cov: {x.cov}\")\n",
    "print(f\" PDF at 0: {x.pdf(xp.zeros(2))}\")\n",
    "print(f\" Log PDF at 0: {x.logpdf(xp.zeros(2))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major difference is that the gradient of the logpdf is automatically computed using PyTorch's automatic differentiation feature. We can inspect the code for the `gradient` method to see how this is done. This should look similar to the example we did for PyTorch gradients above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate gradient for some random point\n",
    "x.gradient(xp.randn(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the code for evaluating gradient\n",
    "x.gradient??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Really utilizing the power of automatic differentiation\n",
    "\n",
    "One of the main use-cases for automatic differentiation in the context of Bayesian inference is to compute the gradient of more complex Bayesian models. To illustrate this, suppose one part of the Bayesian model contains a Gaussian distribution with a covariance matrix that is a function of another random variable, say $s$. The Gaussian could be written as follows.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\mid s \\sim \\mathcal{N}(\\mathbf{0}, \\exp(-s)^2 \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "In CUQIpy-PyTorch (or CUQIpy), we can define this distribution as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Gaussian(xp.zeros(2), lambda s: xp.exp(-s)**2*xp.eye(2))\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize Hamiltonian Monte Carlo to sample from a Bayesian model that contains this distribution, we need to compute the logpdf and gradient respect to $\\mathbf{x}$ and $s$! This is where automatic differentiation comes in!\n",
    "\n",
    "If we define the values of $\\mathbf{x}$ and $s$ we want to compute the gradient of the logpdf at, then evaluate the logpdf, we can use the `.backward()` method to compute the gradient of the logpdf with respect to $\\mathbf{x}$ and $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points to compute gradient at\n",
    "x_in = xp.ones(2, dtype=xp.float32, requires_grad=True)\n",
    "s_in = xp.tensor(2, dtype=xp.float32, requires_grad=True)\n",
    "\n",
    "# Evaluate logpdf\n",
    "output = x.logd(x=x_in, s=s_in)\n",
    "\n",
    "# Compute gradient (dlogpdf/dx, dlogpdf/ds)\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of logpdf wrt x and s\n",
    "print(f\" Gradient wrt x: {x_in.grad}\")\n",
    "print(f\" Gradient wrt s: {s_in.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this was an arbitrary expression for the covariance of the Gaussian. You can try to change the expression and see how the gradient changes.\n",
    "\n",
    "If we compare with the CUQIpy package, we are not able to compute the gradient with respect to $s$, because there is no way to derive the gradient from the user-defined expression using NumPy and SciPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUQIpy Gaussian\n",
    "x_cuqi = cuqi.distribution.Gaussian(np.zeros(2), cov=lambda s: np.exp(-s)**2*np.eye(2), name=\"x\")\n",
    "\n",
    "# Evaluating logpdf goes fine\n",
    "x_cuqi.logd(x=np.ones(2), s=2)\n",
    "\n",
    "# We can even get gradient w.r.t x\n",
    "x_cuqi(s=2).gradient(np.ones(2))\n",
    "\n",
    "# But not with respect to s! This is because there is a user-defined expression for the covariance that we cant differentiate\n",
    "#x_cuqi.gradient(x=np.ones(2), s=2) # This wont work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hamiltonian Monte Carlo with CUQIpy-PyTorch <a class=\"anchor\" id=\"hmc-cuqipy-pytorch\"></a>\n",
    "\n",
    "We can now use the CUQIpy-PyTorch to define a Bayesian model and use Hamiltonian Monte Carlo to sample from the posterior distribution.\n",
    "\n",
    "We built the interface to exactly match CUQIpy! The only difference is that we use the `cuqipy_pytorch` package instead of the `cuqi` package.\n",
    "\n",
    "In the section earlier we already loaded the distributions. What remains is to load a sampler. Here we load a new NUTS sampler, which automatically handles all the details of gradient computation we saw in the previous section, so we do not need to worry about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqipy_pytorch.sampler import NUTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let us even define a function that can simply takes our defined Bayesian model and data and uses the NUTS sampler to sample the parameters of the Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenience function to sample a Bayesian model\n",
    "def sample(*densities, Ns=500, Nb=500, **data):\n",
    "    \"\"\" Sample given by a list of densities. The observations are given as keyword arguments. \"\"\"\n",
    "    P = StackedJointDistribution(*densities)\n",
    "    return NUTS(P(**data)).sample(Ns, Nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function first defines a `cuqi` joint distribution from the given densities, conditions the joint distribution on potential data, then uses the NUTS sampler to sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how this works let us first sample a case with two independent Gaussian distributions.\n",
    "\n",
    "\\begin{align*}\n",
    "x &\\sim \\mathcal{N}(0, 1) \\\\\n",
    "y &\\sim \\mathcal{N}(3, 5)\n",
    "\\end{align*}\n",
    "\n",
    "This is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian model\n",
    "x = Gaussian(0, 1)\n",
    "y = Gaussian(3, 5)\n",
    "\n",
    "# Sample from the model\n",
    "samples = sample(x, y)\n",
    "\n",
    "# Plot the samples\n",
    "samples[\"x\"].plot_trace()\n",
    "samples[\"y\"].plot_trace()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a slightly more complicated model\n",
    "\n",
    "\\begin{align*}\n",
    "d &\\sim \\mathrm{Gamma}(1, 1) \\\\\n",
    "x &\\sim \\mathcal{N}(0, \\exp(-d)) \\\\\n",
    "y &\\sim \\mathcal{N}(x^2, 5)\n",
    "\\end{align*}\n",
    "\n",
    "and suppose we observe $y=1$. We can sample from the posterior distribution $p(d, x \\mid y=1)$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bayesian model\n",
    "d = Gamma(1, 1)\n",
    "x = Gaussian(0, lambda d: xp.exp(-d))\n",
    "y = Gaussian(lambda x: x**2, 5)\n",
    "\n",
    "# Sample from the model, given y=1\n",
    "samples = sample(d, x, y, y=1)\n",
    "\n",
    "# And plot samples of x and d\n",
    "samples[\"d\"].plot_trace()\n",
    "samples[\"x\"].plot_trace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this gives you a good idea of how to use CUQIpy-PyTorch to define Bayesian models and sample from them using Hamiltonian Monte Carlo. We end this section with a classic example of Bayesian inference using Hamiltonian Monte Carlo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eight schools model\n",
    "The eight schools model is a classic example made famous by the Bayesian Data Analysis book by Gelman et. al. \n",
    "\n",
    "It is often used to illustrate the notation and code-style of probabilistic programming languages and whether they are able to handle the model.\n",
    "\n",
    "The actual model is explained in the BDA book or in the Edward 1.0 PPL notebook ([link](https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb)). We do not go into details here.\n",
    "\n",
    "The Bayesian model can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu &\\sim \\mathcal{N}(0, 10^2)\\\\\n",
    "    \\tau &\\sim \\log\\mathcal{N}(5, 1)\\\\\n",
    "    \\boldsymbol \\theta' &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_m)\\\\\n",
    "    \\boldsymbol \\theta &= \\mu + \\tau \\boldsymbol \\theta'\\\\\n",
    "    \\mathbf{y} &\\sim \\mathcal{N}(\\boldsymbol \\theta, \\boldsymbol \\sigma^2 \\mathbf{I}_m)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{y}\\in\\mathbb{R}^m$ and $\\boldsymbol \\sigma\\in\\mathbb{R}^m$ is observed data.\n",
    "\n",
    "In CUQIpy-PyTorch we can define the model and sample as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = xp.tensor([28, 8, -3,  7, -1, 1,  18, 12], dtype=xp.float32)\n",
    "σ_obs = xp.tensor([15, 10, 16, 11, 9, 11, 10, 18], dtype=xp.float32)\n",
    "\n",
    "μ     = Gaussian(0, 10**2)\n",
    "τ     = LogGaussian(5, 1)\n",
    "θp    = Gaussian(xp.zeros(8), 1)\n",
    "θ     = lambda μ, τ, θp: μ+τ*θp\n",
    "y     = Gaussian(θ, cov=σ_obs**2)\n",
    "\n",
    "samples = sample(μ, τ, θp, y, y=y_obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then investigate the posterior samples for $\\boldsymbol{\\theta}$, $\\mu$ and $\\tau$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior samples\n",
    "samples[\"θp\"].plot_violin(); \n",
    "print(samples[\"μ\"].mean()) # Average effect\n",
    "print(samples[\"τ\"].mean()) # Average variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main point is that CUQIpy-PyTorch is very flexible and can be used to sample from any combination of distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian inverse problems with CUQIpy-PyTorch <a class=\" anchor\" id=\"bayesian-inverse-problems\"></a>\n",
    "\n",
    "We now turn our attention to Bayesian inverse problems. To start as a sanity check we can use a testproblem from the main CUQIpy package and compare with. Here we consider the a deconvolution problem with Bayesian model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, 0.2 \\mathbf{I}_n)\\\\\n",
    "\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, 0.05 \\mathbf{I}_m)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{y}$ and $\\mathbf{x}$ are random variables and $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ is a known convolution matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, y_data, probinfo = cuqi.testproblem.Deconvolution1D(dim=50, phantom=\"sinc\").get_components()\n",
    "\n",
    "# CUQIpy Bayesian model\n",
    "x = cuqi.distribution.Gaussian(np.zeros(A.domain_dim), cov=0.2)\n",
    "y = cuqi.distribution.Gaussian(A@x, cov=0.05)\n",
    "\n",
    "BP = cuqi.problem.BayesianProblem(y, x).set_data(y=y_data)\n",
    "samples = BP.sample_posterior(1000) # Automatic sampler selection\n",
    "\n",
    "samples.plot_ci(exact=probinfo.exactSolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we utilize the structure of the problem in CUQIpy, the sampling is very fast.\n",
    "\n",
    "We can write the same model in CUQIpy-PyTorch as shown below. Note here that we have to add the forward model `A` to the autograd framework of PyTorch. There is a function `cuqipy_pytorch.add_forward_model` that does this for us. This requires the CUQIpy forward model to have gradient defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUQIpy test problem\n",
    "A, y_data, probinfo = cuqi.testproblem.Deconvolution1D(dim=50, phantom=\"sinc\").get_components()\n",
    "\n",
    "# Add forward model to PyTorch automatic differentiation framework\n",
    "A = cuqipy_pytorch.model.add_to_autograd(A)\n",
    "\n",
    "# CUQIpy-PyTorch Bayesian model\n",
    "x = Gaussian(xp.zeros(A.domain_dim), 0.2)\n",
    "y = Gaussian(A(x), 0.05)\n",
    "\n",
    "samples = sample(x, y, y=y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling is likely to take a bit longer because we do not utilize the structure of the problem. \n",
    "\n",
    "However, in the CUQIpy-PyTorch version, we are free to modify any expressions for the distributions.\n",
    "\n",
    "For example, suppose that we wanted to square the entries of $\\mathbf{x}$ before evaluating $\\mathbf{A}$ for whatever reason, i.e. the model would be\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, 0.2 \\mathbf{I}_n)\\\\\n",
    "\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}(\\mathbf{x}^2), 0.05 \\mathbf{I}_m)\n",
    "\\end{align*}\n",
    "\n",
    "This would not be possible with CUQIpy, as we can see here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUQIpy test problem\n",
    "A, y_data, probinfo = cuqi.testproblem.Deconvolution1D(dim=50, phantom=\"sinc\").get_components()\n",
    "\n",
    "# CUQIpy Bayesian model\n",
    "x = cuqi.distribution.Gaussian(np.zeros(A.domain_dim), cov=0.2)\n",
    "y = cuqi.distribution.Gaussian(lambda x: A@(x**2), cov=0.05)\n",
    "\n",
    "BP = cuqi.problem.BayesianProblem(y, x).set_data(y=y_data)\n",
    "try:\n",
    "    samples = BP.sample_posterior(1000) # Sampling fails because of the nonlinearity\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using CUQIpy-PyTorch we can easily do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUQIpy test problem\n",
    "A, y_data, probinfo = cuqi.testproblem.Deconvolution1D(dim=50, phantom=\"sinc\").get_components()\n",
    "\n",
    "# Add forward model to PyTorch automatic differentiation framework\n",
    "A = cuqipy_pytorch.model.add_to_autograd(A)\n",
    "\n",
    "# CUQIpy-PyTorch Bayesian model\n",
    "x = Gaussian(xp.zeros(A.domain_dim), 0.2)\n",
    "y = Gaussian(lambda x: A(x**2), np.ones(A.range_dim)*0.05)\n",
    "\n",
    "samples = sample(x, y, y=y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not going to be a good model for this problem, but only added for illustration purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Open-ended exploration <a class=\"anchor\" id=\"open-ended-exploration\"></a>\n",
    "\n",
    "With the tools in place, try exploring the following:\n",
    "\n",
    "- Try playing around with the expression for the forward model in the deconvolution problem. Can you enforce non-negativity on the entries of $\\mathbf{x}$? You may want to switch the phantom to one with non-negative entries only.\n",
    "- Try including other parameters such as noise or prior variance in the Bayesian model and sample it again. These need not be Gamma distributions, but can be any distribution you want.\n",
    "- Try sampling from the eight schools model with different priors on $\\mu$ and $\\tau$.\n",
    "- Try your own problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39df9cdae8ebf7efb1525026a7ebb7fcd202c6f8c14fe7ef64f5e199bee61274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
