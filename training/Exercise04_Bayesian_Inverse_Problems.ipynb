{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Exercise 04: Bayesian Inverse Problems\n",
    "\n",
    "In this notebook, we (finally!) get started with uncertainty quantification for inverse problems by way of Bayesian inference.\n",
    "\n",
    "In essence, the goal of inverse problems is to infer the parameters of a physical model from observations. In the context of uncertainty quantification, we are interested in the uncertainty of the inferred parameters. In the Bayesian framework, uncertainty is quantified by a probability distribution over the parameter space. Finally, in Bayesian inference we use Bayes' theorem to define the *posterior distribution* of the parameters by combining prior knowledge and observed data.\n",
    "\n",
    "The aim of this notebook is to show how to use CUQIpy to combine the all the components needed for Bayesian inference in a way that closely matches the mathematical formalism.\n",
    "\n",
    "**Try to run through parts 1 and 2 before working on the optional exercises**\n",
    "\n",
    "## Learning objectives\n",
    "Going through the notebook, you will see how to:\n",
    "\n",
    "* Define distributions for each of the relevant parameters of an inverse problem from the CUQIpy library.\n",
    "* Define a \"Bayesian model\" by combining distributions into a joint distribution.\n",
    "* Construct a posterior distribution by conditioning the joint distribution on observed data.\n",
    "* Sample a posterior distribution with specific choice of sampler.\n",
    "* Analyze the samples from the posterior distribution.\n",
    "* Compute point estimates of posterior, e.g., MAP or ML.\n",
    "* Describe how the high-level \"BayesianProblem\" combines the above steps into a convenient non-expert interface.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Defining the posterior distribution](#posterior)\n",
    "2. [Sampling the posterior](#sampling)\n",
    "3. [High level interface (BayesianProblem)](#BayesianProblem) ★\n",
    "4. [Computing point estimates of the posterior](#pointestimates) ★\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eddd3",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "As we have seen a few times now, we start of by importing the Python packages we need (including CUQIpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../../CUQIpy/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cuqi\n",
    "from cuqi.distribution import JointDistribution, GaussianCov, Cauchy_diff, Laplace_diff, GMRF\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.testproblem import Deconvolution1D, Deconvolution2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining the posterior distribution <a class=\"anchor\" id=\"posterior\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the posterior distribution. The posterior describes the probability distribution of the parameters we are interested in by combining our prior knowledge of the parameters with the data we have observed. In its most general form, the posterior is given by Bayes' theorem:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta \\mid y) = \\frac{p(y \\mid \\theta)p(\\theta)}{p(y)} \\propto p(y \\mid \\theta)p(\\theta),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta$ is the parameter vector of all the parameters we are interested in inferring and $y$ is the observable data. Here probability density function $p(\\theta)$ is the prior distribution of the parameters and $p(y \\mid \\theta)$ is known as the likelihood function. The denominator $p(y)$ is the *evidence* and is a normalization constant (that we typically ignore because it does not affect the MCMC sampling) that ensures that the posterior integrates to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f3764",
   "metadata": {},
   "source": [
    "### Note on the posterior in CUQIpy\n",
    "\n",
    "In CUQIpy, we use a general approach to Bayesian modeling that matches the mathematical formalism as closely as possible. This means that we define distributions for each of the parameters and then combine them into a joint distribution. The posterior is then defined by *conditioning* the joint distribution on the observed data using Bayes' theorem. This approach is very general and allows us to define a wide range of Bayesian models.\n",
    "\n",
    "Defining a posterior distribution for an inverse problem in CUQIpy can thus be summarized in the following steps:\n",
    "1. Define the deterministic forward model for the inverse problem.\n",
    "2. Define distributions of the parameters.\n",
    "4. Combine the distributions into a joint distribution.\n",
    "5. Condition the joint distribution on the observed data to obtain the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic forward model and data\n",
    "Consider an inverse problem\n",
    "$$\\mathbf{y}=\\mathbf{A}\\mathbf{x},$$\n",
    "where $\\mathbf{A}: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the forward model of the inverse problem, $\\mathbf{y}\\in\\mathbb{R}^m$ is the data and $\\mathbf{x}\\in \\mathbb{R}^n$ is the parameter of interest.\n",
    "\n",
    "For this example let us revisit the `Deconvolution1D` testproblem and extract a CUQIpy forward model and some synthetic data (in this case generated from the `sinc` phantom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# For convenience, we define the dimension of the domain\n",
    "n = A.domain_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem. Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the `probInfo` variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); probInfo.exactSolution.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a1531",
   "metadata": {},
   "source": [
    "## 1.2 Define distributions for the parameters\n",
    "\n",
    "For the parameter $\\mathbf{x}$ we must use a priori knowledge to define a distribution. In this case the sinc phantom in the Deconvolution testproblem looks like it can be represented fairly well by a multivariate Gaussian distribution. Therefore we start by defining an i.i.d. Gaussian distribution as for $\\mathbf{x}$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma_\\mathbf{x}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "with some choice of the standard deviation say $\\sigma_\\mathbf{x}=0.1$. This is done in CUQIpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior standard deviation\n",
    "σ_x = 0.1\n",
    "\n",
    "# Define prior\n",
    "x = GaussianCov(np.zeros(n), σ_x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5008c",
   "metadata": {},
   "source": [
    "For the parameter $\\mathbf{y}$ representing the observed data, we are interested in defining the distribution of $\\mathbf{y} \\mid \\mathbf{x}$ ($\\mathbf{y}$ given $\\mathbf{x}$).\n",
    "\n",
    "Here we can use information about the known characteristics of the observed data in the problem. In this case according to the problem info string shown earlier, the noise is additive Gaussian with a standard deviation of 0.05 and because the noise is the only stochastic element of $\\mathbf{y}$ when $\\mathbf{x}$ is fixed we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} \\mid \\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\sigma_\\mathbf{y}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma_\\mathbf{y} = 0.05$. Notice that this definition depends both on the forward model $\\mathbf{A}$ and $\\mathbf{x}$.\n",
    "\n",
    "In CUQIpy, we can define the distribution matching the mathematical expression with the objects for the forward model `A` and `x` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise standard deviation\n",
    "σ_y = 0.05\n",
    "\n",
    "# Define distributions\n",
    "y = GaussianCov(A@x, σ_y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Have a look at the distributions for $x$ and $y$ by calling `print` on them. \n",
    "- How are the two distributions different?\n",
    "- Is it clear that the distribution for $y$ is a conditional distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2f4a9",
   "metadata": {},
   "source": [
    "### Note on notation\n",
    "\n",
    "It is common (for convenience in terms of notation) not to explicitly write the dependance of each random variable when specifying a complete Bayesian model. For example, for the case above one would often write\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma_\\mathbf{x}^2 I)\\\\\n",
    "\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\sigma_\\mathbf{y}^2 I)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bf801",
   "metadata": {},
   "source": [
    "## 1.3 Joint distribution\n",
    "\n",
    "Up until now we have worked with $\\mathbf{x}$ and $\\mathbf{y}$ as two separate objects. However, in the Bayesian framework we are interested in the joint distribution over $\\mathbf{x}$ and $\\mathbf{y}$. The joint distribution is defined as the product of the individual probability density functions. In our case, the joint distribution can be described in density form as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y},\\mathbf{x}) = p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In CUQIpy, we can define the joint distribution simply by passing each of the distributions as arguments to the `JointDistribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c5126",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ca744",
   "metadata": {},
   "source": [
    "## 1.4 Conditioning on observed data to obtain the posterior\n",
    "\n",
    "The final step in defining the posterior distribution is to condition the joint distribution $p(\\mathbf{y},\\mathbf{x})$ on the observed data. This is done by using Bayes' theorem, which in our case yields\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}) \\propto L(\\mathbf{x} \\mid \\mathbf{y})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}) := p(\\mathbf{y} \\mid \\mathbf{x})$ is used for the likelihood function to emphasize that it is a function on $\\mathbf{x}$ and not on $\\mathbf{y}$.\n",
    "\n",
    "CUQIpy can automatically derive the posterior distribution for any joint distribution by conditioning on one or more parameters. In our case we simply condition on $\\mathbf{y}=\\mathbf{y}^\\mathrm{data}$ to obtain the posterior distribution $p(\\mathbf{x} \\mid \\mathbf{y}^\\mathrm{data})$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=y_data) # Applies Bayes' rule automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab207d",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9def31",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "What happens if you call the `sample` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "# 2. Sampling the posterior <a class=\"anchor\" id=\"sampling\"></a>\n",
    "In CUQIpy, we provide a number of samplers in the sampler module. All samplers have the same signature, namely\n",
    "`Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize [(Linear_RTO)](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.Linear_RTO.html#cuqi.sampler.Linear_RTO) sampler is a good choice. Like any of the other samplers, we set-up the sampler by simply providing the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = cuqi.sampler.Linear_RTO(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "After the sampler is defined we can compute samples via the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, the returned object is a `cuqi.samples.Samples` object.\n",
    "\n",
    "As we have already seen this object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the sampler has converged and there is little need for removing samples that are part of a \"burn-in\" period.\n",
    "\n",
    "The good sampling is in large part due to the Linear_RTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see `samples.burnthin?`) and store the burnthinned samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d95f1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The Linear_RTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the log-posterior. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers above (click the links above to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from Linear_RTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as Linear_RTO or NUTS. For some samplers, the method `sample_adapt` will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "# 3. High-level interface (BayesianProblem) ★ <a class=\"anchor\" id=\"BayesianProblem\"></a>\n",
    "\n",
    "Finally, we make the connection to the `BayesianProblem` in CUQIpy that we saw in an earlier notebook for the non-expert interface.\n",
    "\n",
    "The Bayesian Problem class tries to conveniently wrap most of the steps we have seen in this notebook into a single object.\n",
    "\n",
    "What is needed is to define the deterministic forward model and distributions for each of the parameters. The rest is handled by the Bayesian Problem class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3184658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = GaussianCov(mean=np.zeros(A.domain_dim), cov=0.1**2)\n",
    "y = GaussianCov(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# Define Bayesian problem and do UQ!\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "BP.UQ(exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d2603",
   "metadata": {},
   "source": [
    "One can also use the `sample_posterior` method to get samples of the posterior without the UQ plots.\n",
    "\n",
    "*The automatic sampler selection is still work-in-progress and one of the goals of CUQI project is to determine best choices of samplers for a variety of inverse problems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = BP.sample_posterior(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9382e9",
   "metadata": {},
   "source": [
    "Similar to distributions and samplers, the `BayesianProblem` `sample_posterior` method returns a `cuqi.samples.Samples` object so we can e.g. plot the credibility interval easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_ci(95,exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try another prior such as [GMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.GMRF.html#cuqi.distribution.GMRF), [Laplace_diff](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Laplace_diff.html#cuqi.distribution.Laplace_diff) or [Cauchy_diff](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Cauchy_diff.html#cuqi.distribution.Cauchy_diff) for the 1D case (look up appropriate arguments in the documentation).\n",
    "- Try switching the testproblem from Deconvolution1D to [Deconvolution2D](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.testproblem/cuqi.testproblem.Deconvolution2D.html#cuqi.testproblem.Deconvolution2D) (look up appropriate arguments in the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this code or write your own from scratch\n",
    "\n",
    "# 1. Forward model and data\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# 2. Distributions\n",
    "x = GaussianCov(mean=np.zeros(A.domain_dim), cov=0.1**2)\n",
    "y = GaussianCov(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# 3. Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "\n",
    "# 4. Sample posterior\n",
    "samples = BP.sample_posterior(200)\n",
    "\n",
    "# 5. Analyze posterior\n",
    "samples.plot_ci(exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097194b0",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}^\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine in case something was changed\n",
    "\n",
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = GaussianCov(mean=np.zeros(A.domain_dim), cov=0.1**2)\n",
    "y = GaussianCov(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# Define Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP() # Maximum a posteriori estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931bb8f",
   "metadata": {},
   "source": [
    "The automatic solver selection is also still work-in-progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cab183",
   "metadata": {},
   "source": [
    "After we have computed the MAP, We can the estimate to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a931f3a-9534-48b1-a273-5bc8616954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching to the Deconvolution2D testproblem. You may have to play with the prior standard deviation to get a good MAP estimate.\n",
    "- Try switching the prior to a Cauchy_diff distribution for the 1D case. Does the MAP estimate change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
