{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Exercise 04: Bayesian Inverse Problems\n",
    "\n",
    "In this notebook, we finally get started with uncertainty quantification for inverse problems.\n",
    "\n",
    "In essence, the goal of inverse problems is to infer the parameters of a physical model from observations. In the context of uncertainty quantification, we are interested in the uncertainty of the inferred parameters. In the Bayesian framework uncertainty is quantified by a probability distribution over the parameter space. In this notebook, we use Bayesian inference to infer the so-called posterior distribution of the parameters, which will be described in more detail later.\n",
    "\n",
    "To aim of this notebook is to show how to use CUQIpy to combine the all the components needed for Bayesian inference.\n",
    "\n",
    "**Try to run through parts 1 and 2 before working on the optional exercises**\n",
    "\n",
    "## Learning objectives\n",
    "Going through the notebook, you will learn how to:\n",
    "\n",
    "* Load an existing inverse problem from the CUQIpy library.\n",
    "* Define distributions for each of the relevant variables in the inverse problem.\n",
    "* Define a Bayesian model by combining distributions into a joint distribution.\n",
    "* Construct a posterior distribution by adding observed data to the joint distribution.\n",
    "* Sample a posterior distribution with specific choice of sampler.\n",
    "* Analyze the samples from the posterior distribution.\n",
    "* Compute point estimates of posterior, e.g., MAP or ML.\n",
    "* Describe how the high-level \"BayesianProblem\" combines the above steps into a convenient non-expert interface.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Defining the posterior distribution](#posterior)\n",
    "2. [Sampling the posterior](#sampling)\n",
    "3. [Computing point estimates of the posterior](#pointestimates) ★\n",
    "4. [Connection to BayesianProblem](#BayesianProbem) ★\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eddd3",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "As we have seen a few times now, we start of by importing the Python packages we need (including CUQIpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../../cuqipy/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cuqi\n",
    "from cuqi.distribution import Gaussian, JointDistribution, GaussianCov\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.testproblem import Deconvolution1D, Deconvolution2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining the posterior distribution <a class=\"anchor\" id=\"posterior\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the so-called posterior distribution. In short, the posterior is defined (ignoring scaling constants) as products of likelihoods (the probability of the data given the parameters) and priors (the probability of the parameters given prior knowledge).\n",
    "\n",
    "However, before defining the posterior we first must define a deterministic forward model for the inverse problem and find some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## Forward model and data\n",
    "Consider an inverse problem\n",
    "$$y=Ax+e, \\quad y\\in\\mathbb{R}^m, \\, x\\in \\mathbb{R}^n,$$\n",
    "where $A: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the forward model of the inverse problem and $e\\in\\mathbb{R}^m$ is additive measurement noise.\n",
    "\n",
    "For this example let us revisit the `Deconvolution1D` testproblem and extract a CUQIpy model and some synthetic data (in this case generated from the default phantom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(dim=50) #A, y_data, probInfo = Deconvolution1D(dim=50).model_data_info\n",
    "\n",
    "# For convenience, we define the dimension of the domain\n",
    "n = A.domain_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem. Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the probInfo variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); probInfo.exactSolution.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a1531",
   "metadata": {},
   "source": [
    "## Bayesian modelling\n",
    "\n",
    "The main principle in Bayesian modelling is to define a joint distribution over all the relevant variables in the problem. In the case of this problem, this means that we must define a joint distribution over the prior parameter $x$ and the data parameter $y$. To do this we must first decide on distributions that we can model $x$ and $y$ with.\n",
    "\n",
    "For the prior variable $x$, the default phantom in the Deconvolution testproblem looks like it can be represented well by a multivariate Gaussian distribution. Therefore we start by defining an i.i.d. Gaussian distribution as for $x$.\n",
    "\n",
    "For the data variable $y$ we are actually interested in the distribution of $y \\mid x$ ($y$ given $x$). Here we can use information about the noise in the problem to define a distribution for $y \\mid x$. In this case according to the problem info string, the noise is additive Gaussian with a standard deviation of 0.05 and because the noise is the only stochastic element of $y$ when $x$ is fixed we can define a Gaussian distribution for $y \\mid x$.\n",
    "\n",
    "That is, letting $\\delta = 0.1$ and $\\sigma = 0.05$ we can write the Bayesian model for our inverse problem as\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "x &\\sim \\mathcal{N}(0, \\delta^2 I)\\\\\n",
    "y \\mid x &\\sim \\mathcal{N}(Ax, \\sigma^2 I)\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Often for convenience in terms of notation, we do not explicitly write the dependence of the distributions on the variables they are defined for. For example, we can write the above as\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "x &\\sim \\mathcal{N}(0, \\delta^2 I)\\\\\n",
    "y &\\sim \\mathcal{N}(Ax, \\sigma^2 I)\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "In CUQIpy, we can define the above distributions using almost the same notation as the mathematical description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "d = 0.1\n",
    "s = 0.05\n",
    "\n",
    "# Define distributions\n",
    "x = GaussianCov(np.zeros(n), d**2)\n",
    "y = GaussianCov(A@x, 0.05**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Have a look at the distributions for $x$ and $y$ by calling `print` on them. \n",
    "- What is the difference between the two distributions? \n",
    "- Does this match the mathematical description of the distributions above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bf801",
   "metadata": {},
   "source": [
    "## Joint distribution and posterior\n",
    "\n",
    "Up until now we have worked with $x$ and $y$ as individual random variables. However, in the Bayesian framework we are interested in the joint distribution over $x$ and $y$. The joint distribution is defined as the product of the individual distributions. In our case, this means that the joint distribution can be described in density form as\n",
    "\n",
    "$$\n",
    "p(y,x) = p(y \\mid x)p(x).\n",
    "$$\n",
    "\n",
    "In CUQIpy, we can define the joint distribution as the product of the individual distributions simply by passing them as arguments to the `JointDistribution` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c5126",
   "metadata": {},
   "source": [
    "The joint distribution can give us a lot of information about the problem. This is nicely summarized by calling `print` on the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b57699",
   "metadata": {},
   "source": [
    "### Sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ca744",
   "metadata": {},
   "source": [
    "### The posterior\n",
    "\n",
    "The posterior pdf is given by Bayes rule\n",
    "\n",
    "$$p(x|b)\\propto p(b|x)p(x),$$\n",
    "\n",
    "where $p(\\cdot)$ are probability density functions (pdfs). Here $p(b|x)$ describes the distribution of the data given $x$ and $p(x)$ the distribution of $x$.\n",
    "\n",
    "Once we have the likelihood and prior, we have all components to define the posterior distribution. This is then simply done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint._allow_reduce = True # TODO\n",
    "posterior = joint(y=y_data)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "The posterior is essentially just another CUQIpy distribution. Have a look at `posterior?` to see what attributes and methods are available. What happends if you call the `sample` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "## 2. Sampling the posterior <a class=\"anchor\" id=\"sampling\"></a>\n",
    "In CUQIpy, we provide a number of samplers in the sampler module. All samplers have the same signature, namely\n",
    "`Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize (Linear_RTO) sampler is a good choice. Like any of the other samplers, we set-up the sampler by simply providing the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = cuqi.sampler.Linear_RTO(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "and then running the sampler and storing the samples in the variable `samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, when sampling using the sampler module the returned object is a `cuqi.samples.Samples` object. As we have already seen this object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good without much initial burn-in. This is in large part due to the Linear_RTO sampler. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see `samples.burnthin?`) and store the burnthinned samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from probInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The Linear_RTO sampler can only sample Gaussian posteriors that also have an underlying linear model. It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **pCN** - Works OK with enough samples (>5000 in this case)\n",
    "* **CWMH** - Works OK with enough samples (>5000 in this case)\n",
    "* **NUTS** - A well established sampler in literature. Also NUTS requires gradients!\n",
    "\n",
    "#### ★ Try yourself (optional):  \n",
    "Try sampling the posterior above using the NUTS, CWMH or pCN sampler (see e.g. the help documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from Linear_RTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "## 3. High-level interface (BayesianProblem) ★ <a class=\"anchor\" id=\"BayesianProblem\"></a>\n",
    "\n",
    "Finally, we make the connection to the \"BayesianProblem\" CUQIpy that we saw in exercise 01 for the non-expert interface. Essentially the BayesianProblem tries to conveniently wrap all of the steps we have seen earlier in this notebook into a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3184658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GaussianCov(mean=np.zeros(n), cov=0.1**2)\n",
    "y = GaussianCov(mean=A@x, cov=0.05**2).to_likelihood(y_obs)\n",
    "BP = BayesianProblem(y, x)\n",
    "BP.UQ(exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d2603",
   "metadata": {},
   "source": [
    "For example, the `sample_posterior` method defines a posterior distribution (in the same way as we saw earlier), selects an appropriate CUQIpy sampler and runs the sampler. \n",
    "\n",
    "*The sampler selection is still work-in-progress and part of the CUQI project is to figure out which samplers are best suited for which inverse problems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = BP.sample_posterior(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9382e9",
   "metadata": {},
   "source": [
    "Similar to distributions and samplers the BayesianProblem sample method returns a `cuqi.samples.Samples` object so we can e.g. plot the credibility interval easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_ci(95,exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6a8c6",
   "metadata": {},
   "source": [
    "MAP (and ML) estimates are also supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP()\n",
    "x_map.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7aba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probInfo.exactSolution.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP.sample_prior(5).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching the testproblem from Deconvolution1D to Deconvolution2D\n",
    "- Try another prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4ce42",
   "metadata": {},
   "source": [
    "### Where to go from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf171b5",
   "metadata": {},
   "source": [
    "## 4. Computing point estimates of the posterior ★  <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In Bayesian inverse problems one may also be interested in computing point estimates of the posterior, or perhaps even the likelihood. There are generally two ways to go about this 1) compute point estimates from the posterior samples and 2) compute point estimates using optimization-based methods.\n",
    "\n",
    "In this section, we are going to show-case the CUQIpy solver module aimed at computing point estimates using the second approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd9e07",
   "metadata": {},
   "source": [
    "### MAP estimation\n",
    "The Maxiumum a posteriori (MAP) estimate is equal to the mode of the posterior distribution, and can be computed by maximizing the pdf (or logpdf) of the posterior. Using the CUQIpy solver module this follows a similar flow to what we have seen before with one exception. In this case, we are forced to provide an initial guess. In this case we provide the initial guess as a CUQIarray with the posterior geometry, to allow for later plotting of the map estimate (this will most likely be handled automatically in future versions of CUQIpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a81f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = cuqi.samples.CUQIarray(np.zeros(n), geometry=posterior.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d594e",
   "metadata": {},
   "source": [
    "Given the initial guess, we simply set up a solver to maximize the logpdf of the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = cuqi.solver.maximize(posterior.logpdf, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d6309",
   "metadata": {},
   "source": [
    "With this we can simpy run the solve method to compute the MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map, info = MAP.solve()\n",
    "x_map.plot()\n",
    "probInfo.exactSolution.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd96f65",
   "metadata": {},
   "source": [
    "The info output argument contains some useful information to validate if the optimization went well or not. In this case we can check the convergence status and iteration number.\n",
    "\n",
    "*Note: Compared to the sampler module, in the solver module we have to resort to using a bit more \"Python lingo\" to get our desired results. This should indicate that this module is still at its early design-states*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info[\"message\"])\n",
    "print(\"Number of iterations: {}\".format(info[\"nit\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ecb90",
   "metadata": {},
   "source": [
    "#### Try yourself (optional):  \n",
    "If time permits, try playing around with the solver module. Suggested things to try:\n",
    "* Try computing the maximum likelihood (ML) estimate. What do you expect the ML estimate to look like?\n",
    "* By default the solver will use a numerical estimate of the gradient of the objective function. Can you find a way to pass the actual gradient? Did this increase the convergence speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
