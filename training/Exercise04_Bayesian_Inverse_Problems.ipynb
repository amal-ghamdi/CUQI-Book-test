{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad78cad",
   "metadata": {},
   "source": [
    "# Exercise 04: Bayesian Inverse Problems\n",
    "\n",
    "In this notebook, we get started with uncertainty quantification for inverse problems by way of Bayesian inference.\n",
    "\n",
    "In essence, the goal of inverse problems is to infer the parameters of a physical model from observations. In the context of uncertainty quantification, we are interested in the uncertainty of the inferred parameters.\n",
    "\n",
    "In the Bayesian framework, uncertainty is quantified by a probability distribution over the parameter space. Finally, in Bayesian inference we use Bayes' theorem to define the *posterior distribution* of the parameters by combining prior knowledge and observed data.\n",
    "\n",
    "The aim of this notebook is to show how to use CUQIpy to combine the all the components needed for Bayesian inference in a way that closely matches the mathematical formalism.\n",
    "\n",
    "## Learning objectives\n",
    "Going through the notebook, you will see how to:\n",
    "\n",
    "* Define distributions for each of the relevant parameters of an inverse problem from the CUQIpy library.\n",
    "* Define a Bayesian Problem by combining distributions into a joint distribution.\n",
    "* Construct a posterior distribution by conditioning the joint distribution on observed data.\n",
    "* Sample a posterior distribution with specific choice of sampler.\n",
    "* Analyze the samples from the posterior distribution.\n",
    "* Compute point estimates of posterior, e.g., MAP or ML.\n",
    "* Describe how the high-level \"BayesianProblem\" combines the above steps into a convenient non-expert interface.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Defining the posterior distribution](#posterior)\n",
    "2. [Sampling the posterior](#sampling)\n",
    "3. [High level interface (BayesianProblem)](#BayesianProblem)\n",
    "4. [Computing point estimates of the posterior](#pointestimates) ★\n",
    "\n",
    "The section marked with ★ is optional and can be skipped if you are short on time.\n",
    "\n",
    "## References\n",
    "[1] Gelman, Andrew, et al. \"Bayesian workflow.\" arXiv preprint arXiv:2011.01808 (2020) https://arxiv.org/abs/2011.01808.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016eddd3",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "As we have seen a few times now, we start of by importing the Python packages we need (including CUQIpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cuqi\n",
    "from cuqi.distribution import JointDistribution, Gaussian, CMRF, LMRF, GMRF, Laplace, Lognormal, Gamma\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.testproblem import Deconvolution1D, Deconvolution2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61fdd109",
   "metadata": {},
   "source": [
    "# 1. Defining the posterior distribution <a class=\"anchor\" id=\"posterior\"></a>\n",
    "\n",
    "Solving a Bayesian inverse problem amounts to characterizing the posterior distribution. The posterior describes the probability distribution of the parameters we are interested in by combining our prior knowledge of the parameters with the data we have observed. In its most general form, the posterior is given by Bayes' theorem:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})} \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\boldsymbol{\\theta}$ is the parameter vector of all the parameters we are interested in inferring and $\\mathbf{y}$ is the observable data. \n",
    "\n",
    "The probability density function $p(\\boldsymbol{\\theta})$ is the prior distribution of the parameters and $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ is known as the likelihood function. The denominator $p(\\mathbf{y})$ is the *evidence* and is a normalization constant (that we typically ignore because it does not affect the MCMC sampling) that ensures that the posterior integrates to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30f3764",
   "metadata": {},
   "source": [
    "### Note on Bayesian modelling with CUQIpy\n",
    "\n",
    "CUQIpy uses a general approach to Bayesian modeling that not only aims to define the posterior distribution, but also to define the joint distribution of all the parameters. This more general approach is useful because it allows one to carry out more tasks related to uncertainty quantification of inverse problems such as prior-predictive checks, model checking, posterior-predictive checks and many more. For more details on some of these topics see the overview in [1].\n",
    "\n",
    "In this notebook, we will only focus on defining the posterior distribution for an inverse problem with the purpose of quantifying the uncertainty of the inferred parameters.\n",
    "\n",
    "In short, the posterior is defined by *conditioning* the joint distribution of all the random variables on the observed data using Bayes' theorem and can be summarized in the following steps:\n",
    "\n",
    "1. Define any deterministic forward models.\n",
    "2. Define distributions for each of the random variables.\n",
    "4. Combine the distributions into a joint distribution.\n",
    "5. Condition the joint distribution on the observed data to obtain the posterior distribution.\n",
    "\n",
    "We now go through these steps in more detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4968e40b",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic forward model and data\n",
    "Consider a Bayesian inverse problem\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{e},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the (deterministic) forward model of the inverse problem and $\\mathbf{y}$ and $\\mathbf{x}$ are random variables representing the observed data and parameter of interest respectively. Here $\\mathbf{e}$ is a random variable representing additive noise in the data.\n",
    "\n",
    "For this example let us revisit the `Deconvolution1D` testproblem and extract a CUQIpy forward model and some synthetic data denoted $\\mathbf{y}_\\mathrm{data}$ (which is a realization of $\\mathbf{y}$). Note that this is a linear inverse problem, but the same approach can be used for nonlinear inverse problems as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forward model, data and problem information\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# For convenience, we define the dimension of the domain\n",
    "n = A.domain_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d1a8f1",
   "metadata": {},
   "source": [
    "Before going further let us briefly visualize the data and compare with the exact solution to the problem. Here we should expect to see that the data is a convolved version of the exact solution with some added noise. We can also inspect the `probInfo` variable to get further information about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.subplot(121); probInfo.exactSolution.plot(); plt.title('Exact Solution')\n",
    "plt.subplot(122); y_data.plot(); plt.title('Data')\n",
    "\n",
    "# Print information about the problem\n",
    "print(probInfo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "984a1531",
   "metadata": {},
   "source": [
    "## 1.2 Define distributions for each of the random variables\n",
    "\n",
    "The goal now is to define distributions for each of these random variables. One way to think about this is that we are trying to define the a Bayesian problem for the *data generating process* [1].\n",
    "\n",
    "For the unknown $\\mathbf{x}$ we must use a priori knowledge to define a distribution. In this case the sinc phantom in the Deconvolution testproblem looks like it can be represented fairly well by a multivariate Gaussian distribution. Therefore we start by defining an i.i.d. Gaussian distribution for $\\mathbf{x}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma_\\mathbf{x}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "with some choice of the standard deviation say $\\sigma_\\mathbf{x}=0.5$. This is done in CUQIpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior standard deviation\n",
    "σ_x = 0.5\n",
    "\n",
    "# Define prior\n",
    "x = Gaussian(np.zeros(n), cov=σ_x**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e5008c",
   "metadata": {},
   "source": [
    "From the problem info string we see that the noise is Gaussian with standard deviation $\\sigma_\\mathbf{e}=0.05$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{e} &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma_\\mathbf{e}^2 \\mathbf{I}).\n",
    "\\end{align*}\n",
    "\n",
    "For random variable $\\mathbf{y}$ representing the observed data, we are interested in defining the distribution of $\\mathbf{y} \\mid \\mathbf{x}$. Because the noise is the only stochastic element of $\\mathbf{y}$ when $\\mathbf{x}$ is fixed according to our model we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} \\mid \\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\sigma_\\mathbf{y}^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma_\\mathbf{e}=\\sigma_\\mathbf{y} = 0.05$. Notice that this definition depends both on the forward model $\\mathbf{A}$ and the random variable $\\mathbf{x}$.\n",
    "\n",
    "In CUQIpy, we can define the distribution for $\\mathbf{y} \\mid \\mathbf{x}$ matching the mathematical expression using our variables `A` and `x` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise standard deviation\n",
    "σ_y = 0.05\n",
    "\n",
    "# Define distributions\n",
    "y = Gaussian(A@x, cov=σ_y**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a146bbf",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Have a look at the distributions for $\\mathbf{x}$ and $\\mathbf{y}$ by calling `print` on them. \n",
    "- How are the distributions of the two random variables different?\n",
    "- Is it clear that the distribution for $\\mathbf{y}$ is a conditional distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc2f4a9",
   "metadata": {},
   "source": [
    "### Note on notation\n",
    "\n",
    "It is common (for convenience in terms of notation) not to explicitly write the dependance of each random variable when specifying a complete Bayesian problem. For example, for the case above one would often write\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma_\\mathbf{x}^2 I)\\\\\n",
    "\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\sigma_\\mathbf{y}^2 I),\n",
    "\\end{align*}\n",
    "\n",
    "where the dependance of $\\mathbf{y}$ on $\\mathbf{x}$ is implicit. \n",
    "\n",
    "This compact notation completely specifies the Bayesian problem for the so-called *data generating process*, making clear all the assumptions about the data and parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb3bf801",
   "metadata": {},
   "source": [
    "## 1.3 Joint distribution\n",
    "\n",
    "In the Bayesian framework we are interested in the joint distribution over $\\mathbf{x}$ and $\\mathbf{y}$. The joint distribution is defined as the product of the individual probability density functions. In our simple case, the joint distribution can be described in density form as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y},\\mathbf{x}) = p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In CUQIpy, we can define the joint distribution simply by passing each of the distributions as arguments to the `JointDistribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint distribution p(y,x)\n",
    "joint = JointDistribution(y, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035c5126",
   "metadata": {},
   "source": [
    "Calling `print` on the joint distribution gives a nice overview matching the mathematical description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we are not going to dive further into the details of the joint distribution, and simply use it to define the posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d9ca744",
   "metadata": {},
   "source": [
    "## 1.4 Conditioning on observed data to obtain the posterior\n",
    "\n",
    "The final step in defining the posterior distribution is to condition the joint distribution $p(\\mathbf{y},\\mathbf{x})$ on the observed data, say $\\mathbf{y}_\\mathrm{data}$. This is done by using Bayes' theorem, which in our simple case can easily be derived as\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}) \\propto L(\\mathbf{x} \\mid \\mathbf{y})p(\\mathbf{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where we use the notation $L(\\mathbf{x} \\mid \\mathbf{y}) := p(\\mathbf{y} \\mid \\mathbf{x})$ for the likelihood function to emphasize that it is a function of $\\mathbf{x}$ and not on $\\mathbf{y}$.\n",
    "\n",
    "For the specific case of having observed $\\mathbf{y}_\\mathrm{data}$, we write posterior distribution as $p(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{data})$.\n",
    "\n",
    "CUQIpy can automatically derive the posterior distribution for any joint distribution by passing the observed data as an argument to the \"call\" (condition) method of the joint distribution. This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=y_data) # Applies Bayes' rule automatically"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ab207d",
   "metadata": {},
   "source": [
    "We can now inspect the posterior distribution by calling `print` on it. Notice that the posterior equation matches the mathematical expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9def31",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "The posterior is essentially just another CUQIpy distribution. Have a look at the [Posterior class](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.Posterior.html) in the online documentation to see what attributes and methods are available.\n",
    "\n",
    "Try evaluating the posterior log probability density function (logpdf) at some point say $\\mathbf{1}$ and $2\\cdot\\mathbf{1}$, where $\\mathbf{1}$ is a ones-vector.\n",
    "\n",
    "Try evaluating the posterior pdf at the same points. Can you explain why the pdf gives the same result for both points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e76027",
   "metadata": {},
   "source": [
    "# 2. Sampling the posterior <a class=\"anchor\" id=\"sampling\"></a>\n",
    "\n",
    "Now that we have defined the posterior distribution for our parameter of interest $\\mathbf{x}$ given $\\mathbf{y}_\\mathrm{data}$, we can characterize the parameter and its uncertainty by samples from the posterior distribution. However, in general the posterior is not a simple distribution that we can simply sample from. Instead, we need to rely on Markov Chain Monte Carlo (MCMC) methods to sample from the posterior.\n",
    "\n",
    "In CUQIpy, we provide a number of samplers in the sampler module that can be used to sample probability distributions. All samplers have the same signature, namely `Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In the case of the posterior above which is defined from a linear model and Gaussian likelihood and prior, the Linear Randomize-then-Optimize [(Linear_RTO)](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.Linear_RTO.html#cuqi.sampler.Linear_RTO) sampler is a good choice. Like any of the other samplers, we set up the sampler by simply providing the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = cuqi.sampler.Linear_RTO(posterior)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536514b",
   "metadata": {},
   "source": [
    "After the sampler is defined we can compute samples via the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.sample(500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05e1c5",
   "metadata": {},
   "source": [
    "Similar to directly sampling distributions in CUQIpy, the returned object is a `cuqi.samples.Samples` object.\n",
    "\n",
    "As we have already seen this object has a number of methods available. In this case, we are interested in evaluating if the sampling went well. To do this we can have a look at the chain history for 2 different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_chain([30, 45]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fa415d2",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the sampler has converged and there is little need for removing samples that are part of a \"burn-in\" period.\n",
    "\n",
    "The good sampling is in large part due to the Linear_RTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see `samples.burnthin?`) and store the burnthinned samples in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final = samples.burnthin(Nb=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c5587f",
   "metadata": {},
   "source": [
    "Finally, we can plot a credibility interval of the samples and compare to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final.plot_ci(95, exact=probInfo.exactSolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "779d95f1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The Linear_RTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the logpdf. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d2b8d05",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers (click the links to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from Linear_RTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as Linear_RTO or NUTS. For some samplers, the method `sample_adapt` will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a9868bd",
   "metadata": {},
   "source": [
    "# 3. High-level interface (BayesianProblem) <a class=\"anchor\" id=\"BayesianProblem\"></a>\n",
    "\n",
    "Finally, we make the connection to the `BayesianProblem` class in CUQIpy that provides a the non-expert interface for Bayesian inference.\n",
    "\n",
    "The Bayesian Problem class tries to conveniently wrap most of the steps we have seen in this notebook into a single object.\n",
    "\n",
    "What is needed is to define the deterministic forward model and distributions for each of the parameters. The rest is handled by the Bayesian Problem class as exemplified below.\n",
    "\n",
    "Note that the `BayesianProblem` class automatically defines the posterior distribution and chooses a sampler based on the problem type. \n",
    "*The automatic sampler selection is still work-in-progress and one of the goals of CUQI project is to determine best choices of samplers for a variety of inverse problems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3184658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = Gaussian(mean=np.zeros(A.domain_dim), cov=0.5**2)\n",
    "y = Gaussian(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# Define Bayesian problem, set data and do UQ!\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "BP.UQ(exact=probInfo.exactSolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "364d2603",
   "metadata": {},
   "source": [
    "One can also use the `sample_posterior` method to get samples of the posterior without the UQ plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = BP.sample_posterior(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab9382e9",
   "metadata": {},
   "source": [
    "Similar to distributions and samplers, the `BayesianProblem` `sample_posterior` method returns a `cuqi.samples.Samples` object so we can e.g. plot the credibility interval easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot_ci(95, exact=probInfo.exactSolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "####  Try yourself (optional):  \n",
    "\n",
    "- Try another prior such as [GMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.GMRF.html#cuqi.distribution.GMRF), [LMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.LMRF.html#cuqi.distribution.LMRF) or [CMRF](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.distribution/cuqi.distribution.CMRF.html#cuqi.distribution.CMRF) for the 1D case (look up appropriate arguments in the documentation).\n",
    "- Try switching the testproblem from Deconvolution1D to [Deconvolution2D](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.testproblem/cuqi.testproblem.Deconvolution2D.html#cuqi.testproblem.Deconvolution2D) (look up appropriate arguments in the documentation).\n",
    "- You may also try defining your own Bayesian inverse problem using this interface. ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this code or write your own from scratch\n",
    "\n",
    "# 1. Forward model and data\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# 2. Distributions\n",
    "x = Gaussian(mean=np.zeros(A.domain_dim), cov=0.5**2)\n",
    "y = Gaussian(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# 3. Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)\n",
    "\n",
    "# 4. Sample posterior\n",
    "samples = BP.sample_posterior(200)\n",
    "\n",
    "# 5. Analyze posterior\n",
    "samples.plot_ci(exact=probInfo.exactSolution)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0583cafe",
   "metadata": {},
   "source": [
    "For an example of creating a Bayesian inverse problem with hierarchical priors, see the [Gibbs notebook](Exercise06_Gibbs.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097194b0",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine in case something was changed\n",
    "\n",
    "# Deterministic forward model\n",
    "A, y_data, probInfo = Deconvolution1D.get_components(phantom=\"sinc\")\n",
    "\n",
    "# Distributions for each parameter\n",
    "x = Gaussian(mean=np.zeros(A.domain_dim), cov=0.5**2)\n",
    "y = Gaussian(mean=A@x, cov=0.05**2)\n",
    "\n",
    "# Define Bayesian problem\n",
    "BP = BayesianProblem(y, x).set_data(y=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = BP.MAP() # Maximum a posteriori estimate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2931bb8f",
   "metadata": {},
   "source": [
    "The automatic solver selection is also still work-in-progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24cab183",
   "metadata": {},
   "source": [
    "After we have computed the MAP, we can then estimate to the exact solution (from `probInfo`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a931f3a-9534-48b1-a273-5bc8616954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3aef5f",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "- Try switching to the Deconvolution2D testproblem. You may have to play with the prior standard deviation to get a good MAP estimate.\n",
    "- Try switching the prior to a CMRF distribution for the 1D case. Does the MAP estimate change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff4ac6af9578637e0e623c40bf41129eb04e2c9abec3a9480d43324f3a3fec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
