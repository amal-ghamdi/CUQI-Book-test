{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05:  Solving differential equation-based Bayesian inverse problems using CUQIpy\n",
    "\n",
    "Here we build a Bayesian problem in which the forward model is a partial differential equation model, 1D Heat problem in particular.\n",
    "\n",
    "**Try to at least run through part 1 to 4 before working on the optional exercises**\n",
    "\n",
    "## Learning objectives of this notebook:\n",
    "- Solve PDE-based Bayesian problem using CUQIpy.\n",
    "- Parametrization of the Bayesian parameters (e.g. KL expansion, non-linear maps).\n",
    "- Introducing CUQIpy's PDE class.\n",
    "\n",
    "## Table of contents: \n",
    "* [1. Loading the PDE test problem](#PDE_model)\n",
    "* [2. Building and solving the Bayesian inverse problem](#inverse_problem)\n",
    "* [3. Parametrizing the Bayesian parameters via a general mapping  to enforce positivity](#mapped_geometries)\n",
    "* [4. Parametrizing the Bayesian parameters via step function expansion](#step_function)\n",
    "* [5. Parametrizing the Bayesian parameters via KL expansion](#KL_expansion) ★\n",
    "* [6. Observe on part of the domain](#Partial_Observation) ★\n",
    "* [7. elaboration: the PDEmodel class](#PDE_model_elaborate) ★\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Loading the PDE test problem <a class=\"anchor\" id=\"PDE_model\"></a>\n",
    "\n",
    "We first import the required python standard packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import sys\n",
    "sys.path.append(\"../../CUQIpy/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From cuqi package, we import the classes that we use in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.geometry import MappedGeometry, KLExpansion\n",
    "from cuqi.pde import SteadyStateLinearPDE\n",
    "from cuqi.distribution import GaussianCov, Posterior, Gaussian, JointDistribution\n",
    "from cuqi.sampler import pCN, MetropolisHastings, CWMH\n",
    "from cuqi.testproblem import Heat_1D\n",
    "from cuqi.operator import FirstOrderFiniteDifference\n",
    "from cuqi.pde import SteadyStateLinearPDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test problem `Heat_1D` which provides a one dimensional (1D) time dependent heat model with zero boundary conditions. The model is discretized using finite difference.\n",
    "\n",
    "The PDE is given by:\n",
    "\n",
    "$$ \\frac{\\partial u(x,t)}{\\partial t} - c^2 \\Delta_x u(x,t)   = f(x,t), \\;\\text{in}\\;\\Omega=[0,L] $$\n",
    "$$u(0,t)= u(L,t)= 0 $$\n",
    "\n",
    "where $u(x,t)$ is the temperature and $c^2$ is the thermal diffusivity (assumed to be 1 here). We assume the source term $f$ is zero. The unknown Bayesian parameters for this test problem is the initial heat profile $\\theta(x):=u(x,0)$. The data $\\mathbf{d}$ are the temperature measurements everywhere in the domain at the final time $T$.\n",
    "\n",
    "We load `Heat_1D` using `get_components` method. We can explore `Heat_1D` initialization parameters (which are the same parameters that can be passed to `get_components` method) of the `Heat_1D` test problem by calling `Heat_1D?`. We choose the following set up for the test problem: Number of finite difference nodes N, length of domain L, and the final time T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30  # number of finite difference nodes            \n",
    "L = 1    # Length of the domain\n",
    "T = 0.05  # Final time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the initial condition (the exact solution for the Bayesian problem) to be a step function with three pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myExactSolution = np.zeros(N)\n",
    "myExactSolution[:floor(N/3)+1] = 1\n",
    "myExactSolution[floor(N/3)+1:floor(2*N/3)] = 2\n",
    "myExactSolution[floor(2*N/3):] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we load the `Heat_1D` problem providing our own exact solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, problemInfo = Heat_1D.get_components(dim=N, endpoint=L, max_time=T, exactSolution=myExactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at what we obtain from the test problem. We view the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the returned `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `problemInfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the exact solution of this inverse problem and the exact and noisy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo.exactSolution.plot()\n",
    "problemInfo.exactData.plot()\n",
    "data.plot()\n",
    "plt.legend(['exact solution', 'exact data', 'noisy data']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values of the initial solution and the data at 0 and $L$ are not included in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and solving the Bayesian inverse problem <a class=\"anchor\" id=\"inverse_problem\"></a>\n",
    "\n",
    "Here we want to define the prior $p(x)$, the data distribution $p(y|x)$ and the posterior distribution $p(x|y)$. We start by defining a simple Gaussian prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std = 1.2\n",
    "x = Gaussian(mean*np.ones(N), std, geometry= model.domain_geometry) # The prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Try yourself (optional)\n",
    "* create prior samples (~1 line).\n",
    "* plot the 95% credibility interval of the prior samples (~1 line).\n",
    "* look at the 95% credibility interval of the PDE model solution to quantify the forward uncertainty (~2 lines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the data distribution $p(y|x)$, we first estimate the noise level. Because here we know the exact data, we can estimate the noise level as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_data = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then define the likelihood: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = Gaussian(mean=model, std=sigma_data, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the components we need, we can create the joint distribution $p(x,y)$, from which the posterior distribution can be created by setting $y=\\texttt{data}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution $p(x,y)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution([y, x])\n",
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then we set $y=\\texttt{data}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=data)\n",
    "print(posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the joint distribution to an object of type posterior (this is a temporary hack and in the near future samplers will be able to sample `JointDistributions` directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = posterior._reduce_to_single_density() #TODO: eventually remove this line\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample the posterior. Lets try the preconditioned Crank-Nicolson (pCN) sampler (~60 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = pCN(posterior)\n",
    "posterior_samples = MySampler.sample_adapt(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the $95\\%$ credible interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact = problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean reconstruction of the initial solution matches the general trend of the exact solution to some extent but it does not capture the piece-wise constant nature of the exact solution. Also, if we wish to assume that the initial solution must be positive, we see that the obtained mean and the credible interval has negative values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parametrizing the Bayesian parameters via a general mapping to enforce positivity <a class=\"anchor\" id=\"mapped_geometries\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce the concept of mapped geometries. In many inverse problems, parametrization of the forward model input through possibly nonlinear functions might be needed. For example, in this 1D heat example, lets assume that we want to enforce positivity of the initial condition $u(x,0) =\\theta(x)$. We can use the parametrization $u(x,0) = e^{\\theta(x)}$, where $\\theta$ is the Bayesian parameters (log initial condition). \n",
    "\n",
    "In `CUQIpy`, this can be achieved through a `MappedGeometry` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the mapping function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = lambda x : np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a new version of the `Heat_1D` problem components in which we pass the `map` as a parameter to the `Heat_1D.get_components` method:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, problemInfo = Heat_1D.get_components(dim=N, endpoint=L, max_time=T, exactSolution = myExactSolution, map = map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we repeat the same steps we followed in [section 2](#inverse_problem) to create the posterior distribution from the prior, the data distribution and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior\n",
    "x = Gaussian(mean*np.ones(N), std, geometry= model.domain_geometry)\n",
    "\n",
    "# Data distribution\n",
    "sigma_data = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_data, geometry=model.range_geometry)\n",
    "\n",
    "# Joint distribution and posterior\n",
    "joint =  JointDistribution([y, x])\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a sampler object to sample the posterior distribution (~65 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = pCN(posterior)\n",
    "posterior_samples = MySampler.sample_adapt(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we then plot the credible interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact = problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameterization `map= lambda x : np.exp(x)` ensures that the Bayesian parameters are indeed positive. The solution itself is still not satisfactory. In the next section, we try to improve the solution by incorporating more prior knowledge about the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parametrizing the Bayesian parameters via step function expansion <a class=\"anchor\" id=\" step_function\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the solution of this Bayesian problem is to use better prior information. Here we assume the prior is a step function with three pieces. This also makes the Bayesian problem simpler because now we only have three Bayesian parameters to infer.\n",
    "\n",
    "To test this case we pass `field_type='Step'` to `Heat_1D.get_components`, which creates a `StepExpansion` domain geometry for the model during initializing the `Heat_1D` test problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=30\n",
    "n_steps = 3 # Number of steps in the StepExpansion geometry. \n",
    "#model, data, problemInfo = Heat_1D.get_components(dim=N, endpoint=L, max_time=T,field_type='Step', exactSolution = myExactSolution, n_steps=n_steps)\n",
    "model, data, problemInfo = Heat_1D.get_components(dim=N, endpoint=L, max_time=T,field_type='Step', n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the model.domain_geometry in this case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.domain_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then continue to create the Bayesian problem (prior, data distribution and posterior) with a prior of dimension = 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior\n",
    "x = Gaussian(mean*np.ones(n_steps), std, geometry= model.domain_geometry)\n",
    "\n",
    "# Data distribution\n",
    "sigma_data = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_data, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution([y, x])\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample the posterior using pCN (~60 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = pCN(posterior)\n",
    "posterior_samples = MySampler.sample_adapt(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact = problemInfo.exactSolution)\n",
    "posterior_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the trace plot: a plot of the kernel density estimator (left) and chains (right) of the 3 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show pair plot of 2D marginal posterior distributions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pair plot, we can see clear correlation between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the effective sample size (ESS) which approximately gives the number of independent samples in the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.ess(posterior_samples.to_arviz_inferencedata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it yourself (optional):\n",
    "* For this step function parametrization, try to enforce positivity of the posterior samples via passing `map = lambda x : np.exp(x)` to the `Heat_1D.get_components` method. Then run the pCN sampler again (similar to part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Parametrizing the Bayesian parameters via KL expansion ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the Bayesian inversion for a more general exact solution. We parametrize the Bayesian parameters using Karhunen–Loève (KL) expansion. This will represent the inferred heat initial profile as a linear combination of sine functions. \n",
    "$$ u(x,0) = \\sum_i \\theta_i  (1/i)^{\\text{decay}}  sin(\\frac{i L x}{\\pi}). $$\n",
    "Where $\\theta_i$ are the Bayesian parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the Heat_ID test case and pass `field_type = 'KL'`, which behind the scenes will set the domain geometry of the model to be a KL expansion geometry (`KLExpansion`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=35\n",
    "model, data, problemInfo = Heat_1D.get_components(dim=N, endpoint=L, max_time=T, field_type = 'KL' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we inspect the `model.domain_geometry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.domain_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the exact solution and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo.exactSolution.plot()\n",
    "problemInfo.exactData.plot()\n",
    "data.plot()\n",
    "plt.legend(['exact solution', 'exact data', 'noisy data']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the exact solution here is a general signal that is not constructed from the basis functions. We define the prior $p(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_prior = 9*np.ones(model.domain_dim) #1, 9\n",
    "x = GaussianCov(mean*np.ones(N), sigma_prior, geometry= model.domain_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_data = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_data, geometry=model.range_geometry).to_likelihood(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution([y, x])\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample the posterior, here we use Component-wise Metropolis Hastings (~90 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MySampler = CWMH(posterior, x0=np.ones(N))\n",
    "posterior_samples = MySampler.sample_adapt(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the $95\\%$ credibility interval (you can try plotting different credibility intervals, e.g. $80\\%$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact = problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credibility interval can have zero width at some locations where the upper and lower limit seems to intersect and switch order (uppers becomes lower and vice versa). To look into what actually happen here, we plot some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_burnthin = posterior_samples.burnthin(0,10)\n",
    "for i, s in enumerate(posterior_samples_burnthin):\n",
    "    model.domain_geometry.plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples seem to paint a different picture than what the credibility interval plot shows. Note that the computed credibility interval above, is computed on the domain geometry parameter space, then converted to the function space for plotting. We can alternatively convert the samples to function values first, then compute and plot the credibility interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert samples to function values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funvals_samples = posterior_samples.funvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the credibility interval computed from the function values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funvals_samples.plot_ci(95, exact = problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the credibility interval now reflect what the samples plot shows and does not have these locations where the upper and lower bounds intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effective sample size (ESS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.ess(posterior_samples.to_arviz_inferencedata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the ESS varies considerably among the variables. We can view the trace plot for, let's say, the first and the second variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_trace([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way of looking at the credibility intervals, is to look at the expansion coefficients  $\\theta_i$ credibility intervals. We plot the credibility intervals for these coefficients from both prior  and posterior samples by passing the flag `plot_par=True` to `plot_ci` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x.sample(1000).plot_ci(95, plot_par=True)\n",
    "plt.xticks(np.arange(x.dim)[::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, plot_par=True)\n",
    "plt.xticks(np.arange(x.dim)[::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Observe on part of the domain <a class=\"anchor\" id=\"Partial_Observation\"></a> ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Elaboration: the PDEmodel class <a class=\"anchor\" id=\"PDE_model_elaborate\"></a> ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the model for PDE problems.\n",
    "\n",
    "#### Try it yourself (optional):\n",
    "\n",
    "* View: `model`, `model.pde`, `model.pde.PDE_form`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, for example, create our own PDE model for simple Poisson equation with zero boundaries. We first create the forward difference operator using the cuqi operator `FirstOrderFiniteDifference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poisson = 1000 #Number of nodes\n",
    "L = 1 # Length of the domain\n",
    "dx = L/(n_poisson-1) # grid spacing\n",
    "diff_operator = FirstOrderFiniteDifference(n_poisson,bc_type='zero').get_matrix().todense()/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct the source term (point source):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_term = np.zeros(n_poisson)\n",
    "source_term[int(n_poisson/2)] = 1/dx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the PDE form which consists of the differential operator and the right hand side, and is a function of the Bayesian parameter x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_form = lambda x: (diff_operator.T@diff_operator, x* source_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the CUQI PDEModel, in this case a `SteadyStateLinearPDE` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUQI_pde = SteadyStateLinearPDE(poisson_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `CUQI_pde` has three main methods: \n",
    "\n",
    "1. assemble, which assembles the differential operator and the RHS given the Bayesian parameter x.\n",
    "2. solve, which solves the PDE.\n",
    "3. observe, for now observe returns the solution of the PDE, but it is to be generalized to apply observation operators on the PDE solution (e.g. extracting final temperature at specific or random points).\n",
    "\n",
    "In the following we assemble and solve this Poisson problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUQI_pde.assemble(5)\n",
    "sol, info = CUQI_pde.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(dx,L,n_poisson,endpoint=False),sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it yourself (optional):\n",
    "\n",
    "* Double the magnitude of the source term by editing the line `CUQI_pde.assemble(5)` above. Look at the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
