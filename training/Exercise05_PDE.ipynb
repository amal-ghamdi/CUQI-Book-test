{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05:  Solving partiel differential equation-based Bayesian inverse problems using CUQIpy\n",
    "\n",
    "Here we build a Bayesian problem in which the forward model is a partial differential equation (PDE) model, the 1D heat problem in particular.\n",
    "\n",
    "**Try to at least run through part 1 to 3 before working on the optional exercises.**\n",
    "\n",
    "## Learning objectives of this notebook:\n",
    "- Solve PDE-based Bayesian problem using CUQIpy.\n",
    "- Use different parametrizations of the Bayesian parameters (e.g. KL expansion, non-linear maps).\n",
    "\n",
    "## Table of contents: \n",
    "* [1. Loading the PDE test problem](#PDE_model)\n",
    "* [2. Building and solving the Bayesian inverse problem](#inverse_problem)\n",
    "* [3. Parametrizing the Bayesian parameters via step function expansion](#step_function)\n",
    "* [4. ★ Observe on part of the domain](#Partial_Observation) \n",
    "* [5. ★ Parametrizing the Bayesian parameters via KL expansion](#KL_expansion)\n",
    "\n",
    "★ Indicates optional section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Loading the PDE test problem <a class=\"anchor\" id=\"PDE_model\"></a>\n",
    "\n",
    "We first import the required python standard packages that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From CUQIpy we import the classes that we use in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../CUQIpy/\")\n",
    "\n",
    "from cuqi.testproblem import Heat_1D\n",
    "from cuqi.distribution import GaussianCov, Posterior, Gaussian, JointDistribution\n",
    "from cuqi.sampler import pCN, MetropolisHastings, CWMH\n",
    "from cuqi.geometry import KLExpansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test problem `Heat_1D` which provides a one dimensional (1D) time-dependent heat model with zero boundary conditions. The model is discretized using finite differences.\n",
    "\n",
    "The PDE is given by:\n",
    "\n",
    "$$ \\frac{\\partial u(x,t)}{\\partial t} - c^2 \\Delta_x u(x,t)   = f(x,t), \\;\\text{in}\\;\\Omega=[0,L] $$\n",
    "$$u(0,t)= u(L,t)= 0 $$\n",
    "\n",
    "where $u(x,t)$ is the temperature and $c^2$ is the thermal diffusivity (assumed to be 1 here). We assume the source term $f$ is zero. The unknown Bayesian parameters (random variable) for this test problem is the initial heat profile $\\theta(x):=u(x,0)$. The data $y$ is a random variable containing the temperature measurements everywhere in the domain at the final time $T$ corresponding to an initial $\\theta$:\n",
    "\n",
    "$$y = \\mathcal{G}(\\theta) + \\eta, \\;\\;\\; \\eta\\sim\\mathcal{N}(0,\\sigma_\\text{noise}^2\\mathbf{I}),$$ \n",
    "\n",
    "where $\\mathcal{G}(\\theta)$ is the forward model that maps the initial condition $\\theta$ to the final time solution via solving the 1D time-dependent heat problem. $\\eta$ is the measurement noise.\n",
    "\n",
    "Given observed data $y_\\text{obs}$ the task is to infer the initial heat profile $\\theta$.\n",
    "\n",
    "Before we load the `Heat_1D` problem, let us set the parameters: final time $T$, number of finite difference nodes $N$, and the length of the domain $L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30  # number of finite difference nodes            \n",
    "L = 1    # Length of the domain\n",
    "T = 0.02  # Final time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the initial condition (the exact solution for the Bayesian problem) to be a step function with three pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3\n",
    "n_steps_values = [0,1,2]\n",
    "myExactSolution = np.zeros(N)\n",
    "\n",
    "start_idx=0\n",
    "for i in range(n_steps):\n",
    "    end_idx = floor((i+1)*N/n_steps)\n",
    "    myExactSolution[start_idx:end_idx] = n_steps_values[i]\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the exact solution for each node $x_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(myExactSolution)\n",
    "plt.xlabel(\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load `Heat_1D` using the `get_components` method. We can explore `Heat_1D` initialization parameters (which are the same parameters that can be passed to `get_components` method) by calling `Heat_1D?`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, problemInfo = Heat_1D.get_components(dim=N, \n",
    "                                                  endpoint=L, \n",
    "                                                  max_time=T, \n",
    "                                                  exactSolution=myExactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at what we obtain from the test problem. We view the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the forward parameter, named `x` here, is the Bayesian parameter which we refer to as $\\theta$ above (i.e. the initial condition). \n",
    "\n",
    "\n",
    "We can look at the returned `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `problemInfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the exact solution (exact initial condition) of this inverse problem and the exact and noisy data (the final time solution before and after adding observation noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo.exactSolution.plot()\n",
    "problemInfo.exactData.plot()\n",
    "data.plot()\n",
    "plt.legend(['exact solution', 'exact data', 'noisy data']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values of the initial solution and the data at 0 and $L$ are not included in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Try yourself (optional)\n",
    "* The data plotted above was generated from the model. Confirm that the model actually generates this data (the exact data) by applying `model.forward` on the exact solution (the initial heat profile). This can be done in ~1 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you view the heat profile at time T= 0.001? At 0.002, 0.003, ..., 0.02? What do you notice? (hint: you can do that by choosing different final time when loading `Heat_1D`. Be sure to use different variable names for the returned `model`, `data` and `problemInfo` because they are used in section 2. You can use for example, `model_t2`, `data_t2` and `problemInfo_t2`). Setting the new `Heat_1D` problem can be done in ~1 lines of code and plotting `problemInfo_t2.exactSolution`, `problemInfo_t2.exactData`, `data_t2` can be done in ~3 to 4 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and solving the Bayesian inverse problem <a class=\"anchor\" id=\"inverse_problem\"></a>\n",
    "\n",
    "The joint distribution of the data $y$ and the parameter $x$ (or $\\theta$) is given by\n",
    "\n",
    "$$p(x,y) = p(y|x)p(x)$$\n",
    "\n",
    "\n",
    "\n",
    "Where $p(x)$ is the prior pdf, $p(y|x)$ is the data distribution pdf. We start by defining the prior distribution $\\Pi(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std = 1.2\n",
    "x = Gaussian(mean*np.ones(N), std, geometry= model.domain_geometry) # The prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Try yourself (optional)\n",
    "* Create prior samples (~1 line).\n",
    "* Plot the 95% credibility interval of the prior samples (~1 line).\n",
    "* Look at the 95% credibility interval of the PDE model solution to quantify the forward uncertainty (~2 lines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the data distribution $\\Pi(y|x)$, we first estimate the noise level. Because here we know the exact data, we can estimate the noise level as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then define the data distribution $\\Pi(y|x)$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Gaussian(mean=model, std=sigma_noise, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the components we need, we can create the joint distribution $\\Pi(x,y)$, from which the posterior distribution can be created by setting $y=y_\\text{obs}=$`data`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the joint distribution $\\Pi(x,y)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = JointDistribution(y, x)\n",
    "print(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution pdf is given by the Bayes rule:\n",
    "$$ p(x|y=y_\\text{obs}) \\propto p(y=y_\\text{obs}|x)p(x) $$ \n",
    "By setting $y=\\texttt{data}$ in the joint distribution we obtain the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = joint(y=data)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the joint distribution to an object of type posterior (this is a temporary hack and in the near future samplers will be able to sample `JointDistributions` directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = posterior._reduce_to_single_density() #TODO: eventually remove this line\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample the posterior. Let's try the preconditioned Crank-Nicolson (pCN) sampler (~30 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = pCN(posterior)\n",
    "posterior_samples = MySampler.sample_adapt(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the $95\\%$ credible interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact=problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean reconstruction of the initial solution matches the general trend of the exact solution to some extent but it does not capture the piece-wise constant nature of the exact solution.\n",
    "\n",
    "Also we note that since the heat problem has zero boundary conditions, the initial solution reconstruction tend to go to zero at the right boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parametrizing the Bayesian parameters via step function expansion <a class=\"anchor\" id=\" step_function\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the solution of this Bayesian problem is to use better prior information. Here we assume the prior is a step function with three pieces. This also makes the Bayesian problem simpler because now we only have three Bayesian parameters to infer.\n",
    "\n",
    "To test this case we pass `field_type='Step'` to `Heat_1D.get_components`, which creates a `StepExpansion` domain geometry for the model during initializing the `Heat_1D` test problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3 # number of steps in the step expansion domain geometry\n",
    "N = 30\n",
    "model, data, problemInfo = Heat_1D.get_components(dim=N, \n",
    "                                                  endpoint=L, \n",
    "                                                  max_time=T, \n",
    "                                                  field_type='Step', \n",
    "                                                  n_steps=n_steps, \n",
    "                                                  exactSolution=myExactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `model` in this case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then continue to create the Bayesian problem (prior, data distribution and posterior) with a prior of dimension = n_steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior\n",
    "x = Gaussian(mean*np.ones(n_steps), std, geometry= model.domain_geometry)\n",
    "\n",
    "# Data distribution\n",
    "sigma_noise = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_noise, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution(y, x)\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample the posterior using Metropolis Hastings sampler (~30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = MetropolisHastings(posterior)\n",
    "posterior_samples = MySampler.sample_adapt(25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact=problemInfo.exactSolution)\n",
    "posterior_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the trace plot: a plot of the kernel density estimator (left) and chains (right) of the `n_steps` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show pair plot of 2D marginal posterior distributions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there seems to be some burn-in samples until the chain reaches the high density region. We show the pair plot after removing 200 burn-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.burnthin(200).plot_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, visually, the burn-in is indeed removed. Another observation here is the clear correlation (or inverse correlation) between each pair of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the effective sample size (ESS) which approximately gives the number of independent samples in the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.compute_ess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it yourself (optional):\n",
    "* For this step function parametrization, try to enforce positivity of prior and the posterior samples via log parametrization which can be done by passing `map = lambda x : np.exp(x)` to the `Heat_1D.get_components` method. Then run the MetropolisHastings sampler again (similar to part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observe on part of the domain <a class=\"anchor\" id=\"Partial_Observation\"></a> ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we solve the same problem as in section 3 but with observing the data only on the right half of the domain.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the number of steps to be 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "n_steps = 4 # Number of steps in the StepExpansion geometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write the `observation_nodes` map which can be passed to the `Heat_1D`.\n",
    "It is a lambda function that takes the forward model range grid ('range_grid') as an input and generates a sub grid of the nodes where we have observations (data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_nodes = lambda x: x[np.where(x>L/2)] # observe in the right half of the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the `Heat_1D` problem. Note in this case we do not pass an `exactSolution`. If no `exactSolution` is passed, the `Heat_1D` test problem will create an exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, problemInfo = Heat_1D.get_components(dim=N, \n",
    "                                                  endpoint=L, \n",
    "                                                  max_time=T, \n",
    "                                                  field_type='Step', \n",
    "                                                  n_steps=n_steps, \n",
    "                                                  observation_nodes=observation_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the exact solution of this inverse problem and the exact and noisy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo.exactSolution.plot()\n",
    "problemInfo.exactData.plot()\n",
    "data.plot()\n",
    "plt.legend(['exact solution', 'exact data', 'noisy data']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then continue to create the Bayesian problem (prior, data distribution and posterior) with a prior of dimension = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior\n",
    "x = Gaussian(np.ones(n_steps), std, geometry= model.domain_geometry)\n",
    "\n",
    "# Data distribution\n",
    "sigma_noise = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_noise, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution(y, x)\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample the posterior using MetropolisHastings (~40 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = MetropolisHastings(posterior, x0=np.ones(posterior.dim))\n",
    "posterior_samples = MySampler.sample_adapt(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact=problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the credible interval is wider on the side of the domain where data is not available (the left side) and narrower as we get to the right side of the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Parametrizing the Bayesian parameters via KL expansion ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the Bayesian inversion for a more general exact solution. We parametrize the Bayesian parameters using Karhunen–Loève (KL) expansion. This will represent the inferred heat initial profile as a linear combination of sine functions. \n",
    "$$ u(x,0) = \\sum_i \\theta_i  (1/i)^{\\text{decay}}  sin(\\frac{i L x}{\\pi}). $$\n",
    "Where $\\theta_i$ are the Bayesian parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the Heat_1D test case and pass `field_type = 'KL'`, which behind the scenes will set the domain geometry of the model to be a KL expansion geometry (`KLExpansion`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 35\n",
    "model, data, problemInfo = Heat_1D.get_components(dim=N, \n",
    "                                                  endpoint=L, \n",
    "                                                  max_time=T, \n",
    "                                                  field_type='KL' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we inspect the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the exact solution and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemInfo.exactSolution.plot()\n",
    "problemInfo.exactData.plot()\n",
    "data.plot()\n",
    "plt.legend(['exact solution', 'exact data', 'noisy data']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the exact solution here is a general signal that is not constructed from the basis functions. We define the prior $p(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_prior = 9*np.ones(model.domain_dim)\n",
    "x = GaussianCov(mean*np.ones(N), sigma_prior, geometry= model.domain_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = np.std(problemInfo.exactData - data)*np.ones(model.range_dim) # noise level\n",
    "y = Gaussian(mean=model, std=sigma_noise, geometry=model.range_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint =  JointDistribution(y, x)\n",
    "posterior = joint(y=data)\n",
    "posterior = posterior._reduce_to_single_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample the posterior, here we use Component-wise Metropolis Hastings (~90 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySampler = CWMH(posterior, x0=np.ones(N))\n",
    "posterior_samples = MySampler.sample_adapt(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the $95\\%$ credibility interval (you can try plotting different credibility intervals, e.g. $80\\%$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, exact=problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credibility interval can have zero width at some locations where the upper and lower limits seem to intersect and switch order (uppers becomes lower and vice versa). To look into what actually happen here, we plot some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_burnthin = posterior_samples.burnthin(0,10)\n",
    "for i, s in enumerate(posterior_samples_burnthin):\n",
    "    model.domain_geometry.plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples seem to paint a different picture than what the credibility interval plot shows. Note that the computed credibility interval above, is computed on the domain geometry parameter space, then converted to the function space for plotting. We can alternatively convert the samples to function values first, then compute and plot the credibility interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert samples to function values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funvals_samples = posterior_samples.funvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the credibility interval computed from the function values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funvals_samples.plot_ci(95, exact=problemInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the credibility interval now reflects what the samples plot shows and does not have these locations where the upper and lower bounds intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effective sample size (ESS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.compute_ess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the ESS varies considerably among the variables. We can view the trace plot for, let's say, the first and the second variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_trace([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way of looking at the credibility intervals, is to look at the expansion coefficients  $\\theta_i$ credibility intervals. We plot the credibility intervals for these coefficients from both prior  and posterior samples by passing the flag `plot_par=True` to `plot_ci` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x.sample(1000).plot_ci(95, plot_par=True)\n",
    "plt.xticks(np.arange(x.dim)[::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_ci(95, plot_par=True)\n",
    "plt.xticks(np.arange(x.dim)[::5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
